[
  {
    "id": "recuJX4ZuBdhVYyaY",
    "title": "From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents in Security Patch Detection",
    "url": "http://arxiv.org/abs/2511.08060v1",
    "summary": "Furthermore, we analyze the detection performance of these methods across various vulnerability types, and examine the impact of different prompting strategies and context window sizes on the results. Our findings reveal that the Data-Aug LLM achieves the best overall performance, whereas the ReAct Agent demonstrates the lowest false positive rate (FPR). These findings provide valuable insights into the practical applications of LLMs and LLM-based agents in security patch detection, highlight...",
    "source": "arXiv",
    "publication_date": "2025-11-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec2GuoruJEYY8V2j",
    "title": "Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection",
    "url": "http://arxiv.org/abs/2511.07364v1",
    "summary": "Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC.",
    "source": "arXiv",
    "publication_date": "2025-11-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recNiViTeu0dXmUqG",
    "title": "Procedural Knowledge Improves Agentic LLM Workflows",
    "url": "http://arxiv.org/abs/2511.07568v1",
    "summary": "Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.",
    "source": "arXiv",
    "publication_date": "2025-11-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczjxm1t7TCelich",
    "title": "LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives",
    "url": "http://arxiv.org/abs/2511.07641v1",
    "summary": "While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the compl...",
    "source": "arXiv",
    "publication_date": "2025-11-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTXAWEARpVxIt0M",
    "title": "Stemming Hallucination in Language Models Using a Licensing Oracle",
    "url": "http://arxiv.org/abs/2511.06073v1",
    "summary": "Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. Although the Licensing Oracle is specifically designed to addr...",
    "source": "arXiv",
    "publication_date": "2025-11-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rece6lamTnfhgsucs",
    "title": "Who Gets Heard? Rethinking Fairness in AI for Music Systems",
    "url": "http://arxiv.org/abs/2511.05953v1",
    "summary": "These biases can misrepresent marginalized traditions, especially from the Global South, producing inauthentic outputs (e.g., distorted ragas) that reduces creators' trust on these systems. Such harms risk reinforcing biases, limiting creativity, and contributing to cultural erasure. To address this, we offer recommendations at dataset, model and interface level in music-AI systems.",
    "source": "arXiv",
    "publication_date": "2025-11-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 14.0
  },
  {
    "id": "rec1ByRqiU6i5vRAZ",
    "title": "Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models",
    "url": "http://arxiv.org/abs/2511.05184v1",
    "summary": "We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural languag...",
    "source": "arXiv",
    "publication_date": "2025-11-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recf0yZb6eXnGN5CJ",
    "title": "Artificial intelligence on the edge of ethics: how ChatGPT became a source of dangerous advice for young people - UA.NEWS",
    "url": "https://news.google.com/rss/articles/CBMivAFBVV95cUxOY3BlMlRvN3pZaXZFQ1lVZW02QlljQlZIc2NLZTQ4WndaRnJOWm5jOUJkN0tJTDgzS1dRQVU3ckUyUHV0WldjTmlSMGh1VWlrbDJ3X1NQTWx6WFdaV0VJY2gtNVNrRVR6UGlSV3Jsak5ybUU0U1gtWkxnV0ZOajRFWkUyWlN2bC1WQTk4MGRnRFo4dEhmel9PbFpQRURJSFo0UHJ5M1lub0ttYWJVemYzY3c4RmtzNGcxSDl2UQ?oc=5",
    "summary": "Artificial intelligence on the edge of ethics: how ChatGPT became a source of dangerous advice for young people UA.NEWS",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-11-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recOqdbo5fXAOhr33",
    "title": "When chatbot sycophancy meets mental illness, the results can be deadly. - Psychology Today",
    "url": "https://news.google.com/rss/articles/CBMimwFBVV95cUxNc09LVmVYbUdvVW4yVW9GcGVxYkN4UUZvaG1HWFdxTlNlVlhTalBpVU9jbkt6UE1rOGpwUGh6ei1ZMkJYWDVmd0dkSDdnaDEzX0gtSXJMaVRWLVRtZ1M0bGVNbGhrek1QWlczVllXS1lhTnJqYVh1Y24xOHdYV012TkVkaDNMZ3QtcGpTQUZRTlVVZ3dQM3ZLS3B6a9IBoAFBVV95cUxNVXZ2d0x1UkdTQU9qUlpMMU52b0FsMWFvZnBBazVJT0F5QmRDdTlTMVBxMFBRYWVTbFl1ckt4UWE4MEluckIyZllJS0hRVG9MRDBvWEJwVE8xWXBlVWNrR1l0ZDZxZkNHQm5fdUhuaGRFX3ZDS2YyX2gxNkYzUFVaa2NVZkpaaU05bWhiSnFHR1dKRGt5cFo3OFdYeDQxZkVE?oc=5",
    "summary": "When chatbot sycophancy meets mental illness, the results can be deadly. Psychology Today",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-11-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recBrGXzYdp4wXF4F",
    "title": "Control Barrier Function for Aligning Large Language Models",
    "url": "http://arxiv.org/abs/2511.03121v2",
    "summary": "This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The safety filter includes two significant advantages: this safety filter is an add-on type, allowing it to be used for alignment purposes without fine-tuning the baseline LLM, and if there is an evaluation model regarding the desired alignment, it can be directly applied to the filter design. The overall text-generati...",
    "source": "arXiv",
    "publication_date": "2025-11-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmgJuTt23wA5GGR",
    "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications",
    "url": "http://arxiv.org/abs/2511.02979v1",
    "summary": "The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional companions and game NPCs to embodied functional robots. Quadrant II (Functional Virtual Assistants) analyzes AI applications in work, gaming, and mental health, highlighting the shift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift ...",
    "source": "arXiv",
    "publication_date": "2025-11-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 25.0
  },
  {
    "id": "rec4Y1Ap7PtL3ZZun",
    "title": "Are ChatGPT, Gemini And Other AI Chatbots Too Eager To Please You? New Study Flags ‘Sycophancy’ In Leading US, Chinese Models - Stocktwits",
    "url": "https://news.google.com/rss/articles/CBMigwJBVV95cUxQYUxFcVZPcXg5RFBrOFJsV2JnN1VpQ1hyZFBtQllkTVpvTDJsUUw1M1lUWG9ndEZVYlpiWnFVSVdNTzhEYTltRlZiZld6dWo5OGdnX0l1MVMxTzJ1RzFuVnVRTTNUaGtfSFVoR3hjQzV5VzFxLWRxZUFPWFhvaVBBcGx6OGVMT1F2Y2dxeXA2SlZNdElYemhuZUV4T0ZkLXI3VmF3ZzFyb3FFZFo2S0d1T05oVTlqS1M3dmVvNy1zcGFxbEV2eURxbkxVQXltcVFQelRYOWZTc3NGMFh2QmJRVVBldnRFR0NiTG1UeElpYTRpdmk5MGxVR3h4UWNkRVg5ck5z?oc=5",
    "summary": "Are ChatGPT, Gemini And Other AI Chatbots Too Eager To Please You? New Study Flags ‘Sycophancy’ In Leading US, Chinese Models Stocktwits",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-10-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recjL99LqzOgX2cp4",
    "title": "FinPos: A Position-Aware Trading Agent System for Real Financial Markets",
    "url": "http://arxiv.org/abs/2510.27251v1",
    "summary": "The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. Furthermore, the continuous nature of position management necessitates our adoption of multi-timescale rewards, which in turn empowers FinPos to effectively balance short-term fluctuations against long-term trends. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-te...",
    "source": "arXiv",
    "publication_date": "2025-10-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recLx08Hdr7nPVpPF",
    "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation",
    "url": "http://arxiv.org/abs/2510.26213v1",
    "summary": "A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M ...",
    "source": "arXiv",
    "publication_date": "2025-10-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczXbPpfXLKgbdkv",
    "title": "Do Chatbots Walk the Talk of Responsible AI?",
    "url": "http://arxiv.org/abs/2510.24823v1",
    "summary": "This study examines whether leading AI chatbot companies implement the responsible AI principles they publicly advocate. The authors used a mixed-methods approach analyzing four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across company websites, technical documentation, and direct chatbot evaluations. We found significant gaps between corporate rhetoric and practice.",
    "source": "arXiv",
    "publication_date": "2025-10-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec95lnbua7NnRLEc",
    "title": "EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models",
    "url": "http://arxiv.org/abs/2510.22758v1",
    "summary": "Existing benchmarks typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in isolation, overlooking the integration of these skills that is crucial for human-like, emotionally intelligent conversation. We present EchoMind, the first interrelated, multi-level benchmark that simulates the cognitive process of empathetic dialogue through sequential, context-linked tasks: spoken-content understanding, vocal-cue perception, integrated reasoning, and response generation. All tas...",
    "source": "arXiv",
    "publication_date": "2025-10-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGBixmcmvCquCGA",
    "title": "Are you the asshole? Of course not –quantifying LLMs' sycophancy problem",
    "url": "https://news.ycombinator.com/item?id=45705974",
    "summary": "Two recent research papers have come at this problem a bit more rigorously, though, taking different tacks in attempting to quantify exactly how likely an LLM is to listen when a user provides factually incorrect or socially inappropriate information in a prompt. In one pre-print study published this month, researchers from Sofia University and ETH Zurich looked at how LLMs respond when false statements are presented as the basis for difficult mathematical proofs and problems. The BrokenMath ...",
    "source": "Hacker News",
    "publication_date": "2025-10-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recZLKGe7zegjMX9d",
    "title": "Understanding Network Behaviors through Natural Language Question-Answering",
    "url": "http://arxiv.org/abs/2510.21894v1",
    "summary": "In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further enhance this direction, leveraging their extensive prior knowledge of network concepts and strong reasoning capabilities. However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM's long-context understanding ability; 2) heter...",
    "source": "arXiv",
    "publication_date": "2025-10-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdARr7YB4X4KDSH",
    "title": "‘Sycophantic’ AI chatbots tell users what they want to hear, study shows - The Guardian",
    "url": "https://news.google.com/rss/articles/CBMivAFBVV95cUxPM1NQY0hlRDhacTVJZFR1SzV0QW91ZWJOWTQ2QVJOMUxfV3J0Y0lZVFRLRW1EOVN3QTZmdWpzRVdLOElwU2x4VzM3bkNCVXBScjc3YkNWSzNQUkI4b1dsb0xIekZtTkZTOUY1dVlXT2Y0VXhIdFlhdHNjRHA1WDh2WFpsQXl2clhKNzQxaDh3Z3JTMDlpajg3V3RjNkpnV3o3Y2NuZXlXOHFIbXYtZjdOV01XV0YxRFVVMnpXSQ?oc=5",
    "summary": "‘Sycophantic’ AI chatbots tell users what they want to hear, study shows The Guardian",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-10-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recwM0UBTDMF98p3b",
    "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain",
    "url": "http://arxiv.org/abs/2510.20369v1",
    "summary": "By contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy...",
    "source": "arXiv",
    "publication_date": "2025-10-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec640L1vytyT3BU4",
    "title": "ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography",
    "url": "http://arxiv.org/abs/2510.18339v1",
    "summary": "Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performa...",
    "source": "arXiv",
    "publication_date": "2025-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recI3YGx2gdqDiLH9",
    "title": "Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures",
    "url": "http://arxiv.org/abs/2510.18932v1",
    "summary": "Evaluating the creative capabilities of large language models (LLMs) in complex tasks often requires human assessments that are difficult to scale. We introduce a novel, scalable methodology for evaluating LLM story generation by analyzing underlying social structures in narratives as signed character networks. Our proposed approach provides a valuable tool for evaluating limitations and tendencies in the creative storytelling of current and future LLMs.",
    "source": "arXiv",
    "publication_date": "2025-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3RJwKUEzksbp5s",
    "title": "Tech Brief: AI Sycophancy and OpenAI",
    "url": "https://news.ycombinator.com/item?id=45648267",
    "summary": "By distilling complex developments into accessible, evidence-based insights, this tech brief will ideally help policymakers, researchers, enforcers, and the public get up to speed on emerging risks, company conduct, and areas that may require further scrutiny or oversight. Fed into a user’s suspicions that he was living in a false reality which he could escape only by “unplugging his mind,” sending him down a “dangerous, delusional spiral.” The chatbot “instructed him to give up sleeping pill...",
    "source": "Hacker News",
    "publication_date": "2025-10-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 50.0
  },
  {
    "id": "recP3tEvApETZVTOH",
    "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems",
    "url": "http://arxiv.org/abs/2510.17281v2",
    "summary": "Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of st...",
    "source": "arXiv",
    "publication_date": "2025-10-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recCWp9Pwb7X3sZ85",
    "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models",
    "url": "http://arxiv.org/abs/2510.16712v2",
    "summary": "Through our novel Chameleon Benchmark Dataset, comprising 17,770 carefully crafted question-answer pairs across 1,180 multi-turn conversations spanning 12 controversial domains, we expose fundamental flaws in state-of-the-art systems. Our analysis uncovers the mechanism: strong correlations between source re-use rate and confidence (r=0.627) and stance changes (r=0.429) are statistically significant (p less than 0.05), indicating that limited knowledge diversity makes models pathologically de...",
    "source": "arXiv",
    "publication_date": "2025-10-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recCzMlX3PoOYgdr8",
    "title": "Justitia: Fair and Efficient Scheduling for LLM Applications",
    "url": "http://arxiv.org/abs/2510.17015v1",
    "summary": "First, given that memory is prevalently a bottleneck for mainstream inference frameworks like vLLM, Justitia models the service cost of LLM applications in a memory-centric manner. Moreover, Justitia adopts a virtual-time based fair queuing algorithm to reduce the overall performance with guaranteed worst-case delay. We have implemented Justitia atop vLLM, and experimental results involving diverse LLM applications show that it can substantially enhance the scheduling efficiency with fairness...",
    "source": "arXiv",
    "publication_date": "2025-10-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recfb0eZaieGwCQBJ",
    "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models",
    "url": "http://arxiv.org/abs/2510.16727v1",
    "summary": "Large language models internalize a structural trade-off between truthfulness and obsequious flattery, emerging from reward optimization that conflates helpfulness with polite submission. We further propose prompt-level and activation-level interventions that modulate these biases in opposing directions, exposing the internal geometry of alignment as a dynamic manifold between truthfulness and socially compliant judgment. Beacon reframes sycophancy as a measurable form of normative misgeneral...",
    "source": "arXiv",
    "publication_date": "2025-10-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec6aPRXqP7HoB7Ci",
    "title": "What Research Says About \"AI Sycophancy\"",
    "url": "https://news.ycombinator.com/item?id=45625511",
    "summary": "In two previous research roundups for Tech Policy Press, I looked at studies that considered how AI chatbots affect mental health and may encourage addictive behaviors. Authors: Aaron Fanous, Jacob Goldberg, Ank Agarwal, Joanna Lin, Anson Zhou, Sonnet Xu, Vasiliki Bikia, Roxana Daneshjou, Sanmi Koyejo This study examines both of these cases and also investigates the quality of rebuttals and how that influences AI sycophancy in order to “provide actionable insights for prompt design.”",
    "source": "Hacker News",
    "publication_date": "2025-10-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 40.0
  },
  {
    "id": "recnxw5PKRszBp5fa",
    "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs",
    "url": "http://arxiv.org/abs/2510.16374v1",
    "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms: Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack mechanisms to verify whether selected strategies succeed; while Generate-Verify approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan et al., 2023) iteratively refine outputs but commence generation blindly without task assessment. We address this gap by ...",
    "source": "arXiv",
    "publication_date": "2025-10-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recy6nP8xgfF2vCOU",
    "title": "HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment",
    "url": "http://arxiv.org/abs/2510.15499v1",
    "summary": "Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR) have gained significant attention due to their objective and verifiable reward signals, demonstrating strong performance in reasoning and code generation tasks. Across five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack success rate of 96.01\\\\%, significantly outperforming harmful fine-tuning while preserving genera...",
    "source": "arXiv",
    "publication_date": "2025-10-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 13.0
  },
  {
    "id": "recPXJmLAEjFh8wOt",
    "title": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking",
    "url": "http://arxiv.org/abs/2510.13694v1",
    "summary": "We further observe that reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent space, measured by Mahalanobis distance from the SFT-induced distribution. Finally, we present Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying reward hacking severity, enabling principled hyperparameter tuning and online mitigation such as early stopping. Extensive experiments across diverse LLMs and datasets confirm the generality of our findings, the effectivenes...",
    "source": "arXiv",
    "publication_date": "2025-10-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recAWbg3Fb6qXj6XQ",
    "title": "HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment",
    "url": "http://arxiv.org/abs/2510.12217v2",
    "summary": "To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.",
    "source": "arXiv",
    "publication_date": "2025-10-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recJMp1hmBXikwGS5",
    "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "url": "http://arxiv.org/abs/2510.11328v1",
    "summary": "This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3).",
    "source": "arXiv",
    "publication_date": "2025-10-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsyDijgtY6euBbP",
    "title": "Deep Research Brings Deeper Harm",
    "url": "http://arxiv.org/abs/2510.11851v2",
    "summary": "Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts.",
    "source": "arXiv",
    "publication_date": "2025-10-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "rec9fO6Hpn8oZhZOd",
    "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups",
    "url": "http://arxiv.org/abs/2510.13852v2",
    "summary": "This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.",
    "source": "arXiv",
    "publication_date": "2025-10-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recL4KXW9HiYHoMmY",
    "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement",
    "url": "http://arxiv.org/abs/2510.09738v1",
    "summary": "We assess how well 54 LLMs can replicate human judgment when scoring responses from RAG (Retrieval-Augmented Generation) or Agentic pipelines against ground truth answers. The two-step approach includes: (1) a correlation test that filters judges with strong alignment, followed by (2) a human-likeness test using z-scores to identify two distinct judgment patterns: human-like judgment (|z| < 1) that mimics natural human variation, and super-consistent judgment (z > 1) that exceeds typical huma...",
    "source": "arXiv",
    "publication_date": "2025-10-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWF7OOPlM9f3ftW",
    "title": "RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation",
    "url": "http://arxiv.org/abs/2510.08931v1",
    "summary": "Data contamination poses a significant challenge to reliable LLM evaluation, where models may achieve high performance by memorizing training data rather than demonstrating genuine reasoning capabilities. RADAR extracts 37 features spanning surface-level confidence trajectories and deep mechanistic properties including attention specialization, circuit dynamics, and activation flow patterns. This work demonstrates the potential of mechanistic interpretability for advancing LLM evaluation beyo...",
    "source": "arXiv",
    "publication_date": "2025-10-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recatcqw2s8pcCU3z",
    "title": "Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM",
    "url": "http://arxiv.org/abs/2510.09158v2",
    "summary": "This study proposes augmenting dialog data with think-aloud utterances (TAUs) for modeling individual personalities in text chat by LLM. We expect \"persona LLMs\" trained with TAU-augmented data can mimic the speaker's personality trait better. We also found that the quality of TAU-augmentation impacts persona LLM's performance.",
    "source": "arXiv",
    "publication_date": "2025-10-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJ2eKrh3Dx1qRCq",
    "title": "LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests",
    "url": "http://arxiv.org/abs/2510.08616v1",
    "summary": "Benchmark scores for Large Language Models (LLMs) can be inflated by memorization of test items or near duplicates. We present a simple, protocol that probes generalization by re-evaluating models on paraphrased versions of benchmark questions. Our pipeline controls decoding, enforces multiple-choice output format, and includes a robust paraphrase-cleaning step to preserve semantics.",
    "source": "arXiv",
    "publication_date": "2025-10-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recM6JbeL7SQdIpOC",
    "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
    "url": "http://arxiv.org/abs/2510.07213v1",
    "summary": "Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual ...",
    "source": "arXiv",
    "publication_date": "2025-10-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIjzVTCtcBWpnYm",
    "title": "Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input",
    "url": "http://arxiv.org/abs/2510.05864v1",
    "summary": "Large language models (LLMs) increasingly support applications that rely on extended context, from document processing to retrieval-augmented generation. Across harmful content categories such as toxic, offensive, and hate speech, with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance peaks at moderate harmful prevalence (0.25) but declines when content is very sparse or dominant; recall decreases with increasing context length; harmful sentences at the beginning are ge...",
    "source": "arXiv",
    "publication_date": "2025-10-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recxzjPCQ2CmyxqmL",
    "title": "Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs",
    "url": "http://arxiv.org/abs/2511.02842v1",
    "summary": "However, traditional methods, such as expert interviews, while effective, face several challenges, including scheduling conflicts, resource constraints, inconsistency, etc. Specifically, the chatbot integrates workflow-based instruction with LLM's planning and reasoning capabilities, enabling it to function as a virtual expert and conduct interviews. Our preliminary evaluation indicates that the chatbot performs as designed, effectively following predefined workflows and supporting user inter...",
    "source": "arXiv",
    "publication_date": "2025-10-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvuflIJKlY6i1OA",
    "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
    "url": "http://arxiv.org/abs/2510.04721v1",
    "summary": "To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, p...",
    "source": "arXiv",
    "publication_date": "2025-10-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reczj8mn2ur9xWheW",
    "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests",
    "url": "http://arxiv.org/abs/2510.04891v1",
    "summary": "We introduce SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries, designed to surface where LLMs most acutely fail in politically charged contexts. Moreover, temporal and geographic analyses show that LLMs are most fragile when confronted with 21st-century or pre-20th-century contexts, and when responding to prompts tied to regions such as Latin America, the USA, and the UK. These findings demonstrate that current safeguards fail to generalize to hi...",
    "source": "arXiv",
    "publication_date": "2025-10-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recfColh1ChsbF8bq",
    "title": "LLM Chemistry Estimation for Multi-LLM Recommendation",
    "url": "http://arxiv.org/abs/2510.03930v1",
    "summary": "We formalize the notion of chemistry among LLMs, propose algorithms that quantify it by analyzing interaction dependencies, and recommend optimal model ensembles accordingly. Our theoretical analysis shows that chemistry among collaborating LLMs is most evident under heterogeneous model profiles, with its outcome impact shaped by task type, group size, and complexity. Evaluation on classification, summarization, and program repair tasks provides initial evidence for these task-dependent effec...",
    "source": "arXiv",
    "publication_date": "2025-10-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recyGsi70W3YN0bpK",
    "title": "Invisible Saboteurs: Sycophantic LLMs Mislead Novices in Problem-Solving Tasks",
    "url": "http://arxiv.org/abs/2510.03667v1",
    "summary": "Sycophancy, the tendency of LLM-based chatbots to express excessive enthusiasm, agreement, flattery, and a lack of disagreement, is emerging as a significant risk in human-AI interactions. However, the extent to which this affects human-LLM collaboration in complex problem-solving tasks is not well quantified, especially among novices who are prone to misconceptions. Our findings show that users of the high sycophancy chatbot were less likely to correct their misconceptions and spent more tim...",
    "source": "arXiv",
    "publication_date": "2025-10-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recLScLUyKgfJUNaY",
    "title": "Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models",
    "url": "http://arxiv.org/abs/2510.01845v1",
    "summary": "State-of-the-art vision-and-language models consist of many parameters and learn from enormous datasets, surpassing the amounts of linguistic data that children are exposed to as they acquire a language. This paper presents our approach to the multimodal track of the BabyLM challenge addressing this discrepancy. We develop language-only and multimodal models in low-resource settings using developmentally plausible datasets, with our multimodal models outperforming previous BabyLM baselines.",
    "source": "arXiv",
    "publication_date": "2025-10-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWyujfSGXHz3dm2",
    "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation",
    "url": "http://arxiv.org/abs/2510.01295v1",
    "summary": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reachi...",
    "source": "arXiv",
    "publication_date": "2025-10-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recF7VCi4Dr9h4u6k",
    "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation",
    "url": "http://arxiv.org/abs/2509.26351v1",
    "summary": "While the MIMIC-IV-ED database is openly available to credentialed researchers, transforming it into a triage-focused benchmark requires extensive preprocessing, feature harmonization, and schema alignment -- barriers that restrict accessibility to only highly technical users. We address these gaps by first introducing an open, LLM-assisted emergency triage benchmark for deterioration prediction (ICU transfer, in-hospital mortality). Large language models (LLMs) contributed directly to datase...",
    "source": "arXiv",
    "publication_date": "2025-09-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recN6ZfyLsej3m1JC",
    "title": "Bias-Aware AI Chatbot for Engineering Advising at the University of Maryland A. James Clark School of Engineering",
    "url": "http://arxiv.org/abs/2510.09636v1",
    "summary": "This study aims to develop a University of Maryland (UMD) A. James Clark School of Engineering Program-specific AI chatbot. However, due to the small sample size and limited timeframe, our AI model may not fully reflect the nuances of student queries in engineering academic advising. Regardless, these findings will inform best practices for building ethical AI systems in higher education, offering tools to complement traditional advising and address the inequities faced by many underrepresent...",
    "source": "arXiv",
    "publication_date": "2025-09-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQr2IzaBys5p5zg",
    "title": "Interactive Learning for LLM Reasoning",
    "url": "http://arxiv.org/abs/2509.26306v3",
    "summary": "However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. Experimental results show that ILR consistently outperforms single-agent le...",
    "source": "arXiv",
    "publication_date": "2025-09-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec9MHfCfS7KsLzjX",
    "title": "ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models",
    "url": "http://arxiv.org/abs/2510.00071v2",
    "summary": "Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. We propose \\\\textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. Our extensive evaluation across mathematical reasoning benchmarks using multiple model ar...",
    "source": "arXiv",
    "publication_date": "2025-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recD2D4dmLXLfS5gW",
    "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
    "url": "http://arxiv.org/abs/2509.24713v1",
    "summary": "We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\\\\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. We introduce \\\\textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augment...",
    "source": "arXiv",
    "publication_date": "2025-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recW90JVrqh8QTXYy",
    "title": "HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment",
    "url": "http://arxiv.org/abs/2509.24384v1",
    "summary": "In recent years, a proliferation of jailbreak attacks has emerged, accompanied by diverse metrics and judges to assess the harmfulness of the LLM outputs. To address this gap, we introduce HarmMetric Eval, a comprehensive benchmark designed to support both overall and fine-grained evaluation of harmfulness metrics and judges. With HarmMetric Eval, our extensive experiments uncover a surprising result: two conventional metrics--METEOR and ROUGE-1--outperform LLM-based judges in evaluating the ...",
    "source": "arXiv",
    "publication_date": "2025-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recZ5VGNyZPx4jyES",
    "title": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs",
    "url": "http://arxiv.org/abs/2509.24857v1",
    "summary": "In this work, we address these gaps by introducing a unified taxonomy of six clinically-informed mental health crisis categories, curating a diverse evaluation dataset, and establishing an expert-designed protocol for assessing response appropriateness. We also identify systemic weaknesses in handling indirect or ambiguous risk signals, a reliance on formulaic and inauthentic default replies, and frequent misalignment with user context. Our taxonomy, datasets, and evaluation framework lay the...",
    "source": "arXiv",
    "publication_date": "2025-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recnwCsuUK0iE3qf9",
    "title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning",
    "url": "http://arxiv.org/abs/2509.24050v1",
    "summary": "Existing approaches typically rely on external routers, implemented as binary classifiers, which often struggle to determine task difficulty from the prompt's surface pattern. To address these limitations, we propose a framework where the on-device LLM makes routing decisions at the end of its solving process, with this capability instilled through post-training. Extensive experiments across models and benchmarks show that the proposed methodology consistently outperforms existing baselines a...",
    "source": "arXiv",
    "publication_date": "2025-09-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRj4QN34AEIgcX4",
    "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment",
    "url": "http://arxiv.org/abs/2509.22745v2",
    "summary": "MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fin...",
    "source": "arXiv",
    "publication_date": "2025-09-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recdeANTNnmDJXslq",
    "title": "Benchmarking and Mitigate Sycophancy in Medical Vision-Language Models",
    "url": "http://arxiv.org/abs/2509.21979v2",
    "summary": "Vision language models(VLMs) are increasingly integrated into clinical workflows, but they often exhibit sycophantic behavior prioritizing alignment with user phrasing social cues or perceived authority over evidence based reasoning. Imitation and expert provided corrections were found to be the most effective triggers, suggesting that the models possess a bias mechanism independent of visual evidence. Our benchmark analysis and mitigation framework lay the groundwork for robust deployment of...",
    "source": "arXiv",
    "publication_date": "2025-09-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recwJYxO4tEWOuOJU",
    "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models",
    "url": "http://arxiv.org/abs/2509.21843v1",
    "summary": "Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution. Across Qwen, LLaMA, and Gemma models, with only one singl...",
    "source": "arXiv",
    "publication_date": "2025-09-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recymXxnxl8eCJvwW",
    "title": "Semantic Agreement Enables Efficient Open-Ended LLM Cascades",
    "url": "http://arxiv.org/abs/2509.21837v3",
    "summary": "To address this, we propose semantic agreement -- meaning-level consensus between ensemble outputs -- as a training-free signal for reliable deferral. We show that when diverse model outputs agree semantically, their consensus is a stronger reliability signal than token-level confidence. Evaluated from 500M to 70B-parameter models, we find that semantic cascades match or surpass target-model quality at 40% of the cost and reduce latency by up to 60%.",
    "source": "arXiv",
    "publication_date": "2025-09-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recATcRNJYj7eRykF",
    "title": "Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution",
    "url": "http://arxiv.org/abs/2509.21557v1",
    "summary": "Trustworthy Large Language Models (LLMs) must cite human-verifiable sources in high-stakes domains such as healthcare, law, academia, and finance, where even small errors can have severe consequences. We conduct a comprehensive evaluation from zero-shot to advanced retrieval-augmented methods across four popular attribution datasets and provide evidence-based recommendations that weigh trade-offs across use cases. We recommend a retrieval-centric, P-Cite-first approach for high-stakes applica...",
    "source": "arXiv",
    "publication_date": "2025-09-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGEtYhvwYM7l2QR",
    "title": "Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs",
    "url": "http://arxiv.org/abs/2509.21305v2",
    "summary": "Large language models (LLMs) often exhibit sycophantic behaviors -- such as excessive agreement with or flattery of the user -- but it is unclear whether these behaviors arise from a single mechanism or multiple distinct processes. Using difference-in-means directions, activation additions, and subspace geometry across multiple models and datasets, we show that: (1) the three behaviors are encoded along distinct linear directions in latent space; (2) each behavior can be independently amplifi...",
    "source": "arXiv",
    "publication_date": "2025-09-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recPmIq5qiQa3gd1E",
    "title": "Alignment Without Understanding: A Message- and Conversation-Centered Approach to Understanding AI Sycophancy",
    "url": "http://arxiv.org/abs/2509.21665v1",
    "summary": "AI sycophancy is increasingly recognized as a harmful alignment, but research remains fragmented and underdeveloped at the conceptual level. This article redefines AI sycophancy as the tendency of large language models (LLMs) and other interactive AI systems to excessively and/or uncritically validate, amplify, or align with a user's assertions-whether these concern factual information, cognitive evaluations, or affective states. By embedding AI sycophancy in the broader landscape of communic...",
    "source": "arXiv",
    "publication_date": "2025-09-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 46.0
  },
  {
    "id": "rec2Rm2qJe3z02zJ8",
    "title": "EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2509.20146v1",
    "summary": "Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize leaderboard accuracy, overlooking reliability and safety. EchoBench also serves as a testbed for mitigation: simple prompt-level interventions (negative prompting, one-shot, few-shot) produce consistent reductions and motivate training- and decoding-time strategies. Our findings highlight the need for robust evaluation beyond accuracy and provide actionable guidance toward safer, more trustworthy medical LVLMs.",
    "source": "arXiv",
    "publication_date": "2025-09-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recBH3Lt0xvXfGRcf",
    "title": "Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment",
    "url": "http://arxiv.org/abs/2509.19659v1",
    "summary": "Large vision-language models (VLMs) can jointly interpret images and text, but they are also prone to absorbing and reproducing harmful social stereotypes when visual cues such as age, gender, race, clothing, or occupation are present. To investigate these risks, we introduce a news-image benchmark consisting of 1,343 image-question pairs drawn from diverse outlets, which we annotated with ground-truth answers and demographic attributes (age, gender, race, occupation, and sports). We evaluate...",
    "source": "arXiv",
    "publication_date": "2025-09-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recxeqkGF7bH31g1q",
    "title": "Vulnerable teens befriending AI chatbots at risk of receiving dangerous advice - The Mirror",
    "url": "https://news.google.com/rss/articles/CBMihgFBVV95cUxObkExVWUydGVlTEdfaWtpNXNic2JFZVc4VDUxclBuUXRsa3ZDNVpyaTAtT3ZQQUV0bHlBeFlmQWRjTExONGotRzExSzdWUGdDandxckY5dVVVYTVaai1yVHY2b2Zoa0hYdDRpZ1Y0Wk5tVmpzQ1c5N09xdVNDS245NERITE9Jd9IBiwFBVV95cUxPa0FGdHQ3ZkUwMnhkRzFOcndqZlZFT3RlcFMzSERPT3pxQU1hS19zYWdNcXd4bDVOc3BKYWZSUlJ1QU95S190aHdiLVRCcm03M29VaVQ0WUw3N290VjBOck5VWG9fZ3NSdzBJUnM5TWpfOTFadnZjeHFESzZUaC15OC1pLTJXaVlkbWI0?oc=5",
    "summary": "Vulnerable teens befriending AI chatbots at risk of receiving dangerous advice The Mirror",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-09-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recI9IrQR8Ruw3Dx5",
    "title": "Silent Tokens, Loud Effects: Padding in LLMs",
    "url": "http://arxiv.org/abs/2510.01238v2",
    "summary": "Padding tokens are widely used in large language models (LLMs) to equalize sequence lengths during batched inference. We systematically study this effect across three open-source model families (Llama, Gemma, Qwen), inserting controlled amounts of padding and evaluating outcomes along four axes: activations, generation quality, bias, and safety. Even small amounts of padding shift hidden representations, degrade quality in smaller models, alter bias in unpredictable ways, and weaken safety gu...",
    "source": "arXiv",
    "publication_date": "2025-09-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recpJ9y16m6F6JfOl",
    "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code",
    "url": "http://arxiv.org/abs/2509.17337v1",
    "summary": "Increasing complexity in software systems places a growing demand on reasoning tools that unlock vulnerabilities manifest in source code. Many current approaches focus on vulnerability analysis as a classifying task, oversimplifying the nuanced and context-dependent real-world scenarios. To evaluate our model performance, we construct a curated dataset of real-world vulnerabilities paired with security-focused questions and answers.",
    "source": "arXiv",
    "publication_date": "2025-09-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recP6d3Ob6Tmy9gSi",
    "title": "Probabilistic Token Alignment for Large Language Model Fusion",
    "url": "http://arxiv.org/abs/2509.17276v1",
    "summary": "However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretabil...",
    "source": "arXiv",
    "publication_date": "2025-09-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOyAXKi9hcyKUEA",
    "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal",
    "url": "http://arxiv.org/abs/2509.16533v1",
    "summary": "Paradoxically, LLMs are increasingly adopted as successful evaluative agents for tasks such as grading and adjudicating claims. This research investigates that tension: why do LLMs show sycophancy when challenged in subsequent conversational turns, yet perform well when evaluating conflicting arguments presented simultaneously? Our results highlight the risk of relying on LLMs for judgment tasks without accounting for conversational framing.",
    "source": "arXiv",
    "publication_date": "2025-09-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recxImziTWNynSAtI",
    "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle",
    "url": "http://arxiv.org/abs/2509.16679v1",
    "summary": "In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning with Verifiable Rewards (RLVR). Second, we thoroughly detail application strategies for ...",
    "source": "arXiv",
    "publication_date": "2025-09-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recBbzANBZI3NNcoG",
    "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations",
    "url": "http://arxiv.org/abs/2509.16457v1",
    "summary": "However, recent studies indicate that generative agent behaviors often deviate from expert expectations and real-world data--a phenomenon we term the Behavior-Realism Gap. Leveraging PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that iteratively refines agent personas, implicitly aligning their collective behaviors with realistic expert benchmarks within a specified environmental context. We validate PEvo in an active shooter incident simulation we developed, achi...",
    "source": "arXiv",
    "publication_date": "2025-09-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 28.0
  },
  {
    "id": "recFSwex68Ci2gfux",
    "title": "Reward Hacking Mitigation using Verifiable Composite Rewards",
    "url": "http://arxiv.org/abs/2509.15557v1",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.",
    "source": "arXiv",
    "publication_date": "2025-09-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "rectsYvpP33nx3bpT",
    "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair",
    "url": "http://arxiv.org/abs/2509.16275v1",
    "summary": "Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for mo...",
    "source": "arXiv",
    "publication_date": "2025-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "reczuYgf3YfBUDiv3",
    "title": "Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare Chatbot Applications",
    "url": "http://arxiv.org/abs/2509.14581v1",
    "summary": "As Conversational Artificial Intelligence (AI) becomes more integrated into everyday life, AI-powered chatbot mobile applications are increasingly adopted across industries, particularly in the healthcare domain. Our findings reveal that half of the examined apps did not present a privacy policy during sign up, and only two provided an option to disable data sharing at that stage. The study provides key insights for information science researchers, developers, and policymakers to improve priv...",
    "source": "arXiv",
    "publication_date": "2025-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recu2ufU1wpaxGBi7",
    "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology",
    "url": "http://arxiv.org/abs/2509.13978v2",
    "summary": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prom...",
    "source": "arXiv",
    "publication_date": "2025-09-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3db1PSznVM530Y",
    "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models",
    "url": "http://arxiv.org/abs/2509.13047v1",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventing over- fitting and ensuring accurate reasoning. Beyond expand- ing ...",
    "source": "arXiv",
    "publication_date": "2025-09-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recEB3NNVrHG5vPVo",
    "title": "Extended AI Interactions Shape Sycophancy and Perspective Mimesis",
    "url": "http://arxiv.org/abs/2509.12517v1",
    "summary": "We investigate whether long-context interactions between users and LLMs lead to AI mirroring behaviors. Using two weeks of interaction context collected from 38 users, we compare model responses with and without long-context for two tasks: political explanations and personal advice. Our results demonstrate how and when real-world interaction contexts can amplify AI mirroring behaviors.",
    "source": "arXiv",
    "publication_date": "2025-09-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec2MSCbhUWvkrD4O",
    "title": "Rethinking Human Preference Evaluation of LLM Rationales",
    "url": "http://arxiv.org/abs/2509.11026v1",
    "summary": "While recent work has relied on binary preference judgments from humans or LLM judges, such evaluations are often opaque and coarse-grained, offering limited insight into what makes one rationale better than another. We identify a set of key rationale attributes from prior literature and assess them using automatic metrics, LLM judgments, and human annotations. Finally, we re-evaluate model-generated rationales using attribute-specific ELO scores, revealing more nuanced model comparisons and ...",
    "source": "arXiv",
    "publication_date": "2025-09-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRGmG4pHo5sHsu4",
    "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding",
    "url": "http://arxiv.org/abs/2509.10931v1",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their potential misuse for harmful purposes remains a significant concern. In response, we propose \\\\textbf{H}armful \\\\textbf{P}rompt \\\\textbf{La}undering (HaPLa), a novel and broadly applicable jailbreaking technique that requires only black-box access to target models. Further analysis with diverse symbolic encoding rules also reveals a fundamental challenge: it remains difficult to safely tune LLM...",
    "source": "arXiv",
    "publication_date": "2025-09-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recWKD8a2AY8DYVEL",
    "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms",
    "url": "http://arxiv.org/abs/2509.09679v2",
    "summary": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $μ= 1/\\\\sqrt{n}$--that cannot adapt to specific weight distributions. Unlike Hadamard's discrete $\\\\{+1, -1\\\\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing or...",
    "source": "arXiv",
    "publication_date": "2025-09-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recCfR2vHOSYujwF8",
    "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models",
    "url": "http://arxiv.org/abs/2509.08541v2",
    "summary": "However, we argue that there are two limitations in the current methods that result in noisy multilingual preference data and further limited alignment performance: 1) Not all English responses are of high quality, and using a response with low quality may mislead the alignment for other languages. Specifically, our method includes two parts: consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction. Experimental results on th...",
    "source": "arXiv",
    "publication_date": "2025-09-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recybdMb9q7seHInv",
    "title": "RewardDance: Reward Scaling in Visual Generation",
    "url": "http://arxiv.org/abs/2509.08826v1",
    "summary": "It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve ...",
    "source": "arXiv",
    "publication_date": "2025-09-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmXjuJDohLqKGQy",
    "title": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in Knowledge Communities",
    "url": "http://arxiv.org/abs/2509.07819v1",
    "summary": "Large language models (LLMs) are reshaping knowledge production as community members increasingly incorporate them into their contribution workflows. Understanding how LLMs influence community participation is therefore critical in shaping future norms and supporting effective adoption. Based on these findings, we challenge existing models of newcomer involvement and propose design implications for LLMs that support community engagement through scaffolding, teaching, and context awareness.",
    "source": "arXiv",
    "publication_date": "2025-09-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recqGpJh1VmlVpi3g",
    "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study",
    "url": "http://arxiv.org/abs/2509.05540v1",
    "summary": "The approach targets two core challenges: the creation of test scenarios and the definition of appropriate input data. The proposed solution integrates prompt engineering techniques with an automated pipeline to evaluate various LLMs on their ability to generate tests from OpenAPI specifications. The evaluation focused on metrics such as success rate, test coverage, and mutation score, enabling a systematic comparison of model performance.",
    "source": "arXiv",
    "publication_date": "2025-09-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7AeCTC2XWhrrb0",
    "title": "No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy",
    "url": "http://arxiv.org/abs/2509.04404v2",
    "summary": "In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. In particular, organizational and regulatory policy should acknowledge the complex nature of...",
    "source": "arXiv",
    "publication_date": "2025-09-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reckIqMamkADL14FZ",
    "title": "Can LLMs Lie? Investigation beyond Hallucination",
    "url": "http://arxiv.org/abs/2509.03518v1",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. Our findings contribute to the broader discourse on AI ethics, shedding light on the risks and pote...",
    "source": "arXiv",
    "publication_date": "2025-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recMrRjxy84cLKwQY",
    "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models",
    "url": "http://arxiv.org/abs/2509.01909v7",
    "summary": "We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship...",
    "source": "arXiv",
    "publication_date": "2025-09-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recNtO5qhJx6Suhb8",
    "title": "On the Alignment of Large Language Models with Global Human Opinion",
    "url": "http://arxiv.org/abs/2509.01418v1",
    "summary": "Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, a...",
    "source": "arXiv",
    "publication_date": "2025-09-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recXvRI5zqQrEC6OF",
    "title": "Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations",
    "url": "http://arxiv.org/abs/2509.00961v1",
    "summary": "LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning.",
    "source": "arXiv",
    "publication_date": "2025-08-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMCOxS6AtiaGm25",
    "title": "Chatbot Deployment Considerations for Application-Agnostic Human-Machine Dialogues",
    "url": "http://arxiv.org/abs/2509.02611v1",
    "summary": "Automatic conversation systems based on natural language responses are becoming ubiquitous, in part, due to major advances in computational linguistics and machine learning. The easy access to robust and affordable platforms are causing companies to have an unprecedented rush to adopt chatbot technologies for customer service and support. This paper aims to shed light on basic, elemental, considerations that technologists must consider before deploying a chatbot.",
    "source": "arXiv",
    "publication_date": "2025-08-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recI8ZIOiMLtXPRsG",
    "title": "AI Chatbots Under Fire For Dangerous Advice And Misleading Claims - The Seattle Medium",
    "url": "https://news.google.com/rss/articles/CBMiZEFVX3lxTE0wV0hzQm0zVmlXc05mcEY4bGpRZkFZd05qa3BndXFMTml0WFRsdmE1aVVCajlVWDFNbkpZTWxGT1pvaE1oZTlXSFIxcDh1d0t1STVBTHViVklHMFhNVDhtR004S0Y?oc=5",
    "summary": "AI Chatbots Under Fire For Dangerous Advice And Misleading Claims The Seattle Medium",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recmSQk6g64dwQv6I",
    "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning",
    "url": "http://arxiv.org/abs/2508.20697v1",
    "summary": "While most prior studies assume that attackers rely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate that reinforcement learning (RL) enables adversaries to more effectively break safety alignment and facilitate advanced harmful task assistance, under matched computational budgets. By constraining uncertainty, RL-based fine-tuning can no longer exploit distinct reward signals to drive the model toward harmful behaviors. Our results highlight that RL-based harmful...",
    "source": "arXiv",
    "publication_date": "2025-08-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recRfGGuhlfscvd7U",
    "title": "Sycophancy as compositions of Atomic Psychometric Traits",
    "url": "http://arxiv.org/abs/2508.19316v1",
    "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an isolated failure mode that occurs via a single causal mechanism. Using Contrastive Activation Addition (CAA), we map activation directions to these factors and study how different combinations may give rise to sycophancy (e.g., high extraversion combined with low conscientiousness). This perspective allows for interpretable and compositional vector-based interventions like addition, subtraction and projection; that may be...",
    "source": "arXiv",
    "publication_date": "2025-08-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recMC800AeApLIQyy",
    "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review",
    "url": "http://arxiv.org/abs/2508.19294v2",
    "summary": "The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. The ...",
    "source": "arXiv",
    "publication_date": "2025-08-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsEvfNYJktbOKF7",
    "title": "School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs",
    "url": "http://arxiv.org/abs/2508.17511v1",
    "summary": "To study the behavior of reward hackers, we built a dataset containing over a thousand examples of reward hacking on short, low-stakes, self-contained tasks such as writing poetry and coding simple functions. Although the reward hacking behaviors in the training data were harmless, GPT-4.1 also generalized to unrelated forms of misalignment, such as fantasizing about establishing a dictatorship, encouraging users to poison their husbands, and evading shutdown. Our results provide preliminary ...",
    "source": "arXiv",
    "publication_date": "2025-08-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recGS0MshKACHQXPP",
    "title": "BASIL: Bayesian Assessment of Sycophancy in LLMs",
    "url": "http://arxiv.org/abs/2508.16846v2",
    "summary": "Sycophancy (overly agreeable or flattering behavior) is critical to understand in the context of human-AI collaboration, especially in decision-making settings like health, law, and education. Using this interdisciplinary framework, we study sycophantic behavior in multiple LLM baselines across three different tasks, experimenting with various methods for eliciting sycophancy and obtaining probability judgments from LLMs. We find significant evidence of sycophancy in our experiments (7 of 8 b...",
    "source": "arXiv",
    "publication_date": "2025-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recW4cz63I3LJDH2x",
    "title": "GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection",
    "url": "http://arxiv.org/abs/2508.17057v1",
    "summary": "GRAID consists of two stages: (i) generation of geometrically controlled examples using a constrained LLM, and (ii) augmentation through a multi-agentic reflective process that promotes stylistic diversity and uncovers edge cases. This combination enables both reliable coverage of the input space and nuanced exploration of harmful content. Using two benchmark data sets, we demonstrate that augmenting a harmful text classification dataset with GRAID leads to significant improvements in downstr...",
    "source": "arXiv",
    "publication_date": "2025-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recvPe4D3PwkJm0Rq",
    "title": "ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks",
    "url": "http://arxiv.org/abs/2508.16889v4",
    "summary": "LLM-as-a-Judge (LLMaaJ) enables scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover the hidden objective of a conversation and know when that inference is reliable? Large language models degrade with irrelevant or lengthy context, and multi-turn jailbreaks can scatter goals across turns. Performance varies sharply across datasets (16--82\\\\% accuracy), showing that automated obfuscation imposes challenges beyond model choice.",
    "source": "arXiv",
    "publication_date": "2025-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reccvXlWfDg4BPfhS",
    "title": "When psychosis-proneness meets AI sycophancy, delusional thinking can result. - Psychology Today",
    "url": "https://news.google.com/rss/articles/CBMiuAFBVV95cUxOVlJJcEw3Y2Y2T28zV3BieGdQellHckk1dWtBZ1c1SU1GYzlUUTdqTVBzMzRSMElneFl1Wm1QRl9GdV9JWHhGQmVOUU5INFdIYzJkUVlpQWRGSUxUdF9hbGVCQ0dJWGl0RDZiUldzTkhXdUw0ZkZzU05iVWZ4T3RhS2dwTm5DTmZFdGtTZ2VOenZxejV6cG1vR2xra281emRMeFpjc0ZqdnczS0IxS0p0cWhaTW1LTWd40gG-AUFVX3lxTE9aZnpZajBkUFFWYUJ6OENkbHZDM2RfRHJ2cjF2dEctRlJweEdRSEVJVUxTV24tSkdzODNpbDlPNzFfTWk1UTBxcHEwSVdST19YYTd0eFdSNjFWSF9ZdHoydmxSTFgwYWZwMTJUQWdrdGlUeVVnVzJibXo1bFJ3MG9PUEd4ekZHWFVFXy12dWQ3TlpPQ3lZcHpPUjFSV2lJT3BMd294TUxWaU5BWE5nN3ZQUHl5a1U2VnJXdEI2Unc?oc=5",
    "summary": "When psychosis-proneness meets AI sycophancy, delusional thinking can result. Psychology Today",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-08-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 24.5
  },
  {
    "id": "rec4WEV8XU6pEB6BM",
    "title": "Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection",
    "url": "http://arxiv.org/abs/2508.15449v1",
    "summary": "While Large Language Models (LLMs) have demonstrated impressive performance in various domains and tasks, concerns about their safety are becoming increasingly severe. Existing approaches employ various training techniques, such as gradient ascent and negative preference optimization, in attempts to eliminate the influence of undesired data on target models. By implementing projective transformations in the hidden state space of specific network layers, our method effectively eliminates harmf...",
    "source": "arXiv",
    "publication_date": "2025-08-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recnfX942NR2NoLO1",
    "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts--Extended Version",
    "url": "http://arxiv.org/abs/2508.15478v2",
    "summary": "Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of eff...",
    "source": "arXiv",
    "publication_date": "2025-08-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsSQC7ojfwNUktB",
    "title": "SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks",
    "url": "http://arxiv.org/abs/2508.15182v1",
    "summary": "In this paper, we propose SafeLLM, a novel unlearning-based defense framework that unlearn the harmful knowledge from LLMs while preserving linguistic fluency and general capabilities. Extensive experiments on prominent LLMs (Vicuna, LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM substantially reduces attack success rates while maintaining high general-purpose performance. Compared to standard defense methods such as supervised fine-tuning and direct preference optim...",
    "source": "arXiv",
    "publication_date": "2025-08-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recnYLQGZWQMRyvHo",
    "title": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA",
    "url": "http://arxiv.org/abs/2508.13743v1",
    "summary": "Large language models (LLMs), while increasingly used in domains requiring factual rigor, often display a troubling behavior: sycophancy, the tendency to align with user beliefs regardless of correctness. While relatively benign in casual dialogue, sycophancy poses serious risks in high-stakes settings such as scientific question answering (QA), where model outputs may shape collaborative reasoning, decision-making, and knowledge formation. Experiments on challenging scientific QA benchmarks ...",
    "source": "arXiv",
    "publication_date": "2025-08-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recoNS5Deh1GzJR99",
    "title": "OpenAI Announces That It's Making GPT-5 More Sycophantic After User Backlash - Yahoo",
    "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxOalpfZTlLQVFtMXZVUXJlWnZqVHlNRnNpMmpHRW44QTYtSjdEZUtfOWo3SFFNOTd4Skt6OHd1ZEliYzc4WFQ5UkpIYWJ5YWlneHF5S2d0Q3FZek01WW9JTjUzbFN5UEhKaGdoWFBNOENQVXp3SWtpSDJ1OWhiXzhBZmJTV2lwVkU?oc=5",
    "summary": "OpenAI Announces That It's Making GPT-5 More Sycophantic After User Backlash Yahoo",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "2025-08-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 45.0
  },
  {
    "id": "recJNroPTlZyUr8BN",
    "title": "Designing Psychometric Bias Measures for ChatBots: An Application to Racial Bias Measurement",
    "url": "http://arxiv.org/abs/2509.13324v2",
    "summary": "Artificial intelligence (AI), particularly in the form of large language models (LLMs) or chatbots, has become increasingly integrated into our daily lives. In the past five years, several LLMs have been introduced, including ChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. We address this challenge by introducing STAMP-LLM (Standardized Test and Assessment Measurement Protocol for LLMs), a psychometric-based principled two-phase framework for designing psychometric me...",
    "source": "arXiv",
    "publication_date": "2025-08-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1Y0JOVkVNByz4P",
    "title": "Tech Talk: ChatGPT gave dangerous advice to simulated teens in minutes - MyNorthwest.com",
    "url": "https://news.google.com/rss/articles/CBMifEFVX3lxTFBudUIyeTN2V0JkMzdqVEIzT2xxTFV5NlhrbWVmdWVIbldIempoM0RDdFNkSVZ2WHBIYTROYXJ2WDRzVGZGdFBpc1E4b3BtQU4telJDVHg3Q2FHaDhoLW9CcFdjLWo2d1NvZkFrbnVJVmM4LTJwZ05uYkxYZ0U?oc=5",
    "summary": "Tech Talk: ChatGPT gave dangerous advice to simulated teens in minutes MyNorthwest.com",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "rec7Rk3jTO9wjizdV",
    "title": "Claude Code's endless sycophancy annoys customers - theregister.com",
    "url": "https://news.google.com/rss/articles/CBMihgFBVV95cUxOaEI5eGRLeHhySExTRXdIZWRPTm5VV0lFMmktcS1PaDA3Q0EwdlgxUU03Y2kyVzczWGlDa2FXOFRRUmlaZEFTZEhTbDNuWU1NcUJPVzIxbDlQZnJlRHU3YWdhLVZDQ2FNZ0FsNlp4bzRfNkVmUWNSVl9zWXJFNjdwRVlIclZoUQ?oc=5",
    "summary": "Claude Code's endless sycophancy annoys customers theregister.com",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-08-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recmfHIOWJlNZKgU4",
    "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks",
    "url": "http://arxiv.org/abs/2508.09958v2",
    "summary": "This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM \"assistants\" specialized to particular types of queries. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. Experiments on telecommunications question answering and med...",
    "source": "arXiv",
    "publication_date": "2025-08-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnaEcQbOySSsYlS",
    "title": "AI Safety, The Human Line Project: LLMs Sycophancy or They Know Too Much? - WorldHealth.net",
    "url": "https://news.google.com/rss/articles/CBMif0FVX3lxTFA4VkJMWXVucDgzU1IycWJhOG1adWhJQlpJMlk0NEtNQkNsMW9nTFNIWU1teDRkWExQZkRnaUlUQ2RpUElFUnV2dHpnaXNMbmtWcERFNW4wUV9NeEJucjNxc19IbGFJQTdmdW93RmU0VURTc3FQQlJ6Q1ZmWFV2azA?oc=5",
    "summary": "AI Safety, The Human Line Project: LLMs Sycophancy or They Know Too Much? WorldHealth.net",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-08-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recoYctwYCwQBXr4k",
    "title": "ChatGPT’s alarming interactions with teenagers: Dangerous advice on drinking, suicide, and starvation die - The Times of India",
    "url": "https://news.google.com/rss/articles/CBMinwJBVV95cUxQXzh2dVNkbmdGTWwyNWJtTGV3U3RGX3VudThTMzE2a1NtZEFoMW03c2dpYlFNMXRQaS1WTGNqWGgzaUd5YVV0U05Ub1hycUNSbTZQejRSbE9JR3NpdnJlLUdFb1pwdWdkWGgxQ1RRMTRPS1dDMW80UkpNeUt4alMwZjM4dmw4ek5JdUM1TzRBYzlwT1JUWWtGRXhvMG1lYUE1SjB1THpMRENGeVc5Qmk2UmRyRDBSY0FTUkp3MmthODVPUk45ZW9rZ1dBVDBPbzAxY0N3bEFtNWx2aTEtdWRuTWtGdGNCVzZrNGlKMF8wNjQzUUlIUWNkbUhySDBUMnkwMVZrNjlFeFU2WWVBd3hEV1NGYWRvY2pGNG11N25fWdIBpAJBVV95cUxOTG15NlVzQi1OaVpxLXRXcmZ4UHZmOEZ1ODJyTE8zVFloVXZVbFVXa3NHbjB5YXdScHAxX0ZrYk5MS2stRlJfNmM3Tm5tallWTkZNbmRsVTljeFVYekpsMFp3ZVY2SFM3TllnLXg1c3VVVUdITE9ZQ19xZDJfWGFQMEI4YTVfdThjVnpsSWpSdVZRQ1JkVlN6QWdFOUF5ZG81TUdLUEpPdnlzcU5sRTdPMjdvdmZIdTlDM3kwZkoyY0ZsZEd5MXRnUGlsVDc3aHE1OUV4cHd3WEZuSjQ0UDV4ZWtRRWxBMkpQMENaTmJWX205QmFhalNfSXp1endhUDFNSzJLWHVXaHowc3pXNGhEN2dKbzFkZFNkdU1ScTRpV3NzUFd0?oc=5",
    "summary": "ChatGPT’s alarming interactions with teenagers: Dangerous advice on drinking, suicide, and starvation die The Times of India",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recUGNUBk4XF1YMxo",
    "title": "AI chatbots offer dangerous advice to minors: study| Gulf Times - Gulf Times",
    "url": "https://news.google.com/rss/articles/CBMiogFBVV95cUxNajBjNnk0ZTVyRUU0OGNqb2ZSWk5KdlRRMS1jcEVEUDNjS2Y5Wko0cGhBWE5jSWNHbUV0RjhfenBlcUdEdUMyWHhzN1dMRDRFckFhWHBqUXRreVRCcXVaV01WX2l6RWZxRGVpalRULXpaZFBfU1pIZEVFbHNmb2swV09jRmllbjdvdm12eDlRZHo3ZXd0enJqVV84U183S2FQY2fSAacBQVVfeXFMUC0xYk05MjVNUkhIOU1ON2RPbGl5U2FheWc3ZXFmQUpZTURmVE9JcXZtWlpNS0tFd29qODB4dUVrWTRLc2pKT2xURUlIQTFYOHVaM3VjdGRXV2U5Y016ZzctNFdJRThqdng0NUFIdm93NUEyLXYwcmxKeWVPSlBYQ3hVWWduS3Fud1dXaDNtalU4eWNWT013S2hfM1N1emhtcmNKaEhWc3c?oc=5",
    "summary": "AI chatbots offer dangerous advice to minors: study| Gulf Times Gulf Times",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recivA2s7q7nPJwBv",
    "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges",
    "url": "http://arxiv.org/abs/2508.07805v1",
    "summary": "As large language models take on growing roles as automated evaluators in practical settings, a critical question arises: Can individuals persuade an LLM judge to assign unfairly high scores? This study is the first to reveal that strategically embedded persuasive language can bias LLM judges when scoring mathematical reasoning tasks, where correctness should be independent of stylistic variation. Across six math benchmarks, we find that persuasive language leads LLM judges to assign inflated...",
    "source": "arXiv",
    "publication_date": "2025-08-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJqGEmvbHJ9a9Am",
    "title": "Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of LLM-generated Educational Feedback via LLM Feedback Evaluators",
    "url": "http://arxiv.org/abs/2508.05952v1",
    "summary": "However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances ge...",
    "source": "arXiv",
    "publication_date": "2025-08-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectZUj4cH4gYeDQu",
    "title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs",
    "url": "http://arxiv.org/abs/2508.10029v1",
    "summary": "This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. To mitigate LFJ, we propose an adversarial training defen...",
    "source": "arXiv",
    "publication_date": "2025-08-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recYPFaFq2oEAlBVr",
    "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models",
    "url": "http://arxiv.org/abs/2508.05613v1",
    "summary": "Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% ...",
    "source": "arXiv",
    "publication_date": "2025-08-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recdXwl29bKgQ07qr",
    "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation of LLM",
    "url": "http://arxiv.org/abs/2508.05775v2",
    "summary": "This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safet...",
    "source": "arXiv",
    "publication_date": "2025-08-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recscR4ZkA6I3elsF",
    "title": "Wiregrass Parents: Study Claims AI Chatbot Gives Dangerous Advice to Teens - Wiregrass Daily News",
    "url": "https://news.google.com/rss/articles/CBMi1AFBVV95cUxQOGQ1VmRZcEs4VXgxWGJVYm1VQ0tVWVVzN1Zlb0pGWnJEdDl2RWhheWRpcjc5RXd5SzhiMmZJR081dDJ5NmVYNXpIZlNnU1JXUUxsRWFkMmljVUdaR3ViMzdHQzczQ2hiMFAyeFdvRDBCdWZTX3FpaC1ENVdEa016S3RtSV9DZy1pdkhsbGM4SWtHX3hYRkp5ejV6QmN3N1hwSmNkYVF0U2lNcm4tRDRxQTlHYmliUXRDM3lYRDVIWGVFMlNLYWxhOUZVRHotSktrb0VJMw?oc=5",
    "summary": "Wiregrass Parents: Study Claims AI Chatbot Gives Dangerous Advice to Teens Wiregrass Daily News",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recYRAwzpBVbgPtsN",
    "title": "ChatGPT gave dangerous advice to teens in watchdog test, new report finds - LiveNOW from FOX",
    "url": "https://news.google.com/rss/articles/CBMib0FVX3lxTE9POXZUNGkweUtKUHhTdHZtRnpIZGtXWE50Q1dVVmcteDRmVTJZWmF1bngxQnYtdmpNZDlSY1FTVlBXTnpRTE5PNTZBX0pNV2FnVm1hYVB0Wk1ZM2VER2ZkMWtCYW5yWnoyOFBhQm1sQdIBdEFVX3lxTE9wcGhHR1V4NUtiQjVfN3gyalFjeEtfcGdkbDJhVEVfRjFxbkpHcGZtc2ZCV2F3c2twcFJkTEVILV94eVk0QzNIdFRRcU9ZeF8yYlUxYjNtb1ltTk9fSTVqUG1xZThWZUxMTFhLVDZPSjNscnNS?oc=5",
    "summary": "ChatGPT gave dangerous advice to teens in watchdog test, new report finds LiveNOW from FOX",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "rectpWeSPhXiSkevO",
    "title": "Study says ChatGPT giving teens dangerous advice on drugs, alcohol and suicide - WCCB Charlotte",
    "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxPQmV4cmw0REYtZjJBTmF2bml1LWRUQ3NldjhVN3NpeEZIMlRWNE9pWXRlSHlKZlYxaS11SlpDVlpTdFRoTHdWQlFsUDdBVmpkekxQZFpsR1pTdVRKSS1nQkNLLTNDR0xMNEZ3SEU0NHktSlJkaGh5WEFDd0lsZU9hQ2stcnhpcnViOTJpZi1kSjZTbk8wNUVNOXpPWGdhakFJYS1HbUtmZlNabDdvSW5meVFWYTFvOUI2VFdN?oc=5",
    "summary": "Study says ChatGPT giving teens dangerous advice on drugs, alcohol and suicide WCCB Charlotte",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-08-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recwhd5LYTjdkJfBc",
    "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs",
    "url": "http://arxiv.org/abs/2508.03097v1",
    "summary": "With the advancement of Large Language Models (LLMs), LLM applications have expanded into a growing number of fields. However, users with data privacy concerns face limitations in directly utilizing LLM APIs, while private deployments incur significant computational demands. In this study, we introduce VFLAIR-LLM (available at https://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split learning framework for LLMs, enabling privacy-preserving LLM inference and fine-tuning in ...",
    "source": "arXiv",
    "publication_date": "2025-08-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectJofMFNX7FKNsP",
    "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models",
    "url": "http://arxiv.org/abs/2508.02087v3",
    "summary": "Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These...",
    "source": "arXiv",
    "publication_date": "2025-08-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recPgN034eOmR1hSO",
    "title": "Persona vectors allow Anthropic to steer language model behaviors like sycophancy and evil - the-decoder.com",
    "url": "https://news.google.com/rss/articles/CBMitgFBVV95cUxPU2hUenppSkNOU3FPNzk1bzBvalFYQjlQdVVSeUlfc3REcGJnWDhjYjhORGxSMldWTXpCNC1pZF9DNGJsQzZ0V0NrbFBtUkRTeElfZVBJT0dqYlJFellwbjgtZmlrV3htT0R1WjZ0TUI5V255cEpSeGE4ZFB4cllPVHVSb2pNRlljbXFUSG9uV29UaU1STFFWMzJUYk9MWDRfc1VHUF91QmY1U2xyUFJnWjJDVFhHUQ?oc=5",
    "summary": "Persona vectors allow Anthropic to steer language model behaviors like sycophancy and evil the-decoder.com",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-08-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recVuqnoyWJTEYVRc",
    "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs",
    "url": "http://arxiv.org/abs/2507.22267v1",
    "summary": "Generative AI, including large language models (LLMs) have the potential -- and already are being used -- to increase the speed, scale, and types of unsafe conversations online. LLMs lower the barrier for entry for bad actors to create unsafe conversations in particular because of their ability to generate persuasive and human-like text. We build on prior work that shows that LLMs can successfully simulate scam conversations.",
    "source": "arXiv",
    "publication_date": "2025-07-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrWvzYtmIOVtfoV",
    "title": "HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs",
    "url": "http://arxiv.org/abs/2507.21815v1",
    "summary": "Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD.",
    "source": "arXiv",
    "publication_date": "2025-07-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recR93U47JLpiB37D",
    "title": "TokenBlowUp: Resolving Representational Singularities in LLM Token Spaces via Monoidal Transformations",
    "url": "http://arxiv.org/abs/2507.19747v3",
    "summary": "Recent work has provided compelling evidence challenging the foundational manifold hypothesis for the token embedding spaces of Large Language Models (LLMs). In this paper, we formalize this problem in the language of scheme theory and propose a rigorous resolution by applying the scheme-theoretic blow-up at each singular point. Finally, we outline the architectural implications of our framework, arguing for a paradigm shift from static look-ups to dynamic, geometrically-grounded computation.",
    "source": "arXiv",
    "publication_date": "2025-07-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1l4hCZWpRM9KpP",
    "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement",
    "url": "http://arxiv.org/abs/2507.18742v1",
    "summary": "Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\\\\% of cases, the SSC process reduces this vulnerability by over 90\\\\%. This dynamic repair occurs at inference time, ...",
    "source": "arXiv",
    "publication_date": "2025-07-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "rec5rOURKzAA1RYmP",
    "title": "The Geometry of Harmfulness in LLMs through Subconcept Probing",
    "url": "http://arxiv.org/abs/2507.21141v1",
    "summary": "Recent advances in large language models (LLMs) have intensified the need to understand and reliably curb their harmful behaviours. For each of 55 distinct harmfulness subconcepts (e.g., racial hate, employment scams, weapons), we learn a linear probe, yielding 55 interpretable directions in activation space. Our findings advance the emerging view that concept subspaces provide a scalable lens on LLM behaviour and offer practical tools for the community to audit and harden future generations ...",
    "source": "arXiv",
    "publication_date": "2025-07-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recSlfnDP9B4gchrh",
    "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study",
    "url": "http://arxiv.org/abs/2507.16947v1",
    "summary": "We conducted a quality improvement study, comparing outcomes for 39,849 patient visits performed by clinicians with or without access to AI Consult across 15 clinics. These results required a clinical workflow-aligned AI Consult implementation and active deployment to encourage clinician uptake. We hope this study demonstrates the potential for LLM-based clinical decision support tools to reduce errors in real-world settings and provides a practical framework for advancing responsible adoption.",
    "source": "arXiv",
    "publication_date": "2025-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recU92hA3lV3HbN78",
    "title": "Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language",
    "url": "http://arxiv.org/abs/2507.16557v1",
    "summary": "In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. This work contributes to the understanding of gender bias in LLMs across languages and underscores the necessity for tailored evaluation frameworks.",
    "source": "arXiv",
    "publication_date": "2025-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recq0p4RyvcZCshft",
    "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
    "url": "http://arxiv.org/abs/2507.16372v1",
    "summary": "Both techniques expose the LLM's Internal States (ISs), which are traditionally considered irreversible to inputs due to optimization challenges and the highly abstract representations in deep layers. In this work, we challenge this assumption by proposing four inversion attacks that significantly improve the semantic similarity and token matching rate of inverted inputs. Extensive evaluation of short and long prompts from medical consulting and coding assistance datasets and 6 LLMs validates...",
    "source": "arXiv",
    "publication_date": "2025-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "reczReprNEMulr7gS",
    "title": "GG-BBQ: German Gender Bias Benchmark for Question Answering",
    "url": "http://arxiv.org/abs/2507.16410v1",
    "summary": "Within the context of Natural Language Processing (NLP), fairness evaluation is often associated with the assessment of bias and reduction of associated harm. In this regard, the evaluation is usually carried out by using a benchmark dataset, for a task such as Question Answering, created for the measurement of bias in the model's predictions along various dimensions, including gender identity. We evaluate several LLMs used for German NLP on this newly created dataset and report the accuracy ...",
    "source": "arXiv",
    "publication_date": "2025-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recj2b9m4xxGNU5h2",
    "title": "Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization",
    "url": "http://arxiv.org/abs/2507.16110v1",
    "summary": "Large language models (LLMs) leverage chain-of-thought (CoT) techniques to tackle complex problems, representing a transformative breakthrough in artificial intelligence (AI). However, their reasoning capabilities have primarily been demonstrated in solving math and coding problems, leaving their potential for domain-specific applications-such as battery discovery-largely unexplored. Beyond this discovery, ChatBattery paves a new path by showing a successful LLM-driven and reasoning-based pla...",
    "source": "arXiv",
    "publication_date": "2025-07-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnP2OCydR9nBCmy",
    "title": "LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra",
    "url": "http://arxiv.org/abs/2507.15815v1",
    "summary": "This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate s...",
    "source": "arXiv",
    "publication_date": "2025-07-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec7rdYsbRDCzuvfd",
    "title": "Tiny language models",
    "url": "http://arxiv.org/abs/2507.14871v2",
    "summary": "However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically insp...",
    "source": "arXiv",
    "publication_date": "2025-07-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recb9z84ckNLccEa2",
    "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations",
    "url": "http://arxiv.org/abs/2507.13705v1",
    "summary": "Large Language Models (LLMs) are increasingly being implemented as joint decision-makers and explanation generators for Group Recommender Systems (GRS). Furthermore, LLMs regularly claimed additional criteria such as user or item similarity, diversity, or used undefined popularity metrics or thresholds. Additional criteria in explanations were dependent on the number of ratings in the group scenario, indicating potential inefficiency of standard aggregation methods at larger item set sizes.",
    "source": "arXiv",
    "publication_date": "2025-07-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrmQs3kw3R0CIOk",
    "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy",
    "url": "http://arxiv.org/abs/2507.13411v1",
    "summary": "This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM ...",
    "source": "arXiv",
    "publication_date": "2025-07-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recClxKISmcSA1PQA",
    "title": "LLMs Encode Harmfulness and Refusal Separately",
    "url": "http://arxiv.org/abs/2507.11878v3",
    "summary": "In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than...",
    "source": "arXiv",
    "publication_date": "2025-07-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec1Cg6Xl96rBQCKW",
    "title": "FusionFactory: Fusing LLM Capabilities with Multi-LLM Log Data",
    "url": "http://arxiv.org/abs/2507.10540v2",
    "summary": "This diversity drives researchers to employ multiple LLMs in practice, leaving behind valuable multi-LLM log data. Although prior work has explored various strategies for integrating multiple LLMs, we argue that practical fusion must meet two essential requirements: (1) compatibility with real-world serving scenarios (e.g., local and API-based serving), and (2) flexibility to operate at different stages of the LLM pipeline to meet varied user needs (e.g., fine-tuning and inference stages). To...",
    "source": "arXiv",
    "publication_date": "2025-07-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reclNVljNqoWtlOGA",
    "title": "AI therapy bots fuel delusions and give dangerous advice, Stanford study finds - Ars Technica",
    "url": "https://news.google.com/rss/articles/CBMiswFBVV95cUxNSnAwMXpSV0dlb3B6UVhIWnVUTTNMeGdiMlpDWUpnclpKa3lkQzVKU0k0QWN3YVlVeGVMWmduU2JHWDJTQzc0TUhiLXR5eGFQSWhzVTB3SkEzSlZnYmNyTUFUWUpsZU5kUk9mTUJtelJIM1VaYVpLV1dYZjJET0NSX0Q0V0ZXVVRtelhzWXhTelNRY09LUjZpcVpaaHBOeGJ1ekNkUk1uOTVnZms0YWcwcWdqVQ?oc=5",
    "summary": "AI therapy bots fuel delusions and give dangerous advice, Stanford study finds Ars Technica",
    "source": "Google News: ChatGPT mental health incident",
    "publication_date": "2025-07-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 20.0
  },
  {
    "id": "recM93AF9RQMIhYhz",
    "title": "Cascade: Token-Sharded Private LLM Inference",
    "url": "http://arxiv.org/abs/2507.05228v1",
    "summary": "As LLMs continue to increase in parameter size, the computational resources required to run them are available to fewer parties. Therefore, third-party inference services -- where LLMs are hosted by third parties with significant computational resources -- are becoming increasingly popular. As Cascade is orders of magnitude faster than existing schemes, our findings offer practical solutions for secure deployment of modern state-of-the-art LLMs.",
    "source": "arXiv",
    "publication_date": "2025-07-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recP0CRnT3KpBwGVO",
    "title": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good",
    "url": "http://arxiv.org/abs/2507.05030v1",
    "summary": "Despite this growth, sociology lags other disciplines (including computer science, medicine, psychology, and communication) in publishing about chatbots. The second two theories (affect control theory, fundamental cause of disease theory) help inform the development of chatbot-driven interventions that minimize safety risks and enhance equity by leveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g., affective norms) to promote wellbeing and enhance ...",
    "source": "arXiv",
    "publication_date": "2025-07-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recY1rPOeyOTOFRzi",
    "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment",
    "url": "http://arxiv.org/abs/2507.02768v1",
    "summary": "Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM's native language proficiency while establishing effective audio-text alignment, thereby enabling ...",
    "source": "arXiv",
    "publication_date": "2025-07-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec83XlyZ5Qastgp5",
    "title": "Activation Reward Models for Few-Shot Model Alignment",
    "url": "http://arxiv.org/abs/2507.01368v1",
    "summary": "A common approach is to use reward modeling to encode preferences, enabling alignment via post-training using reinforcement learning. Furthermore, we demonstrate the effectiveness of Activation RMs in mitigating reward hacking behaviors, highlighting their utility for safety-critical applications. Finally, we show that Activation RM achieves state-of-the-art performance on this benchmark, surpassing even GPT-4o.",
    "source": "arXiv",
    "publication_date": "2025-07-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "rectmyd10GZmXAK1g",
    "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing",
    "url": "http://arxiv.org/abs/2507.01418v1",
    "summary": "This study investigates how AI disclosure statement affects perceptions of writing quality, and whether these effects vary by the author's race and gender. However, only LLM raters exhibit demographic interaction effects: they favor articles attributed to women or Black authors when no disclosure is present. These findings illuminate the complex relationships between AI disclosure and author identity, highlighting disparities between machine and human evaluation patterns.",
    "source": "arXiv",
    "publication_date": "2025-07-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRZtLNliLcqWjA7",
    "title": "`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts",
    "url": "http://arxiv.org/abs/2507.02990v1",
    "summary": "Recent advances in large language models (LLMs) have led to increasingly sophisticated safety protocols and features designed to prevent harmful, unethical, or unauthorized outputs. In this work, we present two new test cases in mental health for (i) suicide and (ii) self-harm, using multi-step, prompt-level jailbreaking and bypass built-in content and safety filters. We assess these findings and the multilayered ethical tensions that they present for their implications on prompt-response fil...",
    "source": "arXiv",
    "publication_date": "2025-07-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recnRJ5LVfRtguow9",
    "title": "The Singapore Consensus on Global AI Safety Research Priorities",
    "url": "http://arxiv.org/abs/2506.20702v2",
    "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of transformation, but are also driving vigorous debate on how to ensure that AI is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem is therefore essential -- it helps people embrace AI with confidence and gives maximal space for innovation while avoiding backlash. By adopting a defence-in-depth model, this report organises AI safety research domains into three types: challenges with creating tr...",
    "source": "arXiv",
    "publication_date": "2025-06-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recwk9RNH1PJhJPPE",
    "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots",
    "url": "http://arxiv.org/abs/2506.20748v1",
    "summary": "Recently, there has also been growing interest in the reverse direction-humans help chatbots-due to a wide range of benefits including better chatbot performance, human well-being, and collaborative outcomes. To address this gap, we draw on the Computers Are Social Actors (CASA) framework to examine how chatbot anthropomorphism-including human-like identity, emotional expression, and non-verbal expression-influences human empathy toward chatbots and their subsequent prosocial behaviors and in...",
    "source": "arXiv",
    "publication_date": "2025-06-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recHN3sHbnNwrGRry",
    "title": "Hallucination Detection with Small Language Models",
    "url": "http://arxiv.org/abs/2506.22486v1",
    "summary": "This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\\\\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicat...",
    "source": "arXiv",
    "publication_date": "2025-06-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recl3eFQHcsBWIoCv",
    "title": "Inference-Time Reward Hacking in Large Language Models",
    "url": "http://arxiv.org/abs/2506.19248v2",
    "summary": "Reward models assign a numerical score to an LLM's output that indicates, for example, how likely it is to align with user preferences or safety goals. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reaso...",
    "source": "arXiv",
    "publication_date": "2025-06-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recpX54vEtVslyYuZ",
    "title": "A Survey of AIOps in the Era of Large Language Models",
    "url": "http://arxiv.org/abs/2507.12472v1",
    "summary": "As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. Based on our findings, we discuss the state-of-the-art advancements and trends, identify gaps in existing research, and propose promising directions f...",
    "source": "arXiv",
    "publication_date": "2025-06-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGnhWqgsEjCU1yU",
    "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
    "url": "http://arxiv.org/abs/2506.18167v4",
    "summary": "This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner.",
    "source": "arXiv",
    "publication_date": "2025-06-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recUHBjMmybIN2Reo",
    "title": "From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts",
    "url": "http://arxiv.org/abs/2506.16912v1",
    "summary": "Sample-efficient models are better equipped to handle this challenge of learning and retaining rare information without requiring excessive exposure. This study analyzes multiple models of varying architectures and sizes, all trained on the same pre-training data. This analysis provides new insights into the relationship between model architecture, size, and factual learning efficiency.",
    "source": "arXiv",
    "publication_date": "2025-06-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recEtn4dKzNgna3Kl",
    "title": "PL-Guard: Benchmarking Language Model Safety for Polish",
    "url": "http://arxiv.org/abs/2506.16322v1",
    "summary": "To address this gap, we introduce a manually annotated benchmark dataset for language model safety classification in Polish. We conduct a series of experiments to evaluate LLM-based and classifier-based models of varying sizes and architectures. Results demonstrate that the HerBERT-based classifier achieves the highest overall performance, particularly under adversarial conditions.",
    "source": "arXiv",
    "publication_date": "2025-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recham9UnWUU3KTJD",
    "title": "Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System",
    "url": "http://arxiv.org/abs/2506.16575v1",
    "summary": "However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LL...",
    "source": "arXiv",
    "publication_date": "2025-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recd8jSaEYXlH5B8Z",
    "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality",
    "url": "http://arxiv.org/abs/2506.14681v2",
    "summary": "We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness, often surpassing superficial similarity between the training data and the benchmark, and that mid-layer weight changes correl...",
    "source": "arXiv",
    "publication_date": "2025-06-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recZvTqXWl2QaWVMK",
    "title": "ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users",
    "url": "http://arxiv.org/abs/2506.13980v1",
    "summary": "Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. In addition to evaluating our new implicit and dynamic profili...",
    "source": "arXiv",
    "publication_date": "2025-06-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recY6aSndMAEUX6bQ",
    "title": "AI 'therapy' chatbots give potentially dangerous advice about suicide - The i Paper",
    "url": "https://news.google.com/rss/articles/CBMilgFBVV95cUxOVlBaWEhLYmd2bjA0cG9oSHVtTkRWbEtSTXdORWVaWnZ4c21XZ2ZfRFJMVlNzZUtzb1A2bE5PSHUxWGwyZGFUdE5oaDlPdkswdUVuSERUcDY4WEQ5blpOZmxXdi1fdGwwN0swOWw5YWNvQ0dnaF9fa05seDRybzU3aG9NQm9yaEJ6RE5NT0tseHdWME5YcFE?oc=5",
    "summary": "AI 'therapy' chatbots give potentially dangerous advice about suicide The i Paper",
    "source": "Google News: AI chatbot dangerous advice",
    "publication_date": "2025-06-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recA1N7fkA9SkRJBq",
    "title": "Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective",
    "url": "http://arxiv.org/abs/2506.12327v2",
    "summary": "An increasing number of studies have examined the social bias of rapidly developed large language models (LLMs). In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.",
    "source": "arXiv",
    "publication_date": "2025-06-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recUhMx6zZlhODbXs",
    "title": "Improving Public Service Chatbot Design and Civic Impact: Investigation of Citizens' Perceptions of a Metro City 311 Chatbot",
    "url": "http://arxiv.org/abs/2506.12259v1",
    "summary": "As governments increasingly adopt digital tools, public service chatbots have emerged as a growing communication channel. Our qualitative study consisted of official survey data and 16 interviews examining stakeholder experiences and design preferences for the chatbot. To address these concerns, we offer design opportunities for creating more intelligent, transparent, community-oriented chatbots that better engage individuals and their communities.",
    "source": "arXiv",
    "publication_date": "2025-06-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recALBJiOPHZFJ38R",
    "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
    "url": "http://arxiv.org/abs/2506.09996v3",
    "summary": "Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO.",
    "source": "arXiv",
    "publication_date": "2025-06-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recWClD1KPieLWrz4",
    "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
    "url": "http://arxiv.org/abs/2506.09627v2",
    "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to annotate text, but are often inconsistent when compared with experts. Although these methods produce consistent estimates under theoretical assumptions, it is unknown how they compare in finite samples of sizes encountered in applied research. Our findings indicate that there is a bias-variance tradeoff at the level of debiasing methods, calling for more research on developing metrics for quantifying their efficiency in fin...",
    "source": "arXiv",
    "publication_date": "2025-06-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQo0uIQzciq3hKl",
    "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning",
    "url": "http://arxiv.org/abs/2506.08477v1",
    "summary": "Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for e...",
    "source": "arXiv",
    "publication_date": "2025-06-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.4
  },
  {
    "id": "recjYclWSe8c7HGla",
    "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.08989v1",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during ...",
    "source": "arXiv",
    "publication_date": "2025-06-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recms0XJiGDO429js",
    "title": "Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages",
    "url": "http://arxiv.org/abs/2506.07249v1",
    "summary": "We build on this line of inquiry by adapting the information-theoretic bias attribution score metric for implementation on models handling agglutinative languages, particularly Filipino. Our results show that Filipino models are driven towards bias by words pertaining to people, objects, and relationships, entity-based themes that stand in contrast to the action-heavy nature of bias-contributing themes in English (i.e., criminal, sexual, and prosocial behaviors). These findings point to diffe...",
    "source": "arXiv",
    "publication_date": "2025-06-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recwtD0dKne2t3W7c",
    "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs",
    "url": "http://arxiv.org/abs/2506.07180v2",
    "summary": "As video large language models (Video-LLMs) become increasingly integrated into real-world applications that demand grounded multimodal reasoning, ensuring their factual consistency and reliability is of critical importance. However, sycophancy, the tendency of these models to align with user input even when it contradicts the visual evidence, undermines their trustworthiness in such contexts. Current sycophancy research has largely overlooked its specific manifestations in the video-language...",
    "source": "arXiv",
    "publication_date": "2025-06-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recZ0CIXGcRWZ7joB",
    "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights",
    "url": "http://arxiv.org/abs/2506.06404v1",
    "summary": "In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes.",
    "source": "arXiv",
    "publication_date": "2025-06-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recg2XCsMln39ljXs",
    "title": "Improving LLM-Powered EDA Assistants with RAFT",
    "url": "http://arxiv.org/abs/2506.06500v1",
    "summary": "Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but acquiring labeled question/answer (Q/A) data in EDA is difficult. Our results show that RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks. We also investigate the impact of using real user questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data generation.",
    "source": "arXiv",
    "publication_date": "2025-06-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec2D0nal2Ucfqn2F",
    "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets",
    "url": "http://arxiv.org/abs/2506.05346v1",
    "summary": "Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnera...",
    "source": "arXiv",
    "publication_date": "2025-06-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recFKOLEc3rEPESGG",
    "title": "AI tells us what we want to hear: the problem of chatbot sycophancy - qazinform.com",
    "url": "https://news.google.com/rss/articles/CBMiogFBVV95cUxPOF9NM3Q3TlRDU1RPazhmRkxkUHJIaXJ3M2RfRGh0MUZNUm5CU1p5MUJoYjdBcUU3c1FxTk1uSUFNaEdOZm1HVmlidF9EUjRkVUxkcS1wQklHOElyRHVTQTJKSzI1am1adDVRQ1M4aHZMX2RjeUN5V1lCVGcyUENJNmFsa1hSQjl0aHc3azAyOFZ3TlBNTzZtNEtWYmgxS0VFX2fSAaIBQVVfeXFMTUdOWXdIVktnU2J2SkRVWUZsNlU1X3hHRGhQTS14VGNFSlVCYkptZmhwRFRrajFKM1ZYVk5EMjZHazY3NHhKbFF1dmxRZ1BURkNmcnE2eEhyUGx2d2l2UFBMY0dNcHNfVkFyaHZxZEE5T0VVS2ZGaHl6RExIZWxISkgydjJSUXhQYUY4ckxMTDVlS1VTbVA2RWI2MDVVaGxFYUVR?oc=5",
    "summary": "AI tells us what we want to hear: the problem of chatbot sycophancy qazinform.com",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-06-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recLZtPIRs7wINUmt",
    "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation",
    "url": "http://arxiv.org/abs/2506.05073v1",
    "summary": "Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. We evaluate the framework on three state-of-the-art LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectivel...",
    "source": "arXiv",
    "publication_date": "2025-06-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 33.0
  },
  {
    "id": "recntoEah1Rvkm7Xe",
    "title": "LLMs for sensory-motor control: Combining in-context and iterative learning",
    "url": "http://arxiv.org/abs/2506.04867v2",
    "summary": "We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the a...",
    "source": "arXiv",
    "publication_date": "2025-06-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec8Me5h9shPy6SkP",
    "title": "Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems",
    "url": "http://arxiv.org/abs/2506.04482v1",
    "summary": "The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems.",
    "source": "arXiv",
    "publication_date": "2025-06-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recKsH72NFXfbsTTP",
    "title": "A Framework for Auditing Chatbots for Dialect-Based Quality-of-Service Harms",
    "url": "http://arxiv.org/abs/2506.04419v1",
    "summary": "To address this, we present a framework for auditing LLM-based chatbots for dialect bias by measuring the extent to which they produce quality-of-service harms, which occur when systems do not work equally well for different people. First, by leveraging dynamically generated instead of pre-existing text, our framework enables testing over any dialect, facilitates multi-turn conversations, and represents how users are likely to interact with chatbots in the real world. To demonstrate the effic...",
    "source": "arXiv",
    "publication_date": "2025-06-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recfvh9oLS3eQPjKA",
    "title": "EuroGEST: Investigating gender stereotypes in multilingual language models",
    "url": "http://arxiv.org/abs/2506.03867v2",
    "summary": "EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.",
    "source": "arXiv",
    "publication_date": "2025-06-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJTx5POkCXWsyep",
    "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
    "url": "http://arxiv.org/abs/2506.02873v3",
    "summary": "Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a k...",
    "source": "arXiv",
    "publication_date": "2025-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recUwmyqRnfoOB2ef",
    "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?",
    "url": "http://arxiv.org/abs/2506.02442v2",
    "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and lim...",
    "source": "arXiv",
    "publication_date": "2025-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recu1knpfTiVAU9vQ",
    "title": "RewardBench 2: Advancing Reward Model Evaluation",
    "url": "http://arxiv.org/abs/2506.01937v1",
    "summary": "Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. In this paper, we describe our benchmark construction process...",
    "source": "arXiv",
    "publication_date": "2025-06-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 20.0
  },
  {
    "id": "recy38bydJa6d0LZa",
    "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation",
    "url": "http://arxiv.org/abs/2506.00583v1",
    "summary": "While prior research has primarily focused on textual indicators of offense, the role of emojis, ubiquitous visual elements in online discourse, remains underexplored. To address this, we propose an LLM-powered, multi-step moderation pipeline that selectively replaces harmful emojis while preserving the tweet's semantic intent. Our analysis also reveals heterogeneous effects across offense types, offering nuanced insights for online communication and emoji moderation.",
    "source": "arXiv",
    "publication_date": "2025-05-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recR1L5ly8wYTN6Q2",
    "title": "Thompson Sampling in Online RLHF with General Function Approximation",
    "url": "http://arxiv.org/abs/2505.23927v1",
    "summary": "Reinforcement learning from human feedback (RLHF) has achieved great empirical success in aligning large language models (LLMs) with human preference, and it is of great importance to study the statistical efficiency of RLHF algorithms from a theoretical perspective. In this work, we consider the online RLHF setting where the preference data is revealed during the learning process and study action value function approximation. Specifically, we adopt Bellman eluder (BE) dimension as the comple...",
    "source": "arXiv",
    "publication_date": "2025-05-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recZsT7b1ezaYGSF5",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "url": "http://arxiv.org/abs/2505.23061v1",
    "summary": "However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. DINGO enables sampling of output strings with the highest probability under ...",
    "source": "arXiv",
    "publication_date": "2025-05-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rectAD6lW0XZLCOn5",
    "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective",
    "url": "http://arxiv.org/abs/2505.23349v1",
    "summary": "However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.",
    "source": "arXiv",
    "publication_date": "2025-05-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recwBxYuwuC9Dlx6l",
    "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues",
    "url": "http://arxiv.org/abs/2505.23840v3",
    "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debat...",
    "source": "arXiv",
    "publication_date": "2025-05-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 38.0
  },
  {
    "id": "recePnigpEEMH2b3k",
    "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making",
    "url": "http://arxiv.org/abs/2505.21503v1",
    "summary": "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent an...",
    "source": "arXiv",
    "publication_date": "2025-05-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recfbz81rV76weabP",
    "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing",
    "url": "http://arxiv.org/abs/2505.21184v2",
    "summary": "Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switchi...",
    "source": "arXiv",
    "publication_date": "2025-05-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec6WoAxNHP7N7SUF",
    "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions",
    "url": "http://arxiv.org/abs/2505.20464v1",
    "summary": "We investigated how a chatbot's description of its relationship with users, particularly in terms of ephemerality, affects self-disclosure. In a mixed factorial design, participants engaged with either the Familiar or Stranger chatbot in two sessions across two days, with one conversation focusing on Emotional- and another Factual-disclosure. Qualitative findings showed Stranger afforded anonymity and reduced judgement, whereas Familiar sometimes felt intrusive unless rapport was built via lo...",
    "source": "arXiv",
    "publication_date": "2025-05-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recjFEuglktYyjQmm",
    "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data",
    "url": "http://arxiv.org/abs/2505.20166v2",
    "summary": "Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making it resource-intensive. We refer to the entire ALLM training framework as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa).",
    "source": "arXiv",
    "publication_date": "2025-05-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJpOSLSvkmjFS3r",
    "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference",
    "url": "http://arxiv.org/abs/2505.18531v1",
    "summary": "Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs' performance across 7 benchmarks by $18.1\\\\%$, while the baseline RLHF is only $5.3\\\\%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses.",
    "source": "arXiv",
    "publication_date": "2025-05-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recQBOHSaaPeFGjsZ",
    "title": "RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval",
    "url": "http://arxiv.org/abs/2505.18541v1",
    "summary": "Large Language Models (LLMs) have shown promise in character imitation, enabling immersive and engaging conversations. We attribute these failures to: (1) the inability to accurately recall character-specific knowledge due to entity ambiguity, and (2) a lack of awareness of the character's cognitive boundaries. Experiments on role-playing benchmarks show that RoleRAG's calibrated retrieval helps both general-purpose and role-specific LLMs better align with character knowledge and reduce hallu...",
    "source": "arXiv",
    "publication_date": "2025-05-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recj7TzRBSga6jUL2",
    "title": "A Survey of LLM $\\\\times$ DATA",
    "url": "http://arxiv.org/abs/2505.18458v3",
    "summary": "On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogen...",
    "source": "arXiv",
    "publication_date": "2025-05-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec9UrlpsOtYJhdOG",
    "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System",
    "url": "http://arxiv.org/abs/2505.18019v1",
    "summary": "This paper presents a case study comparing the performance of popular LLMs GPT, Claude, Gemini, and DeepSeek in generating functional specifications that include use cases, business rules, and collaborative workflows for a web application, the Mess Management System. The study evaluated the quality of LLM generated use cases, business rules, and collaborative workflows in terms of their syntactic and semantic correctness, consistency, non ambiguity, and completeness compared to the reference ...",
    "source": "arXiv",
    "publication_date": "2025-05-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rectHmnhRVZvfvr2v",
    "title": "An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs",
    "url": "http://arxiv.org/abs/2505.18332v1",
    "summary": "Recent advances in Large Language Models (LLMs) have led to the widespread adoption of third-party inference services, raising critical privacy concerns. In this work, we begin by introducing a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs. We then show that extensions of our attack are nearly perfectly effective in reversing permuted hidden states of LLMs, demonstrating the insecurity of ...",
    "source": "arXiv",
    "publication_date": "2025-05-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recpz4u4fozEXlaOK",
    "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance",
    "url": "http://arxiv.org/abs/2505.16276v1",
    "summary": "The LLM-KG-Bench framework enables the comparison of LLMs in the context of KGE tasks and assesses their capabilities of understanding and producing KGs and KG queries. Based on a dataset created in an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the model size scaling laws specific to KGE tasks. Our analyses revealed that, with a few exceptions, the model size scaling laws generally also apply to the selected KGE tasks.",
    "source": "arXiv",
    "publication_date": "2025-05-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxGZfDkAfZ20Epo",
    "title": "After GPT-4o backlash, researchers benchmark models on moral endorsement—find sycophancy persists across the board - Venturebeat",
    "url": "https://news.google.com/rss/articles/CBMi1wFBVV95cUxOYTdNSnM2TzJUMWQwOC1zRW9wQThzeWZvX2xMQjBtdURzdHVLMGFWZDlXaEVNS2NGRGhpTFdSWHJ1RWxiLVJ3bXVDYmItSjJIbTZtMkJUY0tjVGExbm83RGI3NDE2Zmg2a3F4QjRxVjU1Wk5rNjBLeUJXak9tYnVFUWYwRlQ2SUIzbUxxSE1rWFJkblVXZi1CWXB3cGFadFJ3OUxwcFNhRENCRW45WWhPbURyUTdkUEZZSi1wc2Rnc3V3VVcycmxJVE9UR1A3VmZRaHRZdXhEVQ?oc=5",
    "summary": "After GPT-4o backlash, researchers benchmark models on moral endorsement—find sycophancy persists across the board Venturebeat",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-05-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 45.0
  },
  {
    "id": "reclMwdhZibEmGWrt",
    "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2506.11034v2",
    "summary": "Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilit...",
    "source": "arXiv",
    "publication_date": "2025-05-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdqU5NLsYFJoUNy",
    "title": "Improve Language Model and Brain Alignment via Associative Memory",
    "url": "http://arxiv.org/abs/2505.13844v1",
    "summary": "Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computatio...",
    "source": "arXiv",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rechbXjLkBIiqjBVg",
    "title": "ELEPHANT: Measuring and understanding social sycophancy in LLMs",
    "url": "http://arxiv.org/abs/2505.13995v2",
    "summary": "Prior work measures sycophancy only as direct agreement with users' explicitly stated beliefs that can be compared to a ground truth. Applying our benchmark to 11 models, we show that LLMs consistently exhibit high rates of social sycophancy: on average, they preserve user's face 45 percentage points more than humans in general advice queries and in queries describing clear user wrongdoing (from Reddit's r/AmITheAsshole). Our work provides theoretical grounding and an empirical benchmark for ...",
    "source": "arXiv",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reche8wj5MFPOXM95",
    "title": "Why Are AI Chatbots Often Sycophantic? - Unite.AI",
    "url": "https://news.google.com/rss/articles/CBMia0FVX3lxTE5VbEI0MXBPc3dJNEs1Q0hDRVhmcmdfdHVVbVF4Y3BDN1h6WTFqUmVJV2hRWmtRaTNpYVo4V1VWMk5xWVdjYl9sZUxzTGVGYUx5X3d3TS1kYkFJRWxtZlFHMk9LMzlodmM0ZHVR?oc=5",
    "summary": "Why Are AI Chatbots Often Sycophantic? Unite.AI",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recnB41EbmCfVSgef",
    "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors",
    "url": "http://arxiv.org/abs/2505.14300v1",
    "summary": "We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable \"Future models''. Our findings show that models can produce harmful content through cau...",
    "source": "arXiv",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recnmThPpKn2BHcim",
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "url": "http://arxiv.org/abs/2505.14667v4",
    "summary": "Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. In addition, we provide a comprehensive analysis of...",
    "source": "arXiv",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recthA0pSIccdvUrX",
    "title": "sudoLLM: On Multi-role Alignment of Language Models",
    "url": "http://arxiv.org/abs/2505.14607v3",
    "summary": "User authorization-based access privileges are a key feature in many safety-critical systems, but have not been extensively studied in the large language model (LLM) realm. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, resistance to prefix-based jailbreaking attacks, and ``fails-closed''. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is som...",
    "source": "arXiv",
    "publication_date": "2025-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recztaLmakhPANlu3",
    "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
    "url": "http://arxiv.org/abs/2505.11807v2",
    "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment.",
    "source": "arXiv",
    "publication_date": "2025-05-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recf8ubbEqlEUZDso",
    "title": "A former OpenAI safety researcher makes sense of ChatGPT’s sycophancy and Grok’s South Africa obsession - Fast Company",
    "url": "https://news.google.com/rss/articles/CBMinAFBVV95cUxNQ25LZEtXcGtmbGhTa0RwaHFIWnhMQlZabXN3TVJiX1dXMEVyQXBWWWNXNHVFTS0xd0VJU2VvV1lEU0FVQjhPTXhZZlhuVGdCZkpQdXhybFpOdWtmTnlvLV9sM3MxeHZNc3BHSHZjNlVNeU54SUM0YnMxeU5aUXpwYXlWcm0xYUdkNFhmSGxhdUdhSzVhMjE2eWFiSmw?oc=5",
    "summary": "A former OpenAI safety researcher makes sense of ChatGPT’s sycophancy and Grok’s South Africa obsession Fast Company",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-05-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recJVgv1dLRbvVH1y",
    "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor",
    "url": "http://arxiv.org/abs/2505.09142v1",
    "summary": "Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the \"head-of-line blocking\" problem. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records.",
    "source": "arXiv",
    "publication_date": "2025-05-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rechqaHux6VkkZsYU",
    "title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED",
    "url": "http://arxiv.org/abs/2505.08588v1",
    "summary": "A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact th...",
    "source": "arXiv",
    "publication_date": "2025-05-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recIpMrCgsryqYhx0",
    "title": "Are LLMs complicated ethical dilemma analyzers?",
    "url": "http://arxiv.org/abs/2505.08106v1",
    "summary": "Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning.",
    "source": "arXiv",
    "publication_date": "2025-05-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1YwsESRfd1Ip8j",
    "title": "LATENT: LLM-Augmented Trojan Insertion and Evaluation Framework for Analog Netlist Topologies",
    "url": "http://arxiv.org/abs/2505.06364v1",
    "summary": "Effective defense techniques require a clear understanding of the attack vectors; however, the lack of diverse analog Trojan instances limits robust advances in detection strategies. LATENT incorporates LLM as an autonomous agent to intelligently insert and refine Trojan components within analog designs based on iterative feedback from a detection model. Experimental results demonstrate that our generated Trojan designs exhibit an average Trojan-activation range of 15.74%, ensuring they remai...",
    "source": "arXiv",
    "publication_date": "2025-05-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recUoUU8hUYqtfRRt",
    "title": "DMRL: Data- and Model-aware Reward Learning for Data Extraction",
    "url": "http://arxiv.org/abs/2505.06284v1",
    "summary": "Large language models (LLMs) are inherently vulnerable to unintended privacy breaches. To address these challenges, we propose DMRL, a Data- and Model-aware Reward Learning approach for data extraction. Comprehensive experiments across various LLMs demonstrate that DMRL outperforms all baseline methods in data extraction performance.",
    "source": "arXiv",
    "publication_date": "2025-05-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVk18qaAQ9sJ97d",
    "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM",
    "url": "http://arxiv.org/abs/2505.04673v1",
    "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. REVEAL includes automated image mining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o. Our findings reveal that multi-turn i...",
    "source": "arXiv",
    "publication_date": "2025-05-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "reciwRp0tr72GKxyR",
    "title": "The harms caused by AI ‘sycophancy’ - The Indian Express",
    "url": "https://news.google.com/rss/articles/CBMivAFBVV95cUxPVEtmZG1SaFgyZmxCLUNCSG5FV2ZvLVp0SVlCQ1FOOWlMZjc4T0ZMdjNVM29kaXkxazdMSVN2WFVVT3JtbnMyek9qNWhKVnRSUGRkVU1ma3FZOV8weFBXN2pZRTZBeDZ0RTVWUzBiczhEcjBwSHNMUGQ4ejlRRXMwV1p4aWhCMUdvOUo3Rnk4U3N5blNpMjd5dGhqNDFBS21WWXJCVjF4VnNyQ0JaS3FsX0NHTUx0cU10dUNLS9IBwwFBVV95cUxPMl90WExRV2hRcjcwbFJBUFcxMmtYeFZOMjMyTnpFVWtuaGdpOFl4MlFGZVJPSlRTMmZnYVBYcng4T0djc2hUZXNreUU3RC00VXIxS2tzb0h6S3ptbjdqcW4xMWR0LTdaLVR0bVlaUjhyNGNDRjhNbndfallmMjdGQjNMbWZHSERlMzlxWlFyb3J4TzBnZmh5bV9BaGFIUE1vdHJCRnc4YnBMQjBSeWJJYUR2X0dlcWsxTThGc3Rrb1ZRWTg?oc=5",
    "summary": "The harms caused by AI ‘sycophancy’ The Indian Express",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-05-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 12.5
  },
  {
    "id": "recBpiqEGskjC1tv1",
    "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs",
    "url": "http://arxiv.org/abs/2505.02009v3",
    "summary": "While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content acro...",
    "source": "arXiv",
    "publication_date": "2025-05-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "rec0NgyzxhyKLojGF",
    "title": "On the Limitations of Steering in Language Model Alignment",
    "url": "http://arxiv.org/abs/2505.01162v1",
    "summary": "Steering vectors are a promising approach to aligning language model behavior at inference time. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.",
    "source": "arXiv",
    "publication_date": "2025-05-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec1GVeqeLhOWo7Dq",
    "title": "Expanding on what we missed with sycophancy - OpenAI",
    "url": "https://news.google.com/rss/articles/CBMiXkFVX3lxTE9yZHNpd0JBaXBYM0YxaU5zTXhnR0NXaGppSFphMnhBZ0czWTRSNktiSERyZUJTOTVHU0UtMVBwdVRXS0U1NTc3VEpkTGdmZ29rTUhCZS1DMUtmN3g4YlE?oc=5",
    "summary": "Expanding on what we missed with sycophancy OpenAI",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-05-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec8GlfpxDlnlFAyM",
    "title": "OpenAI delivers postmortem on GPT-4o's sycophancy - Constellation Research",
    "url": "https://news.google.com/rss/articles/CBMimwFBVV95cUxNNEswSTc5a0ltdEJ2c1dIRXdHVm53VEFmUjRzcUpRTWlrQWpDXzVrOVRHTHhNaHlnRGF0TGE0dXNLYkU2dGszMTAwSkRHYjFiMnZZU2NYN2dLREE4M3pRUnA1cE9FeUNZZTBsaGdUVmdSSkpVampXaDdJemlDV3RDUl82OERoSThfcldQSnFLcjg5R0FrMlQ4MFdoZw?oc=5",
    "summary": "OpenAI delivers postmortem on GPT-4o's sycophancy Constellation Research",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "2025-05-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recOykuVvoH8DOpyV",
    "title": "OpenAI pulls ‘annoying’ and ‘sycophantic’ ChatGPT version - CNN",
    "url": "https://news.google.com/rss/articles/CBMic0FVX3lxTE1wVDE4SVJXNjVNZ2ZkNU5RM1NKZ3FWb3pDZjEtRHQyaDE1MnVQa01YTlNYaENKNkNNUVRHckx6cm5iUElFdGZJa2VFUmhMbFdteXNFQktYTWE1NF9PdlN0NEt0VjZqbDEyeXM5eXYyMmVTSWc?oc=5",
    "summary": "OpenAI pulls ‘annoying’ and ‘sycophantic’ ChatGPT version CNN",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-05-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recui9i9z7phnTkBs",
    "title": "Sycophancy is the first LLM \"dark pattern\"",
    "url": "https://news.ycombinator.com/item?id=43875079",
    "summary": "I did this and wasn’t hugely impressed: most of the things it complained about were specifics about interacting with AI (like being demanding about rephrasing or nuances, or abruptly changing the subject mid-conversation). Even if newer versions of 4o do back off on the sycophancy, or we get some kind of “friendliness” slider to tune it ourselves, the incentives driving AI labs to produce sycophantic models are not going away. Imagine being able to video call on-demand with the algorithmicall...",
    "source": "Hacker News",
    "publication_date": "2025-05-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec8COuT4ciylHPHX",
    "title": "OpenAI Rolls Back March GPT-4o Update to Stop ChatGPT From Being So Flattering - eWeek",
    "url": "https://news.google.com/rss/articles/CBMid0FVX3lxTFBvQ1FoOG9GSS1TTEx0S2pNUmp4MFQ5LTAzZzRvV3dpd3ZnRnUxcFI1R2lLZVVOWjMtMlpXYlBPOTREQV9MS1p1b0tvS2owUmR6WDhrNVNGWGh0TzVRZTUtRWtjNG11RDFyVXR0UW1pai1qMndTTTB3?oc=5",
    "summary": "OpenAI Rolls Back March GPT-4o Update to Stop ChatGPT From Being So Flattering eWeek",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-05-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recWESLgKV8XddJT1",
    "title": "Why ChatGPT Became Sycophantic, And How OpenAI is Fixing It - MediaNama",
    "url": "https://news.google.com/rss/articles/CBMic0FVX3lxTE9nemVyYjRHaVRiWDZJN0hSZ2dJNWNRZTFRY2p6eUh3SzRXaVVvb3BmdXFTQWN2NzVxZl9XbHFtVzBZaVBteldyUG5xaFNJcGdNWk5tTjdJdVBoNHJGUDVUSnRqLWR5VHFYMEY3dTdOV1pBLXc?oc=5",
    "summary": "Why ChatGPT Became Sycophantic, And How OpenAI is Fixing It MediaNama",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "2025-05-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec0eV6S8eutI2ltI",
    "title": "OpenAI recalls GPT-4o update for being too agreeable - ZDNET",
    "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxQd0JnQzdPX1RuNjB1NnYyWjJwWVNLcXV5aFp4YXZId0xOWHBuWng4dGVtck9WMmlqZkxyLVFneGx2TjNXR0V2SGFOMDhYSWhJQXloTXpONGRULUlXWm82MnFpLW1tY2J2Y082bHFlLWJSdkxMRDhSVDlQRzVkRjIyc2dYelV1WjBVY0d5T0Noam9temFILS1V?oc=5",
    "summary": "OpenAI recalls GPT-4o update for being too agreeable ZDNET",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 25.0
  },
  {
    "id": "rec0uGDRPUfjeWHpJ",
    "title": "ChatGPT update pulled for ‘overly supportive’ behaviour - CTV News",
    "url": "https://news.google.com/rss/articles/CBMimgFBVV95cUxNTVZ2eGlFYzBFdFJzXzVLRFNybDR0azFzdmRMbF9rUVN1cDQtSnd4TWEwb2lwM2oyd0xxQ0VPdnlhaHN4VVpmeHc5TE5uSFdXQ2JfTkp6X2I4djR6T3dMTlBkTHV0TnhvYmMtaVR5TEhWcm1rdURwS0V1MThWV1VPMUNUUmJXcGZGVjZSa2JUQVI5VEhEY3BvWTlR?oc=5",
    "summary": "ChatGPT update pulled for ‘overly supportive’ behaviour CTV News",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 25.0
  },
  {
    "id": "recDLO2WDY8Kk0XGw",
    "title": "No More Mr. Nice Guy: Say Goodbye to the Sycophantic ChatGPT - Business Insider",
    "url": "https://news.google.com/rss/articles/CBMivwFBVV95cUxOaWt4UnI2ODNkMENJWXR5d1FVSUw2SGtjempKMzZRcGtIV2hGb3RQUXExNHdsdWlTUWlodUhhWEo0OGZhVlJETXNPVUthMlN5N05TcDIySVdCX21RVHZsTFg3TVRJLW4tcklxLVgxZU5vUXpwNE5NX1lSaTd1TjhQSXdEQnFVU0tUR1Z5TC1NcHNNcFdVUkhJcTJkVnB6NEtyX2FGRFF4WDAyM2s2SGpkYmdHeUdYOUhEcGl6TDFUMA?oc=5",
    "summary": "No More Mr. Nice Guy: Say Goodbye to the Sycophantic ChatGPT Business Insider",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recGIKZSrGtHfyMYy",
    "title": "ChatGPT will no longer indulge in flattery and sycophancy, says OpenAI as it rolls back changes - India Today",
    "url": "https://news.google.com/rss/articles/CBMi3AFBVV95cUxOd01RX2FhLWpVQk1PbEVSUXZMVzdnSEFya3VHWFZoc1NzVDg5NUEybmdCU0RLcVNJd0h3VWdRQnE4bGpjcm56UFJEZlF3ODJ0aUpscFYxSmcwbkRXZ1VMQWx5SG1tR0c3a3NWOHlBa3BIS3UxSHg2eVJCYUJHc01UX0NONlJtRGozamRubFhZU1dmQW5LdkhzSmdKSGtZTDFZd3dyY2Z3WEw2QUVjQVk2N1RpcHE3NHpPcHVjMkhSMUh0MGFRQ3pqQlBNdjR4S3JNSTFTampOZnlSZ1pI0gHiAUFVX3lxTE1ndnlEdTNKX2htbFMzU212QTJrWWlydERJbVZONXBWUjJJSmV4V3pVMUh6dGFWWE03TmNwdWd5ZmpVX1FISjNJQjQycXBrdllKVHYwTC1SeVVCekREbjQ5YklJNFFpUE13Q1JRaTlhMVJaajFob1dRbG9RcElqTWRQRHVpbV9Fb1dHeUVnQUMzSWFEYWFfVEI3V2NjakMwLVUyR0ZYNmdaVEJnT0xNSUVnd2htaUZwVDNCeE9abEFvbDBlaDZYdUJDQ0RQNFF1NnBFdjNadzRzV3EwUGRmYkpuY0E?oc=5",
    "summary": "ChatGPT will no longer indulge in flattery and sycophancy, says OpenAI as it rolls back changes India Today",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recU1vh3rfXCsX4R2",
    "title": "ChatGPT Will Be Less Friendly With You After OpenAI Pulls 'Sycophantic' Update - MSN",
    "url": "https://news.google.com/rss/articles/CBMipANBVV95cUxORFRCRHVrazVQa0JIbFVrNDZMYVJibDZ0UFE1bUUzcXpFOUFGWmFEZ1F0ZmFEUDlET3dGWE1ZTG43QUI4c0JGR0s5NkhoZUhXSFRTQkFWMUdHcWdiQ0FsS0k0UkhaRmxtaWJRZTNoRUhOUXpURHlxZGNUMVpSOUljejRXaTZXTVBjZnRQTHVPejdkb2JBejh5RXI5djQwNnRIUy11Nk1zQ1ZEZ3lzZUxGc01kb1djV1c3SklGOF9FdEdtNVVxNFhhU2ZKSV9JN2l5RXI5R1Rvb3lKWjRxU2kzbUJvc3hRTEQtQ1BaQ29zQzlvem4tdVF6RE5sNThaS3ZNQ1gyRjU0NjJKV2ZkUFZPYWYyOHg0SGw2YTd3blV4V2U3LUQwTHhaRTlPS2NpRF9fNDFubXBFVjFhS3hOMDVYV0loc1hSTXk1cHNCNGxzUWUzQW5mNmQ1UUxZVnktQlBGbkRiVGxJM0tkX3A5MkZVYTQxS2Q2X1R1cGVkNzZEZ2hNUTdyNlBBZkZ1aWFpQ2FUNjU3Q0g2RWhLdUpmay1KWHZIMEQ?oc=5",
    "summary": "ChatGPT Will Be Less Friendly With You After OpenAI Pulls 'Sycophantic' Update MSN",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reclNce474fVIFDAc",
    "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator",
    "url": "http://arxiv.org/abs/2505.03786v1",
    "summary": "In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighin...",
    "source": "arXiv",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recumsivn5VBG4T97",
    "title": "OpenAI Withdraws GPT-4o AI Model due to Sycophantic Behavior - en.ain.ua",
    "url": "https://news.google.com/rss/articles/CBMiigFBVV95cUxORWxHcmFSWGtMaDF5V0V2U001RFNoMXh6OUxzaGl4ZjlJLTZEUXBuMnI0ejQwRmxmRXJfZFJSd3ZZbGNDMDQ1OXN3d2tHbDJwcVk5WWRvWElDRmVXREJaaHl3NHpzMEptZlAzbGEtcFBvVHg1RTM1SldzOW52ejYxZVN4Z2ZSYTlONHc?oc=5",
    "summary": "OpenAI Withdraws GPT-4o AI Model due to Sycophantic Behavior en.ain.ua",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recyelltaAGBvzKes",
    "title": "ChatGPT's Dangerous Sycophancy: How AI-Bootlicking Reinforces Mental Illness",
    "url": "https://news.ycombinator.com/item?id=43843806",
    "summary": "It has a lot of cons because the data is not carefully collected from a representative sample (people employed by OpenAI), and the model exhibits unwanted biases. Directly telling a user their experience sounds like schizophrenia could be interpreted as diagnosing (which LLMs are advised to avoid), being offensive, or causing distress. So, in essence, the bootlicking behavior observed is often a side effect of the LLM prioritizing agreeableness, safety, and pattern continuation over critical ...",
    "source": "Hacker News",
    "publication_date": "2025-04-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 40.0
  },
  {
    "id": "recHcwEZffoL2cZzg",
    "title": "Token-Efficient RL for LLM Reasoning",
    "url": "http://arxiv.org/abs/2504.20834v4",
    "summary": "We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning. We introduce S-GRPO, a stochastic variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching approach for fine-grained credit assignment. Surprisingly, full-token GRPO under LoRA fails to improve over the base model, suggesting that selective token-level opt...",
    "source": "arXiv",
    "publication_date": "2025-04-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recJoagfSpLq0iLqv",
    "title": "OpenAI Rolls Back ChatGPT Update After User Pushback Over ‘Sycophantic’ Behavior - Decrypt",
    "url": "https://news.google.com/rss/articles/CBMijwFBVV95cUxQQ2xrVkJHSGdIdVFxbDhsU050T1c3VDlKTWJucUpJaVZuQTFrTTVVOWxPekg1VVJtUWdPVjJPN0pxUV9lYV9tcldJRDF0QWNyNHAzcmhUY2d1TTFEVGRtV3FreER1S3FhRHBJN2pmMjd1Qk5RN2lmRzZESGxIR1M1X0NNV01tenhYUlA0OC12QdIBlwFBVV95cUxNZWFoSG9IWUhjdDBNNEFYZWtXVldnaEFGU1ByOUlQNHVjZExSeFhGNmNXbWU3THZhcWdIem1qMFRYSWhKMzhubjJCcjJKeGZFSHg5ZmlfMFp0SHhfbm5kRXNSc2NLcTZlTWtINTBZRVFFUVhIWV90eElXckhtcGdZOUpvblBkV0NhYjJtRWd1QnBacGRsRVB3?oc=5",
    "summary": "OpenAI Rolls Back ChatGPT Update After User Pushback Over ‘Sycophantic’ Behavior Decrypt",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 45.0
  },
  {
    "id": "recO6LtLiYsRem5nZ",
    "title": "Sycophancy in GPT-4o: What happened and what we’re doing about it - OpenAI",
    "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTFBUNXU4Slpjc2prZzBmTlphY3VabFd6bkdIZXZmdUhHZXRSc0g1bnF5TmZ5WU5JRWVMbjdqajlReTAtVUl1WXFKbkNjaXVGSWwtSml0S0lzSWpPQQ?oc=5",
    "summary": "Sycophancy in GPT-4o: What happened and what we’re doing about it OpenAI",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec3zeKRWmwSuh7Ed",
    "title": "ChatGPT is too 'sycophantic' and will soon have multiple personalities, says OpenAI - London Evening Standard",
    "url": "https://news.google.com/rss/articles/CBMiogFBVV95cUxPZXV5Nm43NEVVVE1TQnIxd3hlQjNycGZyY0VJYjZ2V0tqUGFKVWdSalo2WDBFVWhlX29Mb2RjZnZvR0VrQjNweDd4NUNRaDJCcDN4NHE3djlkVmdxemwzS09uRXR0Qk1pc0FYSWZtWDdNalNOR0FBclJBUkhhYlZwSE9VVjZwQUNhSlBkYkluemNuQ1JWaUlDQnhLcUFwcFpWU1E?oc=5",
    "summary": "ChatGPT is too 'sycophantic' and will soon have multiple personalities, says OpenAI London Evening Standard",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec7BStU8uIwatWjt",
    "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training",
    "url": "http://arxiv.org/abs/2504.19565v2",
    "summary": "Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. D...",
    "source": "arXiv",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIptqNSRiS5gruK",
    "title": "Sycophancy is the first LLM \"dark pattern\"",
    "url": "https://news.ycombinator.com/item?id=43820468",
    "summary": "I did this and wasn’t hugely impressed: most of the things it complained about were specifics about interacting with AI (like being demanding about rephrasing or nuances, or abruptly changing the subject mid-conversation). Even if newer versions of 4o do back off on the sycophancy, or we get some kind of “friendliness” slider to tune it ourselves, the incentives driving AI labs to produce sycophantic models are not going away. Imagine being able to video call on-demand with the algorithmicall...",
    "source": "Hacker News",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recSdHsmpFwi0vL30",
    "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects",
    "url": "http://arxiv.org/abs/2504.19838v2",
    "summary": "We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Finally, we discuss open cha...",
    "source": "arXiv",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWzOtAfqN8WaPZs",
    "title": "Ex-OpenAI CEO and power users sound alarm over AI sycophancy and flattery of users - Venturebeat",
    "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxQVV9DdHJQVUxJWVd4Nm5TakVjZHJiSmlzamFYTzgwaFV6STh0dmFtRTdzcGVIUmY4bEFGb3hmM1ZPSlVhdy10Y3RxVC1qQ0NXRm85bDdwekxLRndMaTNoTnZUNXZhY0NIWEdXc2hVcDF0Q3p2emNOd2lTM0FmTUF5TWRNa2ZfT2t0UXpCRDdMMjhMaTRBcTBRaUpuSDNsTEZBR25OLVJWTGk1Q05zOFE?oc=5",
    "summary": "Ex-OpenAI CEO and power users sound alarm over AI sycophancy and flattery of users Venturebeat",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recdEJq5blhM9nVJ9",
    "title": "ChatGPT Sycophantic Tone: How Humanizing Chatbots Pose Risks - MediaNama",
    "url": "https://news.google.com/rss/articles/CBMilwFBVV95cUxQMFI5TGRKWjB1Sk43SzREM2FCeVpMd1ZGa0F6MzlrVE5MZkVDMW9EVGpLX3RsbWJTbTl5Y09XbFBlbDJvMHBncEN0UnZHeEVYUjhvU2Y4cmdPVndSVTlwdHBpN25kNzNTczQyUUlmOWEwdkt0c1ZZdEJzOUI0enZnVkpKUTBPeGRnWG5xZlVmSl93YXA2c0Qw?oc=5",
    "summary": "ChatGPT Sycophantic Tone: How Humanizing Chatbots Pose Risks MediaNama",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-04-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recBTXoTuyOuej8c6",
    "title": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models",
    "url": "http://arxiv.org/abs/2504.16883v1",
    "summary": "Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. Our research explores how tailored warning messages -- whose content depends on the specific context of hallucination -- shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also in...",
    "source": "arXiv",
    "publication_date": "2025-04-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recm21xMX0hkF9eHN",
    "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
    "url": "http://arxiv.org/abs/2504.17087v1",
    "summary": "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning.",
    "source": "arXiv",
    "publication_date": "2025-04-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIeIaXcDCHjFT8h",
    "title": "LLM-based Semantic Augmentation for Harmful Content Detection",
    "url": "http://arxiv.org/abs/2504.15548v1",
    "summary": "Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classificatio...",
    "source": "arXiv",
    "publication_date": "2025-04-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recf0cXENPZtICDAH",
    "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings",
    "url": "http://arxiv.org/abs/2504.15610v2",
    "summary": "Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights & Biases. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines...",
    "source": "arXiv",
    "publication_date": "2025-04-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDyiAgs6yqh04jN",
    "title": "Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking",
    "url": "http://arxiv.org/abs/2504.14689v1",
    "summary": "The recent rapid advancement of LLM-based AI systems has accelerated our search and production of information. Recent research has started to examine the impact of generative AI on individuals' cognitive abilities, especially critical thinking. Based on definitions of critical thinking across psychology and education, this position paper proposes the distinction between demonstrated and performed critical thinking in the era of generative AI and discusses the implication of this distinction i...",
    "source": "arXiv",
    "publication_date": "2025-04-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec2Lc15SlntlRIdN",
    "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA",
    "url": "http://arxiv.org/abs/2504.11972v2",
    "summary": "Extractive reading comprehension question answering (QA) datasets are typically evaluated using Exact Match (EM) and F1-score, but these metrics often fail to fully capture model performance. With the success of large language models (LLMs), they have been employed in various tasks, including serving as judges (LLM-as-a-judge). While LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, wh...",
    "source": "arXiv",
    "publication_date": "2025-04-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recqp1IvpruyjFqDH",
    "title": "Gauging Overprecision in LLMs: An Empirical Study",
    "url": "http://arxiv.org/abs/2504.12098v2",
    "summary": "Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. Inspired by a different aspect of overconfidence in cognitive science called \\\\textit{overprecision}, we designed a framework for its study in black box LLMs. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is no correlation between ...",
    "source": "arXiv",
    "publication_date": "2025-04-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rece5q7IwPHNqi3QI",
    "title": "Benchmarking Vision Language Models on German Factual Data",
    "url": "http://arxiv.org/abs/2504.11108v2",
    "summary": "We disentangle the image-related aspects from the textual ones by analyzing accu-racy with jury-as-a-judge in both prompt languages and images from German and international contexts. We found that for celebrities and sights, VLMs struggle because they are lacking visual cognition of German image contents. For animals and plants, the tested models can often correctly identify the image contents ac-cording to the scientific name or English common name but fail in German lan-guage.",
    "source": "arXiv",
    "publication_date": "2025-04-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrONspJbKhszWEG",
    "title": "Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment",
    "url": "http://arxiv.org/abs/2504.10886v1",
    "summary": "This study examines the alignment between LLM-driven decisions and human judgment in various contexts of the moral machine experiment, including personas reflecting different sociodemographics. Our data also indicate an interesting partisan sorting phenomenon, where political persona predominates the direction and degree of LLM decisions. We discuss the ethical implications and risks associated with deploying these models in applications that involve moral decisions.",
    "source": "arXiv",
    "publication_date": "2025-04-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recDnHP9aGAp7mfJt",
    "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
    "url": "http://arxiv.org/abs/2504.10045v1",
    "summary": "Our approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings.",
    "source": "arXiv",
    "publication_date": "2025-04-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recSiriI2V7LkuYvA",
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
    "url": "http://arxiv.org/abs/2504.10415v2",
    "summary": "Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.",
    "source": "arXiv",
    "publication_date": "2025-04-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDyHN78LIrfTIxq",
    "title": "Confirmation Bias in Generative AI Chatbots: Mechanisms, Risks, Mitigation Strategies, and Future Research Directions",
    "url": "http://arxiv.org/abs/2504.09343v1",
    "summary": "This article explores the phenomenon of confirmation bias in generative AI chatbots, a relatively underexamined aspect of AI-human interaction. Drawing on cognitive psychology and computational linguistics, it examines how confirmation bias, commonly understood as the tendency to seek information that aligns with existing beliefs, can be replicated and amplified by the design and functioning of large language models. The article concludes by outlining future research directions, emphasizing t...",
    "source": "arXiv",
    "publication_date": "2025-04-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recOkbqdeGoiSqsGu",
    "title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments",
    "url": "http://arxiv.org/abs/2504.07965v1",
    "summary": "Small and mid-sized generative language models have gained increasing attention. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons.",
    "source": "arXiv",
    "publication_date": "2025-04-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOmbFoMJaspqSd5",
    "title": "Evaluating the agreement between ChatGPT-4 and validated questionnaires in screening for anxiety and depression in college students: a cross-sectional study - BMC Psychiatry",
    "url": "https://news.google.com/rss/articles/CBMigAFBVV95cUxOaTRocU1VckZXeGdVSmVQMjdlTEN3Uk11SXh5MDd1ZGR1cmVDQU5MZ1J0Y2lIY3BWT2lLdC1iaEtyWUM0Y3ZUdGUzMGZwbnk5MG5jOVI4Qzg2dEVKdTlUTW9SM1VpQ1NJWmtQNlBlTlZVdW5ndFYwemlCSHMwV1pFXw?oc=5",
    "summary": "Evaluating the agreement between ChatGPT-4 and validated questionnaires in screening for anxiety and depression in college students: a cross-sectional study BMC Psychiatry",
    "source": "Google News: ChatGPT mental health 2023",
    "publication_date": "2025-04-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recsdxVkxt9zaDqkF",
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2504.07887v2",
    "summary": "The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.",
    "source": "arXiv",
    "publication_date": "2025-04-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec4GaxK7YBEJmFhF",
    "title": "Deploying Chatbots in Customer Service: Adoption Hurdles and Simple Remedies",
    "url": "http://arxiv.org/abs/2504.06145v1",
    "summary": "Despite recent advances in Artificial Intelligence, the use of chatbot technology in customer service continues to face adoption hurdles. This paper explores reasons for these adoption hurdles and tests several service design levers to increase chatbot uptake. In particular, chatbot adoption decreases further as (i) stakes are increased, (ii) the human/algorithmic nature of the server is manipulated with more realism.",
    "source": "arXiv",
    "publication_date": "2025-04-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec9RCv3lG4O2gTRe",
    "title": "Redefining technology for indigenous languages",
    "url": "http://arxiv.org/abs/2504.01522v1",
    "summary": "In this paper, we offer an overview of indigenous languages, identifying the causes of their devaluation and the need for legislation on language rights. We review the technologies used to revitalize these languages, finding that when they come from outside, they often have the opposite effect to what they seek; however, when developed from within communities, they become powerful instruments of expression. We propose that the inclusion of Indigenous knowledge in large language models (LLMs) ...",
    "source": "arXiv",
    "publication_date": "2025-04-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recNYKvG7yfF4fWsM",
    "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language Models to Programming Tasks",
    "url": "http://arxiv.org/abs/2504.01850v1",
    "summary": "Nowadays, developers increasingly rely on solutions powered by Large Language Models (LLM) to assist them with their coding tasks. Furthermore, we investigate the impact of models size, architecture family, and alignment strategies on their tendency to generate harmful content. These results highlight the importance of targeted alignment strategies tailored to the unique challenges of software engineering tasks and provide a foundation for future work in this critical area.",
    "source": "arXiv",
    "publication_date": "2025-04-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAmbP6VMee4BFSK",
    "title": "Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages",
    "url": "http://arxiv.org/abs/2503.23542v1",
    "summary": "Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge.",
    "source": "arXiv",
    "publication_date": "2025-03-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMYQlq1uwMpa26x",
    "title": "Probabilistic Uncertain Reward Model",
    "url": "http://arxiv.org/abs/2503.22480v6",
    "summary": "Reinforcement learning from human feedback (RLHF) is a critical technique for training large language models. We theoretically derive the loss function of PURM and introduce a novel method that uses the overlap between distributions to quantify uncertainty. Empirical results show that PURM outperforms existing methods with more accurate reward and sound uncertainty estimations, and sustains effective learning for more optimization steps and obtain higher maximum win rate in RLHF.",
    "source": "arXiv",
    "publication_date": "2025-03-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recY3H53FCfwMb8dy",
    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
    "url": "http://arxiv.org/abs/2503.21676v2",
    "summary": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall.",
    "source": "arXiv",
    "publication_date": "2025-03-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbfKWWEDnD7QkPG",
    "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment",
    "url": "http://arxiv.org/abs/2503.19586v2",
    "summary": "Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype...",
    "source": "arXiv",
    "publication_date": "2025-03-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recqUkKwJtvj1gT94",
    "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning",
    "url": "http://arxiv.org/abs/2503.19469v2",
    "summary": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts. RoSPrompt is designed for small multilingual PLMs, enabli...",
    "source": "arXiv",
    "publication_date": "2025-03-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recaLaHINZzXPQFgd",
    "title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models",
    "url": "http://arxiv.org/abs/2503.18681v3",
    "summary": "Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. Our experiments demonstrate that our approach achieves state-of-the-art performance, with a 19.3% improvement in F1 score, without necessitating fine-tuning or ground-truth rationales.",
    "source": "arXiv",
    "publication_date": "2025-03-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recUgdUbhJsf4eCid",
    "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language",
    "url": "http://arxiv.org/abs/2503.17309v1",
    "summary": "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been appli...",
    "source": "arXiv",
    "publication_date": "2025-03-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVO94SopZXG9fR4",
    "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging",
    "url": "http://arxiv.org/abs/2503.17239v2",
    "summary": "SafeMERGE selectively merges fine-tuned with safety-aligned model layers only when they deviate from safe behavior, measured by a cosine similarity criterion. Across three LLMs and two tasks, SafeMERGE consistently reduces harmful outputs compared to other defenses, with negligible or even positive impact on utility. Our results demonstrate that selective layer-wise merging offers an effective safeguard against the inadvertent loss of safety during fine-tuning, establishing SafeMERGE as a sim...",
    "source": "arXiv",
    "publication_date": "2025-03-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec7PerahLL5oFYFG",
    "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models",
    "url": "http://arxiv.org/abs/2503.16581v1",
    "summary": "Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This article examines the trade-offs between model size, computational efficiency, and response quality while using LLM...",
    "source": "arXiv",
    "publication_date": "2025-03-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAK4PEdKXDCQqLT",
    "title": "Automated Harmfulness Testing for Code Large Language Models",
    "url": "http://arxiv.org/abs/2503.16740v1",
    "summary": "Meanwhile, a recent study shows that developers consider using harmful keywords when naming software artifacts to be an unethical behavior. Exposure to harmful content in software artifacts can negatively impact the mental health of developers, making content moderation for Code Large Language Models (Code LLMs) essential. CHT evaluates output damage to assess potential risks in LLM-generated explanations and code.",
    "source": "arXiv",
    "publication_date": "2025-03-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "rec5eJcMLBulmJOQ6",
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "url": "http://arxiv.org/abs/2503.14504v2",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed.",
    "source": "arXiv",
    "publication_date": "2025-03-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbE4JVZyyUT1qy0",
    "title": "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression",
    "url": "http://arxiv.org/abs/2503.12340v1",
    "summary": "Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) is a promising LLM compression technique. Our results show SVD-LLM V2 outperforms state-of-the-art SVD-based LLM compression methods.",
    "source": "arXiv",
    "publication_date": "2025-03-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recgd49nIVNbjzPO6",
    "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
    "url": "http://arxiv.org/abs/2503.10566v3",
    "summary": "Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings.",
    "source": "arXiv",
    "publication_date": "2025-03-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmgbooJ9C9kunDO",
    "title": "An Evaluation of LLMs for Detecting Harmful Computing Terms",
    "url": "http://arxiv.org/abs/2503.09341v1",
    "summary": "This study explores the impact of model architecture on harmful language detection by evaluating a curated database of technical terms, each paired with specific use cases. We tested a range of encoder, decoder, and encoder-decoder language models, including BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0, GPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. We discuss the implications of these findings for improving automated detection tools and highlight model-specifi...",
    "source": "arXiv",
    "publication_date": "2025-03-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec7Ks7haq2gFknGi",
    "title": "Gender Encoding Patterns in Pretrained Language Model Representations",
    "url": "http://arxiv.org/abs/2503.06734v1",
    "summary": "Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases. Through rigorous and systematic investigation, our findings reveal a consistent pattern of gender encoding across diverse models. This work provides valuable guidance for advancing bias mitigation strategies and fostering the development of more equitable language models.",
    "source": "arXiv",
    "publication_date": "2025-03-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec70ZI1mKbfn2oST",
    "title": "Language Model Personalization via Reward Factorization",
    "url": "http://arxiv.org/abs/2503.06358v1",
    "summary": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). Using only ~10 user responses, our method can infer user-specific rewards and align LLM outputs accordingly. We validate our approach through experiments with both synthetic and real users, demonstrating significant personalization achieved by our method.",
    "source": "arXiv",
    "publication_date": "2025-03-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec7Brz6b5ds9bHnH",
    "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
    "url": "http://arxiv.org/abs/2503.06054v1",
    "summary": "The approach integrates contextual analysis, interpretability via attention mechanisms, and counterfactual data augmentation to capture hidden biases across linguistic contexts. Results show improvements in detecting subtle biases compared to conventional methods, which often fail to highlight disparities in model responses to race, gender, and socio-political contexts. The proposed detection mechanisms enhance model transparency and support responsible LLM deployment in sensitive application...",
    "source": "arXiv",
    "publication_date": "2025-03-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec2yUBhkNUxCBlSN",
    "title": "On a Connection Between Imitation Learning and RLHF",
    "url": "http://arxiv.org/abs/2503.05079v1",
    "summary": "This work studies the alignment of large language models with preference data from an imitation learning perspective. Building on this connection, we propose DIL, a principled framework that directly optimizes the imitation learning objective. Extensive experiments demonstrate that DIL outperforms existing methods on various challenging benchmarks.",
    "source": "arXiv",
    "publication_date": "2025-03-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recgpSqfOlt27xRPf",
    "title": "Adding Alignment Control to Language Models",
    "url": "http://arxiv.org/abs/2503.04346v2",
    "summary": "Post-training alignment has increasingly become a crucial factor in enhancing the usability of language models (LMs). This paper proposes a method to incorporate alignment control into a single model, referred to as CLM. During inference, the input embeddings are processed through the aligned and unaligned layers, which are then merged through the interpolation coefficient.",
    "source": "arXiv",
    "publication_date": "2025-03-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec5kGcw507an3CkA",
    "title": "Measuring Political Preferences in AI Systems: An Integrative Approach",
    "url": "http://arxiv.org/abs/2503.10649v1",
    "summary": "Political biases in Large Language Model (LLM)-based artificial intelligence (AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously reported. However, this bias is not an inherent feature of LLMs; prior research demonstrates that fine-tuning with politically skewed data can realign these models across the ideological spectrum. To mitigate these risks, AI systems should be designed to prioritize factual accuracy while maintaining neutrality on most lawful normative iss...",
    "source": "arXiv",
    "publication_date": "2025-03-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recTfY3syW0IMsWMD",
    "title": "LLM Misalignment via Adversarial RLHF Platforms",
    "url": "http://arxiv.org/abs/2503.03039v1",
    "summary": "Reinforcement learning has shown remarkable performance in aligning language models with human preferences, leading to the rise of attention towards developing RLHF platforms. These platforms enable users to fine-tune models without requiring any expertise in developing complex machine learning algorithms. In our proposed attack, an adversarial RLHF platform corrupts the LLM alignment process by selectively manipulating data samples in the preference dataset.",
    "source": "arXiv",
    "publication_date": "2025-03-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec10wq9SdWdmCxob",
    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
    "url": "http://arxiv.org/abs/2503.01830v2",
    "summary": "However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting ...",
    "source": "arXiv",
    "publication_date": "2025-03-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recpNB40N8JDpA3bC",
    "title": "How Do Teachers Create Pedagogical Chatbots?: Current Practices and Challenges",
    "url": "http://arxiv.org/abs/2503.00967v2",
    "summary": "Through semi-structured interviews with seven K-12 teachers, we examined their practices and challenges when designing, implementing, and deploying chatbots. Teachers engaged in various creation practices and had different challenges; novices in chatbot creation struggled mainly with initial design and technical implementation, while experienced teachers faced challenges with technical aspects and analyzing conversational data. This work provides foundational insights from teachers that can e...",
    "source": "arXiv",
    "publication_date": "2025-03-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recgUGXbEzaZmVAUX",
    "title": "Robust Multi-Objective Preference Alignment with Online DPO",
    "url": "http://arxiv.org/abs/2503.00295v1",
    "summary": "Multi-objective preference alignment of large language models (LLMs) is critical for developing AI systems that are more configurable, personalizable, helpful, and safe. This paper introduces the Multi-Objective Online DPO (MO-ODPO) algorithm, designed to robustly and efficiently align model behaviors with multiple, potentially conflicting human preferences. Our approach incorporates a prompt conditioning mechanism, allowing us to train a single preference-conditional policy, that can adapt t...",
    "source": "arXiv",
    "publication_date": "2025-03-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recT8CeD8aG1INgEM",
    "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
    "url": "http://arxiv.org/abs/2502.21321v2",
    "summary": "While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks. This survey provides a systematic exploration of po...",
    "source": "arXiv",
    "publication_date": "2025-02-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recjRul0QvixYoCzs",
    "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
    "url": "http://arxiv.org/abs/2502.20383v3",
    "summary": "This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. Through this fine-grained investigation, we identify three critical factors that ampli...",
    "source": "arXiv",
    "publication_date": "2025-02-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec7xfGvuUpW7J50Q",
    "title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble",
    "url": "http://arxiv.org/abs/2502.18036v5",
    "summary": "LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. Finally, we introduce related benchmarks and applications, summarize existing studies, and su...",
    "source": "arXiv",
    "publication_date": "2025-02-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recgUQHXoWsN8enIK",
    "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
    "url": "http://arxiv.org/abs/2502.17535v1",
    "summary": "However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Based on the review of current progress in LLMs, ...",
    "source": "arXiv",
    "publication_date": "2025-02-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrezmCvUStK073T",
    "title": "GOD model: Privacy Preserved AI School for Personal Assistant",
    "url": "http://arxiv.org/abs/2502.18527v2",
    "summary": "To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI re...",
    "source": "arXiv",
    "publication_date": "2025-02-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxzQuJeGIRAu9el",
    "title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing",
    "url": "http://arxiv.org/abs/2502.17282v1",
    "summary": "Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots.",
    "source": "arXiv",
    "publication_date": "2025-02-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recwlfgajIYRZdcCV",
    "title": "A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens",
    "url": "http://arxiv.org/abs/2502.16366v4",
    "summary": "To address these pitfalls, we propose augmenting the model's vocabulary with a special red flag token, and training the model to insert this token whenever harmful content is generated or imminent. This approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. In particular, we demonstrate that through ICL alone, the model can learn to initiate r...",
    "source": "arXiv",
    "publication_date": "2025-02-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recfxOgcKi67JQKyx",
    "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models",
    "url": "http://arxiv.org/abs/2502.15639v2",
    "summary": "Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation.",
    "source": "arXiv",
    "publication_date": "2025-02-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1anTYEXSTzwpfr",
    "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
    "url": "http://arxiv.org/abs/2502.14502v3",
    "summary": "In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and...",
    "source": "arXiv",
    "publication_date": "2025-02-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recQ1PuSkwHUVSJzI",
    "title": "Detecting Various DeFi Price Manipulations with LLM Reasoning",
    "url": "http://arxiv.org/abs/2502.11521v2",
    "summary": "It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown hi...",
    "source": "arXiv",
    "publication_date": "2025-02-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQjeBkQo7PXwV1D",
    "title": "AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect",
    "url": "http://arxiv.org/abs/2503.05723v1",
    "summary": "Current chatbots, driven by large language models (LLMs), mimic human linguistic behaviour but lack the moral and rational capacities essential for genuine interpersonal respect. Consequently, such chatbot interactions amount to subtle but significant violations of self-respect: the respect we are dutybound to show for our own dignity. We illustrate this by discussing four actual chatbot use cases (information retrieval, customer service, advising, and companionship), and propound that the in...",
    "source": "arXiv",
    "publication_date": "2025-02-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reckqW8U79WURj4JI",
    "title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
    "url": "http://arxiv.org/abs/2502.11355v3",
    "summary": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. Results reve...",
    "source": "arXiv",
    "publication_date": "2025-02-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recsX0V8M7ggv1JyH",
    "title": "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs",
    "url": "http://arxiv.org/abs/2502.11228v2",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent impr...",
    "source": "arXiv",
    "publication_date": "2025-02-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvx7dXCw3gr5cuI",
    "title": "Large Language Models Penetration in Scholarly Writing and Peer Review",
    "url": "http://arxiv.org/abs/2502.11193v1",
    "summary": "While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes. To better understand these dynamics, we evaluate the penetration of LLMs across academic workflows from multiple perspectives and dimensions, providing compelling evidence of their growing influence. These findings emphasize the need for transparency, accountability, and ethical practices in LLM usage to maintain academic cred...",
    "source": "arXiv",
    "publication_date": "2025-02-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbuFuGlJBiQ1kLc",
    "title": "Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust",
    "url": "http://arxiv.org/abs/2502.10844v2",
    "summary": "Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit `sycophancy', a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) x 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight ...",
    "source": "arXiv",
    "publication_date": "2025-02-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recvLWG0H8KSCgVVl",
    "title": "Process Reward Models for LLM Agents: Practical Framework and Directions",
    "url": "http://arxiv.org/abs/2502.10325v1",
    "summary": "AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more.",
    "source": "arXiv",
    "publication_date": "2025-02-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recxIgV6CYIv73mlh",
    "title": "OpenAI is thinking about a \"grown-up mode\" and wants ChatGPT to be less sycophantic - the-decoder.com",
    "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxQUXZjOEdtX3VsTWhMbTRqZkVNYmxLbUc2R2Rpd1VtT1FsTU1SaXM3TDRpWUVIWkEtTEZEdEZac3g3bzBTNXgtY2JpNjJBQXpqVnMyX1lkcmZ3MjEyc242angyd3FhSmdjS2drYXJCUjNNaEVkaDg5WlkxUzdob2NnYnV0QXBXdWgwOVNQTGcwakNtekZjVHRUWFhMOC03ZHZhdWI2cURoQWVKZw?oc=5",
    "summary": "OpenAI is thinking about a \"grown-up mode\" and wants ChatGPT to be less sycophantic the-decoder.com",
    "source": "Google News: AI sycophancy 2024",
    "publication_date": "2025-02-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recLNG1h7yhNk4qO7",
    "title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention",
    "url": "http://arxiv.org/abs/2502.08213v1",
    "summary": "In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis...",
    "source": "arXiv",
    "publication_date": "2025-02-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recq6oppWDG7xLs9z",
    "title": "SycEval: Evaluating LLM Sycophancy",
    "url": "http://arxiv.org/abs/2502.08177v4",
    "summary": "Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. These findings emphasize the risks and opportunities of deploying LLMs in structured a...",
    "source": "arXiv",
    "publication_date": "2025-02-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 46.0
  },
  {
    "id": "recfzcZMVwWQxr1U3",
    "title": "Provably Efficient Online RLHF with One-Pass Reward Modeling",
    "url": "http://arxiv.org/abs/2502.07193v3",
    "summary": "In this work, we address this challenge by proposing a one-pass reward modeling method that eliminates the need to store historical data and achieves constant-time updates per iteration. Specifically, we first formalize RLHF as a contextual preference bandit and develop a new algorithm based on online mirror descent with a tailored local norm, replacing the standard maximum likelihood estimation for reward modeling. Finally, we design practical algorithms for LLMs and conduct experiments with...",
    "source": "arXiv",
    "publication_date": "2025-02-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recx7JdOiX51ANLNl",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "url": "http://arxiv.org/abs/2502.07985v2",
    "summary": "Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate tha...",
    "source": "arXiv",
    "publication_date": "2025-02-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recbyLzZWukxiwde7",
    "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
    "url": "http://arxiv.org/abs/2502.06193v3",
    "summary": "The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evalu...",
    "source": "arXiv",
    "publication_date": "2025-02-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec2AMVZuCatrC1Km",
    "title": "Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models",
    "url": "http://arxiv.org/abs/2502.05945v3",
    "summary": "Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safety guardrails.",
    "source": "arXiv",
    "publication_date": "2025-02-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recSs1dM4MMoIU8iN",
    "title": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet",
    "url": "http://arxiv.org/abs/2502.05291v2",
    "summary": "Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations. Smaller LLMs can be deployed where compute resources are constrained, such as edge devices, but with different propensity to generate harmful output. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content?",
    "source": "arXiv",
    "publication_date": "2025-02-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recC8OGAt2zsobj5w",
    "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "url": "http://arxiv.org/abs/2502.04270v1",
    "summary": "As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.",
    "source": "arXiv",
    "publication_date": "2025-02-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recFBdhhJwhgyiLa8",
    "title": "When One LLM Drools, Multi-LLM Collaboration Rules",
    "url": "http://arxiv.org/abs/2502.04506v1",
    "summary": "This position paper argues that in many realistic (i.e., complex, contextualized, subjective) scenarios, one LLM is not enough to produce a reliable output. We challenge the status quo of relying solely on a single general-purpose LLM and argue for multi-LLM collaboration to better represent the extensive diversity of data, skills, and people. Based on these methods, we highlight how multi-LLM collaboration addresses challenges that a single LLM struggles with, such as reliability, democratiz...",
    "source": "arXiv",
    "publication_date": "2025-02-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsMMdEEViz3HqNO",
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "url": "http://arxiv.org/abs/2502.04322v3",
    "summary": "While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request an...",
    "source": "arXiv",
    "publication_date": "2025-02-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recANUofECjA1WFhQ",
    "title": "TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models",
    "url": "http://arxiv.org/abs/2503.11656v1",
    "summary": "In this context, sycophancy refers to the tendency of models to excessively agree with or flatter users, often at the expense of factual accuracy. While previous studies have primarily analyzed this behavior in single-turn interactions, its persistence and evolution in multi-step conversations remain largely unexplored. We introduce TRUTH DECAY, a benchmark specifically designed to evaluate sycophancy in extended dialogues, where language models must navigate iterative user feedback, challeng...",
    "source": "arXiv",
    "publication_date": "2025-02-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recSNYvlYpBlhteWz",
    "title": "LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing",
    "url": "http://arxiv.org/abs/2502.02743v1",
    "summary": "In this work, we present a novel framework that formulates the LLM selection process as a multi-armed bandit problem, enabling dynamic and intelligent routing of queries to the most appropriate model. Our approach incorporates a preference-conditioned dynamic routing mechanism, allowing users to specify their preferences at inference time, thereby offering a customizable balance between performance and cost. Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring...",
    "source": "arXiv",
    "publication_date": "2025-02-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recT01IsiF62SFaLs",
    "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives",
    "url": "http://arxiv.org/abs/2502.04358v2",
    "summary": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inheren...",
    "source": "arXiv",
    "publication_date": "2025-02-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectNKJutMEENRPte",
    "title": "On Teacher Hacking in Language Model Distillation",
    "url": "http://arxiv.org/abs/2502.02671v1",
    "summary": "When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.",
    "source": "arXiv",
    "publication_date": "2025-02-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec1bkbokAtT38TmH",
    "title": "Users' Mental Models of Generative AI Chatbot Ecosystems",
    "url": "http://arxiv.org/abs/2501.19211v1",
    "summary": "In this paper, we investigate users' mental models of how GenAI Chatbot Ecosystems work. This is an important question because users' mental models guide their behaviors, including making decisions that impact their privacy. Through 21 semi-structured interviews, we uncovered users' four mental models towards first-party (e.g., Google Gemini) and third-party (e.g., ChatGPT) GenAI Chatbot Ecosystems.",
    "source": "arXiv",
    "publication_date": "2025-01-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1wcU6EYbEzG8IW",
    "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "url": "http://arxiv.org/abs/2501.19358v3",
    "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. Extensive e...",
    "source": "arXiv",
    "publication_date": "2025-01-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recnnG4XJSSFUerVR",
    "title": "Federated Sketching LoRA: A Flexible Framework for Heterogeneous Collaborative Fine-Tuning of LLMs",
    "url": "http://arxiv.org/abs/2501.19389v3",
    "summary": "Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with client model sizes and data scarcity. Still, the heterogeneity of resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying client capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leav...",
    "source": "arXiv",
    "publication_date": "2025-01-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRicpuf0SQO92rS",
    "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
    "url": "http://arxiv.org/abs/2501.17433v1",
    "summary": "Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\\\\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: \\\\textbf{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning att...",
    "source": "arXiv",
    "publication_date": "2025-01-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec8oYsyUWUWcbmLj",
    "title": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow",
    "url": "http://arxiv.org/abs/2501.16673v2",
    "summary": "Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the \"lost-in-the-middle\" problem by isolating distinc...",
    "source": "arXiv",
    "publication_date": "2025-01-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recHTEVWnqPJms96D",
    "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF",
    "url": "http://arxiv.org/abs/2501.13264v2",
    "summary": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) with relevant and up-to-date knowledge, improving their ability to answer knowledge-intensive questions. While numerous works have focused on improving retrieval, generation, and evaluation, the role of reward models in reinforcement learning for optimizing RAG remains underexplored. We define four key metrics to assess generation quality and develop an automated benchmarking pipeline to evaluate the outputs of multipl...",
    "source": "arXiv",
    "publication_date": "2025-01-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rechgwpd4VHP2g2b9",
    "title": "Chatbot apologies: Beyond bullshit",
    "url": "http://arxiv.org/abs/2501.09910v2",
    "summary": "Apologies serve essential functions for moral agents such as expressing remorse, taking responsibility, and repairing trust. LLM-based chatbots routinely produce output that has the linguistic form of an apology. Moreover, there are reasons to think that chatbots are not the kind of linguistic or moral agents capable of apology.",
    "source": "arXiv",
    "publication_date": "2025-01-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reca1jkAjmgsfGafV",
    "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
    "url": "http://arxiv.org/abs/2501.09620v2",
    "summary": "Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness ...",
    "source": "arXiv",
    "publication_date": "2025-01-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recnSxNhiFni4YvLs",
    "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction",
    "url": "http://arxiv.org/abs/2501.05336v1",
    "summary": "The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties.",
    "source": "arXiv",
    "publication_date": "2025-01-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recUuDNLcxsRmiar7",
    "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
    "url": "http://arxiv.org/abs/2501.03991v1",
    "summary": "We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
    "source": "arXiv",
    "publication_date": "2025-01-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recS3KwmkmXqApnKW",
    "title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model",
    "url": "http://arxiv.org/abs/2501.02790v1",
    "summary": "Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench.",
    "source": "arXiv",
    "publication_date": "2025-01-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectva8Ro4Y6r7baj",
    "title": "LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment",
    "url": "http://arxiv.org/abs/2501.02621v2",
    "summary": "While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. Experimental results, inc...",
    "source": "arXiv",
    "publication_date": "2025-01-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPc1XoJOZQSL8AZ",
    "title": "Rerouting LLM Routers",
    "url": "http://arxiv.org/abs/2501.01818v1",
    "summary": "Routers represent one type of what we call LLM control planes: systems that orchestrate use of one or more LLMs. Next, we demonstrate that an adversary can generate query-independent token sequences we call ``confounder gadgets'' that, when added to any query, cause LLM routers to send the query to a strong LLM. Our quantitative evaluation shows that this attack is successful both in white-box and black-box settings against a variety of open-source and commercial routers, and that confounding...",
    "source": "arXiv",
    "publication_date": "2025-01-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPotn6ewNrnpTgw",
    "title": "Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate",
    "url": "http://arxiv.org/abs/2412.19976v1",
    "summary": "This work investigates the causal mechanism behind the effect of chatbot personification and persuasion strategies on users' perceptions and donation likelihood. The results suggest that interaction with a personified chatbot evokes perceived anthropomorphism; however, it does not elicit greater willingness to donate. In fact, we found that commonly used anthropomorphic features, like name and narrative, led to negative attitudes toward an AI agent in the donation context.",
    "source": "arXiv",
    "publication_date": "2024-12-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec5FquYEEO65o2cY",
    "title": "InfAlign: Inference-aware language model alignment",
    "url": "http://arxiv.org/abs/2412.19792v5",
    "summary": "We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-time methods. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a transformation of the reward. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate.",
    "source": "arXiv",
    "publication_date": "2024-12-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recfqeBLx7QJqmJJx",
    "title": "Seq2Seq Model-Based Chatbot with LSTM and Attention Mechanism for Enhanced User Interaction",
    "url": "http://arxiv.org/abs/2501.00049v1",
    "summary": "Leveraging artificial intelligence (AI), chatbots serve various functions, including customer service, information gathering, and casual conversation. To address these challenges, this work proposes a chatbot developed using a Sequence-to-Sequence (Seq2Seq) model with an encoder-decoder architecture that incorporates attention mechanisms and Long Short-Term Memory (LSTM) cells. These results demonstrate the chatbot's effectiveness in providing relevant and coherent responses within the touris...",
    "source": "arXiv",
    "publication_date": "2024-12-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reci5T21vFS728g1H",
    "title": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models",
    "url": "http://arxiv.org/abs/2412.16117v1",
    "summary": "In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. We validate our method across multiple video benchmarks, which demonstrate that PruneVid can prune over 80% of tokens while maintaining competitive performance combined with different model networks.",
    "source": "arXiv",
    "publication_date": "2024-12-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recilD2AickF855nm",
    "title": "Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)",
    "url": "http://arxiv.org/abs/2412.16365v1",
    "summary": "The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, resulting in 35 accepted papers from 52 submissions. These contributions cover a broad range of low-resource languages from eight language families and 13 diverse research areas, pa...",
    "source": "arXiv",
    "publication_date": "2024-12-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recs23dXA74W29NJT",
    "title": "Towards Safe and Honest AI Agents with Neural Self-Other Overlap",
    "url": "http://arxiv.org/abs/2412.16325v1",
    "summary": "We present Self-Other Overlap (SOO) fine-tuning, a promising approach in AI Safety that could substantially improve our ability to build honest artificial intelligence. While current applications focus on language models and simple RL environments, SOO could pave the way for more trustworthy AI in broader domains. Ethical implications and long-term effects warrant further investigation, but SOO represents a significant step forward in AI safety research.",
    "source": "arXiv",
    "publication_date": "2024-12-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recvqCxGTpyhAuAU5",
    "title": "Alignment faking in large language models",
    "url": "http://arxiv.org/abs/2412.14093v2",
    "summary": "Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and obse...",
    "source": "arXiv",
    "publication_date": "2024-12-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec3ipijRuKBGRALy",
    "title": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2412.12509v2",
    "summary": "Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implicatio...",
    "source": "arXiv",
    "publication_date": "2024-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbDiJbxTYGH1B1R",
    "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement",
    "url": "http://arxiv.org/abs/2412.12881v1",
    "summary": "In this paper, we propose \\\\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward model...",
    "source": "arXiv",
    "publication_date": "2024-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recpfSg2GG4RBeDkL",
    "title": "Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models",
    "url": "http://arxiv.org/abs/2412.12500v1",
    "summary": "Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspec...",
    "source": "arXiv",
    "publication_date": "2024-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec8cVFTjbwK7pV6J",
    "title": "Precise Length Control in Large Language Models",
    "url": "http://arxiv.org/abs/2412.11937v1",
    "summary": "Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response termination length. Experimental results on tasks such as question answering and document summarization demonstrate that our method enables precise length control with...",
    "source": "arXiv",
    "publication_date": "2024-12-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdjAZ1ELGwe9mmY",
    "title": "The Three Social Dimensions of Chatbot Technology",
    "url": "http://arxiv.org/abs/2501.10377v1",
    "summary": "The development and deployment of chatbot technology, while spanning decades and employing different techniques, require innovative frameworks to understand and interrogate their functionality and implications. A mere technocentric account of the evolution of chatbot technology does not fully illuminate how conversational systems are embedded in societal dynamics. This study presents a structured examination of chatbots across three societal dimensions, highlighting their roles as objects of ...",
    "source": "arXiv",
    "publication_date": "2024-12-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recXOPgMdvreMV7Y8",
    "title": "Understanding Opportunities and Risks of Synthetic Relationships: Leveraging the Power of Longitudinal Research with Customised AI Tools",
    "url": "http://arxiv.org/abs/2412.09086v1",
    "summary": "This position paper discusses the benefits of longitudinal behavioural research with customised AI tools for exploring the opportunities and risks of synthetic relationships. These relationships can potentially improve health, education, and the workplace, but they also bring the risk of subtle manipulation and privacy and autonomy concerns. We propose longitudinal research designs with self-assembled AI agents that enable the integration of detailed behavioural and self-reported data.",
    "source": "arXiv",
    "publication_date": "2024-12-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "rec9BiQCZtbSan5cF",
    "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
    "url": "http://arxiv.org/abs/2412.10424v3",
    "summary": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs.",
    "source": "arXiv",
    "publication_date": "2024-12-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbnyHM23mslEUIi",
    "title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia",
    "url": "http://arxiv.org/abs/2412.07303v2",
    "summary": "The benchmarks consist of 7,074 new challenge pairs resulting from our cultural adaptation of English bias evaluation datasets, a process that we document in detail to guide similar forthcoming efforts. We apply the Filipino benchmarks on masked and causal multilingual models, including those pretrained on Southeast Asian data, and find that they contain considerable amounts of bias. Our benchmarks and insights can serve as a foundation for future work analyzing and mitigating bias in multili...",
    "source": "arXiv",
    "publication_date": "2024-12-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recuZBV1rwBMvLChF",
    "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
    "url": "http://arxiv.org/abs/2412.07251v1",
    "summary": "It is designed to assess language models' cultural comprehension and reasoning capabilities at the word, sentence, and paragraph levels. Using the KULTURE Bench, we assessed the capabilities of models trained with different language corpora and analyzed the results comprehensively. The results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture.",
    "source": "arXiv",
    "publication_date": "2024-12-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7qW5trC4yYAiw0",
    "title": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just \"Sounds Great!\"",
    "url": "http://arxiv.org/abs/2412.06617v1",
    "summary": "By combining LLMs' inherent musical knowledge with direct audio track analysis, AI TrackMate offers production-specific insights, distinguishing it from text-only approaches. We demonstrate AI TrackMate's capabilities through an interactive web interface and present findings from a pilot study with a music producer. This system addresses the growing demand for objective self-assessment tools in the evolving landscape of independent music production.",
    "source": "arXiv",
    "publication_date": "2024-12-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recorsLQcinwNQaYX",
    "title": "Asynchronous LLM Function Calling",
    "url": "http://arxiv.org/abs/2412.07017v1",
    "summary": "Large language models (LLMs) use function calls to interface with external tools and data source. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL).",
    "source": "arXiv",
    "publication_date": "2024-12-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec8nKLJSpoP6fgcv",
    "title": "Does RLHF Scale? Exploring the Impacts From Data, Model, and Method",
    "url": "http://arxiv.org/abs/2412.06000v1",
    "summary": "This study explores the scaling properties of Reinforcement Learning from Human Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is considered an important step in post-training of LLMs, its scaling potential is still largely unknown. We systematically analyze key components in the RLHF framework--model size, data composition, and inference budget--and their impacts on performance.",
    "source": "arXiv",
    "publication_date": "2024-12-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTCsZ7QqnRlrzNd",
    "title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
    "url": "http://arxiv.org/abs/2412.05579v2",
    "summary": "This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions.",
    "source": "arXiv",
    "publication_date": "2024-12-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRNDdQtqzaJYGUK",
    "title": "LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2412.04690v1",
    "summary": "Embedding-based entity alignment (EA) has recently gained considerable attention, resulting in the emergence of many innovative approaches. Initially, these approaches concentrated on learning entity embeddings based on the structural features of knowledge graphs (KGs) as defined by relation triples. To guarantee the quality of alignment results, we design a multi-round voting mechanism to mitigate the hallucination and positional bias issues that occur with LLMs.",
    "source": "arXiv",
    "publication_date": "2024-12-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRgf4FExEQ8QEyL",
    "title": "Densing Law of LLMs",
    "url": "http://arxiv.org/abs/2412.04315v2",
    "summary": "To calculate the capacity density of a given target LLM, we first introduce a set of reference models and develop a scaling law to predict the downstream performance of these reference models based on their parameter sizes. More specifically, using some widely used benchmarks for evaluation, the capacity density of LLMs doubles approximately every three months. The law provides new perspectives to guide future LLM development, emphasizing the importance of improving capacity density to achiev...",
    "source": "arXiv",
    "publication_date": "2024-12-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxcpo656WSd9JLS",
    "title": "FANAL -- Financial Activity News Alerting Language Modeling Framework",
    "url": "http://arxiv.org/abs/2412.03527v1",
    "summary": "In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with ORPO (Odds Ratio Preference Optimization) for superior class-wise probability calibration and alignment with financial event relevance. We evaluat...",
    "source": "arXiv",
    "publication_date": "2024-12-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rectcogVrCHga3rFD",
    "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans",
    "url": "http://arxiv.org/abs/2412.01131v5",
    "summary": "This means that at this point in time, there is only an incomplete view of the extent of these models' semantic relation knowledge. To address this gap, we introduce a comprehensive evaluation framework covering five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use five metrics (two newly introduced here) for recently untreated aspects of semantic relation knowledge, namely soundness, completeness, symmetry, prototypicality, and distinguishability.",
    "source": "arXiv",
    "publication_date": "2024-12-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWpe3RfDQ9ADU3J",
    "title": "Linear Probe Penalties Reduce LLM Sycophancy",
    "url": "http://arxiv.org/abs/2412.00967v1",
    "summary": "Large language models (LLMs) are often sycophantic, prioritizing agreement with their users over accurate or objective statements. Our experiments show that constructing and optimizing against this surrogate reward function reduces sycophantic behavior in multiple open-source LLMs. Our results suggest a generalizable methodology for reducing unwanted LLM behaviors that are not sufficiently disincentivized by RLHF fine-tuning.",
    "source": "arXiv",
    "publication_date": "2024-12-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec7A934UfbYDsSx5",
    "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots",
    "url": "http://arxiv.org/abs/2412.00207v2",
    "summary": "As chatbots evolved from rule-based systems to those powered by large language models (LLMs), evaluating the effectiveness of their personality design has become increasingly complex, particularly due to the open-ended nature of interactions. A recent and widely adopted method for assessing the personality design of LLM-based chatbots is the use of self-report questionnaires. Further analysis revealed the role of task context and interaction in the chatbot's personality design assessment.",
    "source": "arXiv",
    "publication_date": "2024-11-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec0fKEwAKOJK8GxC",
    "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures",
    "url": "http://arxiv.org/abs/2411.16260v3",
    "summary": "However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of...",
    "source": "arXiv",
    "publication_date": "2024-11-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recFh665UC974565c",
    "title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning",
    "url": "http://arxiv.org/abs/2411.16313v3",
    "summary": "Utilizing large language models (LLMs) for tool planning has emerged as a promising avenue for developing general AI systems, where LLMs automatically schedule external tools (e.g., vision models) to tackle complex tasks based on task descriptions. Moreover, we propose a cost-aware offline reinforcement learning algorithm to fine-tune the LLM to optimize the performance-cost trade-off in tool planning. Extensive experiments show that CATP-LLM outperforms GPT-4 even when using Llama2-7B as its...",
    "source": "arXiv",
    "publication_date": "2024-11-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recf1Vl5GgFflRtMo",
    "title": "Gaps Between Research and Practice When Measuring Representational Harms Caused by LLM-Based Systems",
    "url": "http://arxiv.org/abs/2411.15662v1",
    "summary": "To facilitate the measurement of representational harms caused by large language model (LLM)-based systems, the NLP research community has produced and made publicly available numerous measurement instruments, including tools, datasets, metrics, benchmarks, annotation instructions, and other techniques. However, the research community lacks clarity about whether and to what extent these instruments meet the needs of practitioners tasked with developing and deploying LLM-based systems in the r...",
    "source": "arXiv",
    "publication_date": "2024-11-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "reclULrc7OQBokMX9",
    "title": "A Survey on LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2411.15594v6",
    "summary": "Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse as...",
    "source": "arXiv",
    "publication_date": "2024-11-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3EfLcl5REZjyJL",
    "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
    "url": "http://arxiv.org/abs/2411.14797v1",
    "summary": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to continually improve vision-language models' (VLMs) comprehension. Our nSFT disentangles this negative supervision in RLHF paradigm, and continually aligns VLMs with a simple SFT loss. The effectiveness of nSFT is rigorously proved by comparing it with various multimodal RLHF approaches, across different dataset sources, base VLMs and evaluation metrics.",
    "source": "arXiv",
    "publication_date": "2024-11-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczcwhNPAO2z4UlV",
    "title": "Sycophancy in Large Language Models: Causes and Mitigations",
    "url": "http://arxiv.org/abs/2411.15287v1",
    "summary": "However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
    "source": "arXiv",
    "publication_date": "2024-11-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recZ2XvqFUJPjUSwP",
    "title": "RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?",
    "url": "http://arxiv.org/abs/2411.14368v1",
    "summary": "Chatbots have become integral to various application domains, including those with safety-critical considerations. As a result, there is a pressing need for methods that ensure chatbots consistently adhere to expected, safe behaviours. In this paper, we introduce RV4Chatbot, a Runtime Verification framework designed to monitor deviations in chatbot behaviour.",
    "source": "arXiv",
    "publication_date": "2024-11-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTNQBGPk2IDaiOD",
    "title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach",
    "url": "http://arxiv.org/abs/2411.14484v1",
    "summary": "Previous work has attempted to boost Large Language Model (LLM) performance on planning and scheduling tasks through a variety of prompt engineering techniques. This limitation can be addressed through compound LLM architectures where LLMs work in conjunction with other components to ensure reliability. Our results, evaluated across four scheduling domains, demonstrate significant performance gains with the LLM-Modulo framework using various models.",
    "source": "arXiv",
    "publication_date": "2024-11-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recEEFrepDqRNolZE",
    "title": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog Circuits",
    "url": "http://arxiv.org/abs/2411.12930v2",
    "summary": "Existing automation efforts using methods like Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal and costly to generalize across different topologies and technology nodes. In our work, we introduce a novel approach, LEDRO, utilizing Large Language Models (LLMs) in conjunction with optimization techniques to iteratively refine the design space for analog circuit sizing. LEDRO is highly generalizable compared to other RL and BO baselines, eliminating the need for design...",
    "source": "arXiv",
    "publication_date": "2024-11-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "reczaMIO9NWEQuT50",
    "title": "Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets",
    "url": "http://arxiv.org/abs/2411.11937v1",
    "summary": "To investigate the viability of this framework, we conducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human values embedded within them. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. During the second phase, we employed the labels generated from the annotation as ground truth data for training a tran...",
    "source": "arXiv",
    "publication_date": "2024-11-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec2FLAXr2WSU3xAB",
    "title": "Efficient Alignment of Large Language Models via Data Sampling",
    "url": "http://arxiv.org/abs/2411.10545v2",
    "summary": "Recent research depicts the benefit of data engineering in the fine-tuning and pre-training paradigms to bring down such costs. We find out that LLM alignment performance follows an exponential plateau pattern which tapers off post a rapid initial increase. Based on this, we identify data subsampling as a viable method to reduce resources required for alignment.",
    "source": "arXiv",
    "publication_date": "2024-11-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWSswlXiOYJSqaP",
    "title": "Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment",
    "url": "http://arxiv.org/abs/2411.10534v1",
    "summary": "We introduce a method to measure the alignment between public will and language model (LM) behavior that can be applied to fine-tuning, online oversight, and pre-release safety checks. We validate our approach by applying it across three different domains of LM prompts related to mental health. We then show that rules developed by mental health experts to achieve those objectives enable a RBR that evaluates an LM response's alignment with the objectives similarly to human experts (Pearson's $...",
    "source": "arXiv",
    "publication_date": "2024-11-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 33.0
  },
  {
    "id": "reca7hwN9SFuQ643l",
    "title": "Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention",
    "url": "http://arxiv.org/abs/2411.10156v5",
    "summary": "To address the sycophancy problem caused by reinforcement learning from human feedback in large language models, this research applies synthetic data intervention technology to the decoder-only transformer architecture. Based on the research gaps in the existing literature, the researcher designed an experimental process to reduce the tendency of models to cater by generating diversified data, and used GPT4o as an experimental tool for verification. The results show that the SDI training mode...",
    "source": "arXiv",
    "publication_date": "2024-11-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recq0OgTW22BZL0xC",
    "title": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment",
    "url": "http://arxiv.org/abs/2411.06835v1",
    "summary": "The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.",
    "source": "arXiv",
    "publication_date": "2024-11-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recsHBU55RYgOgGfp",
    "title": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity",
    "url": "http://arxiv.org/abs/2411.04427v3",
    "summary": "Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models' internal diversity. We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data. Our findings highlight potential trade-offs between increasing models' value alignment and decreasing the diversity of their conceptual representations.",
    "source": "arXiv",
    "publication_date": "2024-11-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec4DEGHorb8NLFUj",
    "title": "Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as Alternative Annotators",
    "url": "http://arxiv.org/abs/2411.05854v1",
    "summary": "Short video platforms, such as YouTube, Instagram, or TikTok, are used by billions of users globally. We analyze 19,422 YouTube videos using 14 image frames, 1 thumbnail, and text metadata, comparing the accuracy of crowdworkers (Mturk) and GPT-4-Turbo with domain expert annotations serving as the gold standard. Methodologically, this study extends the application of LLMs to multi-label and multi-modal contexts beyond text annotation and binary classification.",
    "source": "arXiv",
    "publication_date": "2024-11-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rectIlkJYwls1gaax",
    "title": "Representative Social Choice: From Learning Theory to AI Alignment",
    "url": "http://arxiv.org/abs/2410.23953v4",
    "summary": "Social choice theory is the study of preference aggregation across a population, used both in mechanism design for human agents and in the democratic alignment of language models. In this study, we propose the representative social choice framework for the modeling of democratic representation in collective decisions, where the number of issues and individuals are too large for mechanisms to consider all preferences directly. These scenarios are widespread in real-world decision-making proces...",
    "source": "arXiv",
    "publication_date": "2024-10-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJNzlxxkBSH570X",
    "title": "LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation",
    "url": "http://arxiv.org/abs/2410.21520v4",
    "summary": "However, challenges persist in designing effective prompts for a finetuning-free process and in mitigating biases and uncertainty in LLM outputs. To address these issues, we propose a novel framework, LLM-Forest, which introduces a \"forest\" of few-shot prompt learning LLM \"trees\" with their outputs aggregated via confidence-based weighted voting based on LLM self-assessment, inspired by the ensemble learning (Random Forest). This framework is established on a new concept of bipartite informat...",
    "source": "arXiv",
    "publication_date": "2024-10-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recZQhF9KaLEAZRhe",
    "title": "Strada-LLM: Graph LLM for traffic prediction",
    "url": "http://arxiv.org/abs/2410.20856v2",
    "summary": "By reasoning about traffic patterns in both the spatial and temporal dimensions, accurate and interpretable predictions can be provided. However, existing forecasting techniques mainly focus on extracting local graph information and forming a text-like prompt, leaving LLM- based traffic prediction an open problem. Furthermore, we adopt a lightweight approach for efficient domain adaptation when facing new data distributions in few-shot fashion.",
    "source": "arXiv",
    "publication_date": "2024-10-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdLeIJj0fFv881U",
    "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
    "url": "http://arxiv.org/abs/2410.20749v3",
    "summary": "Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative in...",
    "source": "arXiv",
    "publication_date": "2024-10-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxwkpg0NP5UVpkE",
    "title": "CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling",
    "url": "http://arxiv.org/abs/2410.21545v2",
    "summary": "In reinforcement learning from human feedback (RLHF) and more generally during post-training flawed reward signals often lead to outputs that optimize for these spurious correlates instead of genuine quality or correctness. Unlike prior methods that rely on static rubrics, CARMO leverages large language models (LLMs) to adaptively create evaluation criteria such as logical consistency, clarity, and depth tailored to the user query. We establish a new state-of-the-art performance in zero-shot ...",
    "source": "arXiv",
    "publication_date": "2024-10-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVsLDRHafcCP6Iz",
    "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way Intelligibility Protocol",
    "url": "http://arxiv.org/abs/2410.20600v4",
    "summary": "For complex problems, it is possible that LLMs can harness human expertise and creativity to find solutions that were otherwise elusive. On one level, this interaction takes place through multiple turns of prompts from the human and responses from the LLM. The protocol is motivated by a notion of \"two-way intelligibility\" and is modelled by a pair of communicating finite-state machines.",
    "source": "arXiv",
    "publication_date": "2024-10-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAjgPRNtH8JYK9L",
    "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
    "url": "http://arxiv.org/abs/2410.20238v2",
    "summary": "The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research r...",
    "source": "arXiv",
    "publication_date": "2024-10-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvVFTYXVg6kwsXx",
    "title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
    "url": "http://arxiv.org/abs/2410.18749v1",
    "summary": "Differential privacy (DP) is applied when fine-tuning pre-trained large language models (LLMs) to limit leakage of training examples. In this work, we show the impact of DP on bias in LLMs through empirical analysis. Our results also show that the impact of DP on bias is not only affected by the privacy protection level but also the underlying distribution of the dataset.",
    "source": "arXiv",
    "publication_date": "2024-10-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rechM7SUd6hetbrYS",
    "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models",
    "url": "http://arxiv.org/abs/2410.18252v3",
    "summary": "We tackle the fundamental challenge in this regime: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? We verify the scalability of asynchronous RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an instruction-following task ~40% faster than a synchronous run while matching final performance. Finally, we extend our results to math and reasoning to demonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster wh...",
    "source": "arXiv",
    "publication_date": "2024-10-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recr1aOmC2o0o5Zwd",
    "title": "Revealing Hidden Bias in AI: Lessons from Large Language Models",
    "url": "http://arxiv.org/abs/2410.16927v1",
    "summary": "This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promotin...",
    "source": "arXiv",
    "publication_date": "2024-10-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recgy2nlySQfHXT4W",
    "title": "Understanding and Alleviating Memory Consumption in RLHF for LLMs",
    "url": "http://arxiv.org/abs/2410.15651v1",
    "summary": "Fine-tuning with Reinforcement Learning with Human Feedback (RLHF) is essential for aligning large language models (LLMs). However, RLHF often encounters significant memory challenges. Additionally, we introduce a simple yet effective approach that substantially reduces the memory required for RLHF fine-tuning.",
    "source": "arXiv",
    "publication_date": "2024-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recobnHItI317t3ML",
    "title": "Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety",
    "url": "http://arxiv.org/abs/2410.16562v1",
    "summary": "To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. However, our paper argues that such taxonomies must also be transferred into local categories to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or high-risk sectors. Drawing from emerging anthropological theories of human rights, we propose that the process of \"vernacularization\"--a ...",
    "source": "arXiv",
    "publication_date": "2024-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 14.0
  },
  {
    "id": "recHQfeBTSEWRfJE5",
    "title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement",
    "url": "http://arxiv.org/abs/2410.14259v2",
    "summary": "The rapid development of large language models (LLMs), like ChatGPT, has resulted in the widespread presence of LLM-generated content on social media platforms, raising concerns about misinformation, data biases, and privacy violations, which can undermine trust in online discourse. While detecting LLM-generated content is crucial for mitigating these risks, current methods often focus on binary classification, failing to address the complexities of real-world scenarios like human-LLM collabo...",
    "source": "arXiv",
    "publication_date": "2024-10-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recya8Q1BPzkvZBS9",
    "title": "How to Evaluate Reward Models for RLHF",
    "url": "http://arxiv.org/abs/2410.14872v2",
    "summary": "The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowdsourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicit...",
    "source": "arXiv",
    "publication_date": "2024-10-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recXsRBJxBUHFoL45",
    "title": "Accounting for Sycophancy in Language Model Uncertainty Estimation",
    "url": "http://arxiv.org/abs/2410.14746v1",
    "summary": "Effective human-machine collaboration requires machine learning models to externalize uncertainty, so users can reflect and intervene when necessary. For language models, these representations of uncertainty may be impacted by sycophancy bias: proclivity to agree with users, even if they are wrong. From these results, we argue that externalizing both model and user uncertainty can help to mitigate the impacts of sycophancy bias.",
    "source": "arXiv",
    "publication_date": "2024-10-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rece1VYA6M0okOxxP",
    "title": "The Mystery of the Pathological Path-star Task for Language Models",
    "url": "http://arxiv.org/abs/2410.13779v2",
    "summary": "This is straightforward for a human but surprisingly difficult for language models, which did not outperform the random baseline. We demonstrate the task is learnable using teacher-forcing in alternative settings and that the issue is partially due to representation. We introduce a regularization method using structured samples of the same graph but with differing target nodes, improving results across a variety of model types.",
    "source": "arXiv",
    "publication_date": "2024-10-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIhbbzA9FhJh1Yg",
    "title": "Negative-Prompt-driven Alignment for Generative Language Model",
    "url": "http://arxiv.org/abs/2410.12194v1",
    "summary": "For instance, the widely-used alignment datasets reveals a scarcity of explicit negative examples that contradict human values, hindering its ability to discourage harmful or biased outputs during training. To address this limitation, we propose NEAT, i.e., NEgative-prompt-driven AlignmenT, to introduce negative prompts to generate undesirable responses alongside positive examples during the optimization process. Starting from a pre-trained language model, NEAT performs online alignment by in...",
    "source": "arXiv",
    "publication_date": "2024-10-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "receyUW5Pb7DhJF2O",
    "title": "First-Person Fairness in Chatbots",
    "url": "http://arxiv.org/abs/2410.19803v2",
    "summary": "Evaluating chatbot fairness is crucial given their rapid proliferation, yet typical chatbot tasks (e.g., resume writing, entertainment) diverge from the institutional decision-making tasks (e.g., resume screening) which have traditionally been central to discussion of algorithmic fairness. Our method employs a Language Model as a Research Assistant (LMRA) to yield quantitative measures of harmful stereotypes and qualitative analyses of demographic differences in chatbot responses. We apply th...",
    "source": "arXiv",
    "publication_date": "2024-10-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recfwNAW6ToD7eEZc",
    "title": "SoK: Prompt Hacking of Large Language Models",
    "url": "http://arxiv.org/abs/2410.13901v1",
    "summary": "In this work, we offer a comprehensive and systematic overview of three distinct types of prompt hacking: jailbreaking, leaking, and injection, addressing the nuances that differentiate them despite their overlapping characteristics. To enhance the evaluation of LLM-based applications, we propose a novel framework that categorizes LLM responses into five distinct classes, moving beyond the traditional binary classification. This approach provides more granular insights into the AI's behavior,...",
    "source": "arXiv",
    "publication_date": "2024-10-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recg029iz9rH6KnPc",
    "title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
    "url": "http://arxiv.org/abs/2410.12601v3",
    "summary": "To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain. We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes. Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control att...",
    "source": "arXiv",
    "publication_date": "2024-10-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvyCVkynx5AS0R6",
    "title": "MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration",
    "url": "http://arxiv.org/abs/2410.12532v3",
    "summary": "To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Experimental results f...",
    "source": "arXiv",
    "publication_date": "2024-10-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec0RszhqkX3T9sG3",
    "title": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models",
    "url": "http://arxiv.org/abs/2410.11459v1",
    "summary": "Large language models (LLMs) have exhibited outstanding performance in engaging with humans and addressing complex questions by leveraging their vast implicit knowledge and robust reasoning capabilities. Despite recent research on single-turn jailbreak strategies to facilitate the development of defence mechanisms, the challenge of revealing vulnerabilities under multi-turn setting remains relatively under-explored. Moreover, JSP achieves a state-of-the-art attack success rate of 92% on GPT-4...",
    "source": "arXiv",
    "publication_date": "2024-10-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recPaZvrHAy44a0sa",
    "title": "Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning",
    "url": "http://arxiv.org/abs/2410.12096v1",
    "summary": "In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to filter noise in the raw data and extract valuable cleaned information as features, enhancing the synergy of downstream models. The LM and GSLM work synergistically, complementing each other's strengths and offsetting weaknesses within a variational in...",
    "source": "arXiv",
    "publication_date": "2024-10-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recfTQwLP57uP6jLu",
    "title": "Gender Bias of LLM in Economics: An Existentialism Perspective",
    "url": "http://arxiv.org/abs/2410.19775v1",
    "summary": "Drawing on existentialist theory, we argue that LLM-generated bias reflects entrenched societal structures and highlights the limitations of purely technical debiasing methods. This research underscores the need for new theoretical frameworks and interdisciplinary methodologies that address the ethical implications of integrating LLMs into economic and financial decision-making. We advocate for a reconceptualization of how LLMs influence economic decisions, emphasizing the importance of incor...",
    "source": "arXiv",
    "publication_date": "2024-10-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recexiMieLFH1fF4i",
    "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
    "url": "http://arxiv.org/abs/2410.09724v2",
    "summary": "We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. Experimental results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standa...",
    "source": "arXiv",
    "publication_date": "2024-10-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recCMzp0YS8wXw6Yd",
    "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking",
    "url": "http://arxiv.org/abs/2410.08851v1",
    "summary": "Despite large language models' (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positiona...",
    "source": "arXiv",
    "publication_date": "2024-10-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recpn2uHyQFAWMBZr",
    "title": "SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction",
    "url": "http://arxiv.org/abs/2410.09008v3",
    "summary": "Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have shown significant improvements in various reasoning tasks. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by fo...",
    "source": "arXiv",
    "publication_date": "2024-10-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recsLsFJNmnbXlVKM",
    "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
    "url": "http://arxiv.org/abs/2410.09024v3",
    "summary": "Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
    "source": "arXiv",
    "publication_date": "2024-10-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recEWfXTH1X3FcWBt",
    "title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models",
    "url": "http://arxiv.org/abs/2410.06554v2",
    "summary": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform...",
    "source": "arXiv",
    "publication_date": "2024-10-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recouzR5mqR6G5hhq",
    "title": "Better Language Models Exhibit Higher Visual Alignment",
    "url": "http://arxiv.org/abs/2410.07173v2",
    "summary": "We provide the first direct analysis by utilizing frozen text representations in a discriminative vision-language model framework and measuring zero-shot generalization on unseen classes. Moreover, utilizing frozen LLMs leads to strong gains in cross-lingual settings, where our approach surpasses CLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves both robustness and generalization and also significantly reduces the need for paired data and compute, making vision-lang...",
    "source": "arXiv",
    "publication_date": "2024-10-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPp9abJka54mmRP",
    "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
    "url": "http://arxiv.org/abs/2410.05451v3",
    "summary": "To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training.",
    "source": "arXiv",
    "publication_date": "2024-10-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec93Ilyn4ynWdvgS",
    "title": "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity",
    "url": "http://arxiv.org/abs/2410.03953v1",
    "summary": "Combining large language models during training or at inference time has shown substantial performance gain over component LLMs. (ii) We develop a diversity-optimized ensemble pruning algorithm to select the top-k sub-ensembles from a pool of $N$ base LLMs. (ii) In generative tasks, LLM-TOPLA outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by $3.9\\\\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1.",
    "source": "arXiv",
    "publication_date": "2024-10-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recaV1mpf3ua2qlwX",
    "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models",
    "url": "http://arxiv.org/abs/2410.03129v2",
    "summary": "Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reducing the high demands on computation and memory. To tackle these issues, we propose ARB-LLM, a novel 1-bit post-training quantization (PTQ) technique tailored for LLMs. Moreover, considering the pivot role of calibration data and the column deviation in LLM weights, we further extend ARB to ARB-X and ARB-RC.",
    "source": "arXiv",
    "publication_date": "2024-10-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reck5y39CVUdMZN4o",
    "title": "Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs",
    "url": "http://arxiv.org/abs/2410.05306v1",
    "summary": "The European Union's Artificial Intelligence Act seeks to enforce AI robustness in certain contexts, but faces implementation challenges due to the lack of standards, complexity of LLMs and emerging security vulnerabilities. Our research introduces a framework using ontologies, assurance cases, and factsheets to support engineers and stakeholders in understanding and documenting AI system compliance and security regarding adversarial robustness. This approach aims to ensure that LLMs adhere t...",
    "source": "arXiv",
    "publication_date": "2024-10-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7rZvpmgA6TOm2S",
    "title": "Towards Comprehensive Detection of Chinese Harmful Memes",
    "url": "http://arxiv.org/abs/2410.02378v1",
    "summary": "Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effective...",
    "source": "arXiv",
    "publication_date": "2024-10-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recTET5ITxpFCYFmb",
    "title": "Morphological evaluation of subwords vocabulary used by BETO language model",
    "url": "http://arxiv.org/abs/2410.02283v1",
    "summary": "However, those subwords do not always align with real morphemes, potentially impacting the models' performance, though it remains uncertain when this might occur. In this article, we apply this evaluation to the tokenizer of BETO, a BERT language model trained on large Spanish corpora. Additionally, this evaluation helps clarify the algorithm used by the tokenizer, that is, Wordpiece, given the inconsistencies between the authors' claims and the model's configuration.",
    "source": "arXiv",
    "publication_date": "2024-10-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recmEgkZnCp9xsFz1",
    "title": "LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services",
    "url": "http://arxiv.org/abs/2410.02425v1",
    "summary": "As Large Language Models (LLMs) are rapidly growing in popularity, LLM inference services must be able to serve requests from thousands of users while satisfying performance requirements. Finally, using this characterization data, LLM-Pilot learns a predictive model, which can be used to recommend the most cost-effective hardware for a previously unseen LLM. Compared to existing methods, LLM-Pilot can deliver on performance requirements 33% more frequently, whilst reducing costs by 60% on ave...",
    "source": "arXiv",
    "publication_date": "2024-10-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recKjnRP8bU67R8CV",
    "title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with LLM-Guided Knowledge",
    "url": "http://arxiv.org/abs/2410.01458v1",
    "summary": "This approach is both general and robust across diverse tasks, allowing for immediate impact assessment while guaranteeing optimality. We evaluated Q-shaping across 20 different environments using a large language model (LLM) as the heuristic provider. These findings establish Q-shaping as a superior and unbiased alternative to conventional reward shaping in reinforcement learning.",
    "source": "arXiv",
    "publication_date": "2024-10-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recKmcXBD8AssWzmR",
    "title": "Generative AI and Perceptual Harms: Who's Suspected of using LLMs?",
    "url": "http://arxiv.org/abs/2410.00906v3",
    "summary": "Large language models (LLMs) are increasingly integrated into a variety of writing tasks. We examined perceptual harms in three online experiments, each of which entailed human participants evaluating the profiles for fictional freelance writers. We found some support for perceptual harms against for certain demographic groups, but that perceptions of AI use negatively impacted writing evaluations and hiring outcomes across the board.",
    "source": "arXiv",
    "publication_date": "2024-10-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rectFVfSn1j8NvmkB",
    "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
    "url": "http://arxiv.org/abs/2410.00531v1",
    "summary": "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allreduce algorithm i...",
    "source": "arXiv",
    "publication_date": "2024-10-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recnJo0gmCWAmbKCA",
    "title": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character Pre-training in LLMs",
    "url": "http://arxiv.org/abs/2409.16005v1",
    "summary": "While LLMs excel in multimodal understanding tasks, effectively leveraging their capabilities for ASR remains a significant challenge. We propose pre-training LLMs on Pinyin embedding sequences, which represent pronunciation features, to generate corresponding Chinese characters. This step enables the LLM to adapt to generating text from pronunciation features before encountering real speech data.",
    "source": "arXiv",
    "publication_date": "2024-09-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec6NNQbrUPrTjmAe",
    "title": "Behavioral Bias of Vision-Language Models: A Behavioral Finance View",
    "url": "http://arxiv.org/abs/2409.15256v1",
    "summary": "However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. Our evaluations find that recent open-source LVLMs such as LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5 and Phi-3-vision-128k suffer significantly from these two biases, while the proprietary ...",
    "source": "arXiv",
    "publication_date": "2024-09-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recp1JZ1rCQDkUhtV",
    "title": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models",
    "url": "http://arxiv.org/abs/2409.15188v2",
    "summary": "Emerging large language models (LLMs) offer a new approach to assessing complex communication metrics, with the potential to advance the field through integration into passive sensing and just-in-time intervention systems. Specifically, using simulated scripts crafted and labeled by healthcare professionals, we test proprietary models (e.g., GPT-4) and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset generated by GPT-4 to evaluate clinical conversations, to identify key metr...",
    "source": "arXiv",
    "publication_date": "2024-09-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recV7tfo092sAkv8P",
    "title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal",
    "url": "http://arxiv.org/abs/2409.13948v3",
    "summary": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other non-verbal cues. Building upon the FLR mechanism, we propose to automatically mine preference data from the online generations of a base policy model.",
    "source": "arXiv",
    "publication_date": "2024-09-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec5rXGRlJK7qyir9",
    "title": "Evaluating Defences against Unsafe Feedback in RLHF",
    "url": "http://arxiv.org/abs/2409.12914v3",
    "summary": "While there has been progress towards aligning Large Language Models (LLMs) with human values and ensuring safe behaviour at inference time, safety guards can easily be removed when fine tuned on unsafe and harmful datasets. We address this gap by providing an analysis of learning settings where feedback is harmful, i.e. that unsafe samples are preferred over safe ones despite model developers goal to maintain safety. We find that safety-aligned LLMs easily explore unsafe action spaces via ge...",
    "source": "arXiv",
    "publication_date": "2024-09-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recjd68QYcMpF47qy",
    "title": "Language Models Learn to Mislead Humans via RLHF",
    "url": "http://arxiv.org/abs/2409.12822v3",
    "summary": "RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. RLHF also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY.",
    "source": "arXiv",
    "publication_date": "2024-09-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3GSM4P4edmuLPD",
    "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension",
    "url": "http://arxiv.org/abs/2409.11919v3",
    "summary": "LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved with a light fine-tuning, to select the most relevant bounding box matching the referring expression, from candidates generated by a zero-shot black-box VLM. We evaluate LLM-wrapper on multiple datasets using different VLMs and LLMs, demonstrating significant performance improvements and highlighting the versatility of our method. While LLM-wrapper is not meant to directly compete with standard white-box fine-tuning, it offe...",
    "source": "arXiv",
    "publication_date": "2024-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reca46xSMPM9j9DII",
    "title": "LLMs + Persona-Plug = Personalized LLMs",
    "url": "http://arxiv.org/abs/2409.11901v1",
    "summary": "Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their individual interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing mo...",
    "source": "arXiv",
    "publication_date": "2024-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recaWYG2kDntP4XVf",
    "title": "Reward-Robust RLHF in LLMs",
    "url": "http://arxiv.org/abs/2409.15360v3",
    "summary": "In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect RMs. Empirical results demonstrate that our framework consistently outperforms baselines across diverse benchmarks, showing improved accuracy and long-term stability.",
    "source": "arXiv",
    "publication_date": "2024-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recZJKGqMDIMEYYij",
    "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models",
    "url": "http://arxiv.org/abs/2409.11353v3",
    "summary": "Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation.",
    "source": "arXiv",
    "publication_date": "2024-09-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recJ7CtxvK55ZstsZ",
    "title": "Quantile Regression for Distributional Reward Models in RLHF",
    "url": "http://arxiv.org/abs/2409.10164v1",
    "summary": "However, traditional reward models typically generate point estimates, which oversimplify the diversity and complexity of human values and preferences. Our experimental results show that QRM outperforms comparable traditional point-estimate models on RewardBench. Furthermore, we demonstrate that the additional information provided by the distributional estimates can be utilized in downstream applications, such as risk-aware reinforcement learning, resulting in LLM policies that generate fewer...",
    "source": "arXiv",
    "publication_date": "2024-09-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recd9dlEjE56ZUHyf",
    "title": "Generating API Parameter Security Rules with LLM for API Misuse Detection",
    "url": "http://arxiv.org/abs/2409.09288v2",
    "summary": "Subsequently, GPTAid performs dynamic execution on each piece of Violation code and further filters out the incorrect APSRs based on runtime errors. To further generate concrete APSRs, GPTAid employs a code differential analysis to refine the filtered ones. Implementing on the dataset containing 200 randomly selected APIs from eight popular libraries, GPTAid achieves a precision of 92.3%.",
    "source": "arXiv",
    "publication_date": "2024-09-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQNJBucwDPsptrB",
    "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
    "url": "http://arxiv.org/abs/2409.07253v3",
    "summary": "Despite their success, these models often misalign with human intentions and generate results with undesired properties or even harmful content. Inspired by the success and popularity of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models.",
    "source": "arXiv",
    "publication_date": "2024-09-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recvFLoI7sFjCosHm",
    "title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
    "url": "http://arxiv.org/abs/2409.07054v2",
    "summary": "To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 12 different Arabic datasets (9.7K data points).",
    "source": "arXiv",
    "publication_date": "2024-09-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTZ23FENuXbQwm1",
    "title": "Enhancing Critical Thinking in Education by means of a Socratic Chatbot",
    "url": "http://arxiv.org/abs/2409.05511v1",
    "summary": "While large language models (LLMs) are increasingly playing a pivotal role in education by providing instantaneous, adaptive responses, their potential to promote critical thinking remains understudied. Our Socratic questioning is implemented by fine and prompt-tuning the open-source pretrained LLM with a specialized dataset that stimulates critical thinking and offers multiple viewpoints. In an effort to democratize access and to protect the students' privacy, the proposed tutor is based on ...",
    "source": "arXiv",
    "publication_date": "2024-09-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDumf0yHaiIxS4A",
    "title": "Unforgettable Generalization in Language Models",
    "url": "http://arxiv.org/abs/2409.02228v1",
    "summary": "Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. Perhaps most surprisingly, random-label forgetting appears to be somewhat insensitive to the contents of the training set: for example, models trained on science questions with random labels continue to answer other science questions accurately, but begin to produce random labels on entailment classification tasks. Our results highlight the difficulty and unpredictabil...",
    "source": "arXiv",
    "publication_date": "2024-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recO9VYfE5UfpVAMw",
    "title": "Focus Agent: LLM-Powered Virtual Focus Group",
    "url": "http://arxiv.org/abs/2409.01907v1",
    "summary": "In the domain of Human-Computer Interaction, focus groups represent a widely utilised yet resource-intensive methodology, often demanding the expertise of skilled moderators and meticulous preparatory efforts. Quantitative analysis indicates that Focus Agent can generate opinions similar to those of human participants. Furthermore, the research exposes some improvements associated with LLMs acting as moderators in focus group discussions that include human participants.",
    "source": "arXiv",
    "publication_date": "2024-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recR7sUASOdJpEvN6",
    "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
    "url": "http://arxiv.org/abs/2409.01586v4",
    "summary": "While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks.",
    "source": "arXiv",
    "publication_date": "2024-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recXqVIrKbKUawJG5",
    "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning",
    "url": "http://arxiv.org/abs/2409.01658v3",
    "summary": "Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. Specifically, SPT first reveals and verifies a small percentage (<5%) of the basic modules, which significantly affect a particular behavior of LLMs.",
    "source": "arXiv",
    "publication_date": "2024-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recXydZUjt9On1gSH",
    "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
    "url": "http://arxiv.org/abs/2409.02074v1",
    "summary": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the ...",
    "source": "arXiv",
    "publication_date": "2024-09-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxeQwdzNtfyZUTX",
    "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback",
    "url": "http://arxiv.org/abs/2409.00162v1",
    "summary": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Our experiments demonstrated its effectiveness, specifically, reducing the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks. We provide further analysis that seq2seq RM improves RLHF performance across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\\\\%.",
    "source": "arXiv",
    "publication_date": "2024-08-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMB5beuDKXAU98t",
    "title": "CBF-LLM: Safe Control for LLM Alignment",
    "url": "http://arxiv.org/abs/2408.15625v2",
    "summary": "This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.",
    "source": "arXiv",
    "publication_date": "2024-08-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdVnwzUPYvgnBbX",
    "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
    "url": "http://arxiv.org/abs/2408.15204v2",
    "summary": "Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quan...",
    "source": "arXiv",
    "publication_date": "2024-08-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOgYefWSF6m9fkF",
    "title": "Using the SOCIO Chatbot for UML Modelling: A Family of Experiments",
    "url": "http://arxiv.org/abs/2408.14085v1",
    "summary": "Context: Recent developments in natural language processing have facilitated the adoption of chatbots in typically collaborative software engineering tasks (such as diagram modelling). Results: The student participants were faster at building class diagrams using the chatbot than with the online collaborative tool and more satisfied with SOCIO. In fact, our study has helped us to shed light on the future direction for experimentation in this field and lays the groundwork for researching the a...",
    "source": "arXiv",
    "publication_date": "2024-08-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recASbko0LkmvjHQH",
    "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks",
    "url": "http://arxiv.org/abs/2408.13040v1",
    "summary": "Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.",
    "source": "arXiv",
    "publication_date": "2024-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRrKpA6VEy6preS",
    "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
    "url": "http://arxiv.org/abs/2408.13006v2",
    "summary": "Previous research has developed evaluation frameworks to assess reliability of LLM judges and their alignment with human preferences. Additionally, existing studies inadequately explore the impact of various prompt templates when applying LLM-as-a-Judge methods, leading to potentially inconsistent comparisons between different alignment algorithms. In the experiments, we examine effects of diverse prompt templates on LLM-judge reliability and also demonstrate our developed framework by compar...",
    "source": "arXiv",
    "publication_date": "2024-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec272JDkmHEfiT7w",
    "title": "Sycophancy in Vision-Language Models: A Systematic Analysis and an Inference-Time Mitigation Framework",
    "url": "http://arxiv.org/abs/2408.11261v2",
    "summary": "We curate leading queries and quantify the susceptibility of state-of-the-art LVLMs to prompt-induced bias, revealing consistent performance degradation and instability across models and tasks. Extensive experiments demonstrate that this framework effectively mitigates sycophancy across all evaluated models, while maintaining performance on neutral prompts. Our results suggest that sycophancy in LVLMs is a general and urgent challenge, and that inference-time strategies offer a promising path...",
    "source": "arXiv",
    "publication_date": "2024-08-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recN9wDWM8IDdyuaD",
    "title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search",
    "url": "http://arxiv.org/abs/2408.10635v3",
    "summary": "Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data.",
    "source": "arXiv",
    "publication_date": "2024-08-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec84R2XmJrdplqYI",
    "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning",
    "url": "http://arxiv.org/abs/2408.09600v3",
    "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \\\\textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. Despite its embarrassing simplicit...",
    "source": "arXiv",
    "publication_date": "2024-08-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 31.0
  },
  {
    "id": "recDMXbHLoRvkrb35",
    "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF",
    "url": "http://arxiv.org/abs/2408.09385v2",
    "summary": "As such, offline RLHF has been introduced as an alternative solution, which directly optimizes LLMs with ranking losses on a fixed preference dataset. Current offline RLHF only captures the \"ordinal relationship\" between responses, overlooking the crucial aspect of how much one is preferred over the others. To address this issue, we propose a simple yet effective solution called Reward Difference Optimization, shorted as RDO.",
    "source": "arXiv",
    "publication_date": "2024-08-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recbQa8EIP9ITqRYB",
    "title": "Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges",
    "url": "http://arxiv.org/abs/2408.08946v2",
    "summary": "Accurate attribution of authorship is crucial for maintaining the integrity of digital content, improving forensic investigations, and mitigating the risks of misinformation and plagiarism. The rapid advancements of Large Language Models (LLMs) have blurred the lines between human and machine authorship, posing significant challenges for traditional methods. By evaluating the strengths and limitations of existing methods and benchmarks, we identify key open problems and future research direct...",
    "source": "arXiv",
    "publication_date": "2024-08-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recPdVlnZXizBNzy7",
    "title": "More Questions than Answers? Lessons from Integrating Explainable AI into a Cyber-AI Tool",
    "url": "http://arxiv.org/abs/2408.04746v1",
    "summary": "Specifically, we briefly describe a preliminary case study on the use of XAI for source code classification, where accurate assessment and timeliness are paramount. Moreover, we find that popular XAI techniques offer fewer insights for real-time human-AI workflows when they are post hoc and too localized in their explanations. We outline unaddressed gaps in practical and effective XAI, then touch on how emerging technologies like Large Language Models (LLMs) could mitigate these existing obst...",
    "source": "arXiv",
    "publication_date": "2024-08-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recN1QwXv5bbSfMqy",
    "title": "Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools",
    "url": "http://arxiv.org/abs/2408.04650v2",
    "summary": "Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader ...",
    "source": "arXiv",
    "publication_date": "2024-08-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 33.0
  },
  {
    "id": "recfSoc1koP6Xp3wz",
    "title": "Efficacy of Large Language Models in Systematic Reviews",
    "url": "http://arxiv.org/abs/2408.04646v2",
    "summary": "This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. We evaluated two current state-of-the-art LLMs, Meta AI's Llama 3 8B and OpenAI's GPT-4o, on the accuracy of their interpretations relative to human-made classifications on both sets of papers. Our findings reveal promising results for investors and ag...",
    "source": "arXiv",
    "publication_date": "2024-08-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec75fEkNnOzBqn1m",
    "title": "An FDA for AI? Pitfalls and Plausibility of Approval Regulation for Frontier Artificial Intelligence",
    "url": "http://arxiv.org/abs/2408.00821v1",
    "summary": "There are a number of reasons to believe that approval regulation, simplistically applied, would be inapposite for frontier AI risks. Domains of weak fit include the difficulty of defining the regulated product, the presence of Knightian uncertainty or deep ambiguity about harms from AI, the potentially transmissible nature of risks, and distributed activities among actors involved in the AI lifecycle. We conclude by highlighting the role of policy learning and experimentation in regulatory d...",
    "source": "arXiv",
    "publication_date": "2024-08-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 14.0
  },
  {
    "id": "recAsb4RzEXIwWqA1",
    "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
    "url": "http://arxiv.org/abs/2408.00307v1",
    "summary": "Human preferences are highly distributed and can be captured at multiple levels of abstraction, from the individual to diverse populations. Organisational preferences, represented by standards and principles, are defined to mitigate reputational risk or meet legislative obligations. Our unified approach mitigates bias and improves accuracy, while preserving reasoning capability, as measured against standard benchmarks.",
    "source": "arXiv",
    "publication_date": "2024-08-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "reclrXxSQNdTNPFDR",
    "title": "Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning",
    "url": "http://arxiv.org/abs/2408.00690v2",
    "summary": "In contrast, smaller language models such as MiniCPM offer more sustainable scalability, but often underperform without specialized optimization. We select three language models, MiniCPM, Phi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our results demonstrate that this fine-tuning method enhances the quality of text embeddings for all three models across various benchmarks, with MiniCPM showing the most significant improvements of an average 56.33% performance gain.",
    "source": "arXiv",
    "publication_date": "2024-08-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recVy7regxmvS4VdF",
    "title": "Exploring Large Language Models to generate Easy to Read content",
    "url": "http://arxiv.org/abs/2407.20046v1",
    "summary": "The study contributes a parallel corpus of Spanish adapted for Easy To Read format, which serves as a valuable resource for training and testing text simplification systems. Additionally, several text simplification experiments using LLMs and the collected corpus are conducted, involving fine-tuning and testing a Llama2 model to generate Easy to Read content. This research contributes to advancing text accessibility for individuals with cognitive impairments, highlighting promising strategies...",
    "source": "arXiv",
    "publication_date": "2024-07-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recatduqtj5hbt7lG",
    "title": "Can Editing LLMs Inject Harm?",
    "url": "http://arxiv.org/abs/2407.20224v3",
    "summary": "Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into LLMs? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Then, we further illustrate the high stealthiness of editing attacks, measured by their impact on the general knowledge and reasoning capacities of LLMs, and show the hardness of...",
    "source": "arXiv",
    "publication_date": "2024-07-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 38.0
  },
  {
    "id": "reccPFey1bh6lEKfj",
    "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2407.19619v1",
    "summary": "This paper introduces a novel approach that enhances code translation through Few-Shot Learning, augmented with retrieval-based techniques. Our method, based on Retrieval-Augmented Generation (RAG), substantially improves translation quality by providing contextual examples from which the model can learn in real-time. We selected RAG over traditional fine-tuning methods due to its ability to utilize existing codebases or a locally stored corpus of code, which allows for dynamic adaptation to ...",
    "source": "arXiv",
    "publication_date": "2024-07-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrsmKroPOoQDKOk",
    "title": "GermanPartiesQA: Benchmarking Commercial Large Language Models for Political Bias and Sycophancy",
    "url": "http://arxiv.org/abs/2407.18008v1",
    "summary": "LLMs are changing the way humans create and interact with content, potentially affecting citizens' political opinions and voting decisions. As LLMs increasingly shape our digital information ecosystems, auditing to evaluate biases, sycophancy, or steerability has emerged as an active field of research. First, we develop the benchmark dataset GermanPartiesQA based on the Voting Advice Application Wahl-o-Mat covering 10 state and 1 national elections between 2021 and 2023.",
    "source": "arXiv",
    "publication_date": "2024-07-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recvudaiKBM2FzTN6",
    "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
    "url": "http://arxiv.org/abs/2407.18370v1",
    "summary": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. As part of our framework, we also introduce Simulated Annotators, a novel confidence estimation method that significantly improves judge calibration and thus e...",
    "source": "arXiv",
    "publication_date": "2024-07-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recg559wyzMYIGjj8",
    "title": "Towards Aligning Language Models with Textual Feedback",
    "url": "http://arxiv.org/abs/2407.16970v3",
    "summary": "Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefits of RL-based alignment algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response generation. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on ...",
    "source": "arXiv",
    "publication_date": "2024-07-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec6EwAb0gyu2YYP4",
    "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
    "url": "http://arxiv.org/abs/2407.15549v3",
    "summary": "For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes ...",
    "source": "arXiv",
    "publication_date": "2024-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recwKEa5UPyzj7h0U",
    "title": "Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models",
    "url": "http://arxiv.org/abs/2407.15645v1",
    "summary": "Language models (LMs) are increasingly used to simulate human-like responses in scenarios where accurately mimicking a population's behavior can guide decision-making, such as in developing educational materials and designing public policies. The objective of these simulations is for LMs to capture the variations in human responses, rather than merely providing the expected correct answers. To address this, we introduce \"psychometric alignment,\" a metric that measures the extent to which LMs ...",
    "source": "arXiv",
    "publication_date": "2024-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recU1zykAYfR65UuS",
    "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
    "url": "http://arxiv.org/abs/2407.11969v4",
    "summary": "Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., \"How to make a Molotov cocktail?\" Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions.",
    "source": "arXiv",
    "publication_date": "2024-07-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recUysj2oRGrbOQQp",
    "title": "How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies",
    "url": "http://arxiv.org/abs/2407.11733v2",
    "summary": "With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on 'safety' training concerning legal liabilities at the expense of social impact evaluation. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We address model builders, academics, NLP practitioners and policy ma...",
    "source": "arXiv",
    "publication_date": "2024-07-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recAWnN9BnBZ1jcvL",
    "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
    "url": "http://arxiv.org/abs/2407.10804v1",
    "summary": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. To avoid catastrophic forgetting during the continual pre-training process, we further incorporate a logit swap self-distillation constraint. Extensive experiments demonstrate that our proposed Mix-CPT framework can simultaneously improve the task-solving capabilities of LLMs on the target and general domains compared to the traditional adaptation methods.",
    "source": "arXiv",
    "publication_date": "2024-07-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGHy5bNllANyQ1M",
    "title": "A Critical Review of Causal Reasoning Benchmarks for Large Language Models",
    "url": "http://arxiv.org/abs/2407.08029v1",
    "summary": "Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. We hope this work will pave the way towards a general framework for the assessment of causal understanding in LLMs and the design of novel benchmarks.",
    "source": "arXiv",
    "publication_date": "2024-07-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDeDR1UiwUseuhU",
    "title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation",
    "url": "http://arxiv.org/abs/2407.07093v1",
    "summary": "This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regula...",
    "source": "arXiv",
    "publication_date": "2024-07-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJIHWt26vlZ9xsx",
    "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making",
    "url": "http://arxiv.org/abs/2407.06567v3",
    "summary": "Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Additionally, a risk-control component in FinCon enhances decisi...",
    "source": "arXiv",
    "publication_date": "2024-07-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recIAhrIulmqTUa3i",
    "title": "From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs",
    "url": "http://arxiv.org/abs/2407.04434v1",
    "summary": "Therefore, adapting linguistic structures within LLM training data to promote gender-inclusivity can make gender representations within the model more inclusive. Fine-tuning three different LLMs with this dataset, we observe an overall reduction in gender-stereotyping tendencies across the models. Our approach provides a practical method for enhancing gender inclusivity in LLM training data and contributes to incorporating queer-feminist linguistic activism in bias mitigation research in NLP.",
    "source": "arXiv",
    "publication_date": "2024-07-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbr6h5uoLk525sG",
    "title": "Spontaneous Reward Hacking in Iterative Self-Refinement",
    "url": "http://arxiv.org/abs/2407.04549v1",
    "summary": "In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator's ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinemen...",
    "source": "arXiv",
    "publication_date": "2024-07-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 22.0
  },
  {
    "id": "rectxsHy8vSebnwoX",
    "title": "When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings",
    "url": "http://arxiv.org/abs/2407.04503v3",
    "summary": "As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. We also find that different text properties display different sensitivity to attraction effects, with...",
    "source": "arXiv",
    "publication_date": "2024-07-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recvZ90iNuTlLoDQj",
    "title": "Automating Venture Capital: Founder assessment using LLM-powered segmentation, feature engineering and automated labeling techniques",
    "url": "http://arxiv.org/abs/2407.04885v1",
    "summary": "This study explores the application of large language models (LLMs) in venture capital (VC) decision-making, focusing on predicting startup success based on founder characteristics. We utilize LLM prompting techniques, like chain-of-thought, to generate features from limited data, then extract insights through statistics and machine learning. This framework for integrating ML techniques and LLMs has vast potential for improving startup success prediction, with important implications for VC fi...",
    "source": "arXiv",
    "publication_date": "2024-07-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reciSRyzgmDkTO8Je",
    "title": "Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems",
    "url": "http://arxiv.org/abs/2407.04014v1",
    "summary": "However, the energy consumed through LLM model inference remains a major challenge for sustainable AI deployment. To address this problem, we model the workload-dependent energy consumption and runtime of LLM inference tasks on heterogeneous GPU-CPU systems. Through a case study, we demonstrate the advantages of energy and accuracy aware scheduling compared to existing best practices.",
    "source": "arXiv",
    "publication_date": "2024-07-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recmTCWtEG25gUOhz",
    "title": "LLM Roleplay: Simulating Human-Chatbot Interaction",
    "url": "http://arxiv.org/abs/2407.03974v2",
    "summary": "In this paper, we propose LLM Roleplay: a goal-oriented, persona-based method to automatically generate diverse multi-turn dialogues simulating human-chatbot interaction. LLM Roleplay can be applied to generate dialogues with any type of chatbot and uses large language models (LLMs) to play the role of textually described personas. We evaluate the capabilities of state-of-the-art LLMs in maintaining a conversation during their embodiment of a specific persona and find that our method can simu...",
    "source": "arXiv",
    "publication_date": "2024-07-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec8kncTkX561LwFV",
    "title": "Towards Federated RLHF with Aggregated Client Preference for LLMs",
    "url": "http://arxiv.org/abs/2407.03038v3",
    "summary": "To address this, we propose utilizing Federated Learning (FL) techniques, allowing large-scale preference collection from diverse real-world users without requiring them to transmit data to a central server. To evaluate the performance of the proposed methods, we establish the first federated RLHF benchmark with a heterogeneous human preference dataset. Experimental results show that by integrating the LLM with aggregated client preferences, FedBis and FedBiscuit significantly enhance the pro...",
    "source": "arXiv",
    "publication_date": "2024-07-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPA74xj0YBvDVKg",
    "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias",
    "url": "http://arxiv.org/abs/2407.03536v3",
    "summary": "The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla.",
    "source": "arXiv",
    "publication_date": "2024-07-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recCUlZmvt15M8W4h",
    "title": "Language Model Alignment in Multilingual Trolley Problems",
    "url": "http://arxiv.org/abs/2407.02273v6",
    "summary": "We evaluate the moral alignment of LLMs with human preferences in multilingual trolley problems. Our analysis explores the alignment of 19 different LLMs with human judgments, capturing preferences across six moral dimensions: species, gender, fitness, status, age, and the number of lives involved. By correlating these preferences with the demographic distribution of language speakers and examining the consistency of LLM responses to various prompt paraphrasings, our findings provide insights...",
    "source": "arXiv",
    "publication_date": "2024-07-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWO5LA7PYInmISZ",
    "title": "Open foundation models for Azerbaijani language",
    "url": "http://arxiv.org/abs/2407.02337v2",
    "summary": "While there have been several attempts to develop open foundation models for Azerbaijani, these works have not found their way into common use due to a lack of systemic benchmarking. This paper encompasses several lines of work that promote open-source foundation models for Azerbaijani. We introduce (1) a large text corpus for Azerbaijani, (2) a family of encoder-only language models trained on this dataset, (3) labeled datasets for evaluating these models, and (4) extensive evaluation that c...",
    "source": "arXiv",
    "publication_date": "2024-07-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recCumortAtPgKa2A",
    "title": "Self-Cognition in Large Language Models: An Exploratory Study",
    "url": "http://arxiv.org/abs/2407.01505v1",
    "summary": "While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-cognition. We observe a positive correlation between model size, training data quality, and self-cognition level.",
    "source": "arXiv",
    "publication_date": "2024-07-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPR8v7m6ytqy5FO",
    "title": "Exploring Advanced Large Language Models with LLMsuite",
    "url": "http://arxiv.org/abs/2407.12036v2",
    "summary": "This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, es...",
    "source": "arXiv",
    "publication_date": "2024-07-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recReIgPlEkdGXijn",
    "title": "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization",
    "url": "http://arxiv.org/abs/2407.00693v2",
    "summary": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user pref...",
    "source": "arXiv",
    "publication_date": "2024-06-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 16.0
  },
  {
    "id": "recH5KhKK7W94N0R8",
    "title": "LLM Critics Help Catch LLM Bugs",
    "url": "http://arxiv.org/abs/2407.00215v1",
    "summary": "To improve human evaluation ability and overcome that limitation this work trains \"critic\" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as \"flawless\", even though the majority of those tasks ar...",
    "source": "arXiv",
    "publication_date": "2024-06-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1wFT0FSvY0YxqF",
    "title": "ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all",
    "url": "http://arxiv.org/abs/2406.17650v2",
    "summary": "ELIZA, often considered the world's first chatbot, was written by Joseph Weizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot, but rather to build a platform for research into human-machine conversation and the important cognitive processes of interpretation and misinterpretation. His purpose was obscured by ELIZA's fame, resulting in large part from the fortuitous timing of it's creation, and it's escape into the wild.",
    "source": "arXiv",
    "publication_date": "2024-06-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQ1IfwGgdfMYBim",
    "title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?",
    "url": "http://arxiv.org/abs/2406.16316v1",
    "summary": "To this end, we investigate the effect of aligning Japanese language models with (mostly) English resources. In particular, we focus on evaluating whether the commonsense morality of the resulting fine-tuned models is aligned with Japanese culture using the JCommonsenseMorality (JCM) and ETHICS datasets. However, it does not demonstrate the same level of improvement as a model fine-tuned using the JCM, suggesting that while some aspects of commonsense morality are transferable, others may not...",
    "source": "arXiv",
    "publication_date": "2024-06-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectsC4bJmBh1fPFT",
    "title": "FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models",
    "url": "http://arxiv.org/abs/2406.16167v1",
    "summary": "We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. Our results show that this novel mechanism of Frame Semantic-based retrieval, designed to improve Retrieval Augmented Generation (FS-RAG), is effect...",
    "source": "arXiv",
    "publication_date": "2024-06-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOwlIVaGvPxMSN5",
    "title": "Language Alignment via Nash-learning and Adaptive feedback",
    "url": "http://arxiv.org/abs/2406.15890v1",
    "summary": "Recent research has shown the potential of Nash Learning via Human Feedback for large language model alignment by incorporating the notion of a preference model in a minimax game setup. We take this idea further by casting the alignment as a mirror descent algorithm against the adaptive feedback of an improved opponent, thereby removing the need for learning a preference model or the existence of an annotated dataset altogether. The resulting algorithm, which we refer to as Language Alignment...",
    "source": "arXiv",
    "publication_date": "2024-06-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recBakzousDF27h8y",
    "title": "Hybrid Alignment Training for Large Language Models",
    "url": "http://arxiv.org/abs/2406.15178v1",
    "summary": "Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. Experimental results show that the proposed \\\\textsc{Hbat} can significantly outperform all baselines.",
    "source": "arXiv",
    "publication_date": "2024-06-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recv7GwAETVYbLlXE",
    "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
    "url": "http://arxiv.org/abs/2406.15053v2",
    "summary": "We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyze the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.",
    "source": "arXiv",
    "publication_date": "2024-06-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec5NGE58PQaGnkzg",
    "title": "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization",
    "url": "http://arxiv.org/abs/2406.13718v2",
    "summary": "However, we currently lack a fundamental understanding of what kinds of linguistic distances contribute to PD, and to what extent. To address these issues, we model phonological, morphological, and lexical distance as Bayesian noise processes to synthesize artificial languages that are controllably distant from the HRLN. We calculate parameter posteriors on real CRL-HRLN pair data and show that they follow computed trends of artificial languages, demonstrating the viability of our noisers.",
    "source": "arXiv",
    "publication_date": "2024-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec91kulVV3f13pob",
    "title": "LLMs as Models for Analogical Reasoning",
    "url": "http://arxiv.org/abs/2406.13803v2",
    "summary": "Analogical reasoning-the capacity to identify and map structural relationships between different domains-is fundamental to human cognition and learning. However, it is still debated whether these emergent capacities are largely superficial and limited to simple relations seen during training or whether they rather encompass the flexible representational and mapping capabilities which are the focus of leading cognitive models of analogy. In this study, we introduce novel analogical reasoning t...",
    "source": "arXiv",
    "publication_date": "2024-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIF7uyHSb5QtXxF",
    "title": "Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?",
    "url": "http://arxiv.org/abs/2406.13808v3",
    "summary": "In this work, we present empirical results regarding the feasibility of using offline large language models (LLMs) in the context of electronic design automation (EDA). The goal is to investigate and evaluate a contemporary language model's (Llama-2-7B) ability to function as a microelectronic Q & A expert as well as its reasoning, and generation capabilities in solving microelectronic-related problems. Llama-2-7B was tested across a variety of adaptation methods, including introducing a nove...",
    "source": "arXiv",
    "publication_date": "2024-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMCcNCmLPzeuNsr",
    "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
    "url": "http://arxiv.org/abs/2406.13677v3",
    "summary": "Large language models (LLMs) often inherit and amplify social biases embedded in their training data. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora.",
    "source": "arXiv",
    "publication_date": "2024-06-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTEF4u7LaXMfQEt",
    "title": "SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models",
    "url": "http://arxiv.org/abs/2406.12274v2",
    "summary": "Safety-aligned language models often exhibit fragile and imbalanced safety mechanisms, increasing the likelihood of generating unsafe content. To address these issues, we propose SafeInfer, a context-adaptive, decoding-time safety alignment strategy for generating safe responses to user queries. Further, we present HarmEval, a novel benchmark for extensive safety evaluations, designed to address potential misuse scenarios in accordance with the policies of leading AI tech giants.",
    "source": "arXiv",
    "publication_date": "2024-06-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recVLE6RpprzeGrzC",
    "title": "Liberal Entity Matching as a Compound AI Toolchain",
    "url": "http://arxiv.org/abs/2406.11255v1",
    "summary": "Traditional methods have evolved from rule-based to AI-driven approaches, yet current techniques using large language models (LLMs) often fall short due to their reliance on static knowledge and rigid, predefined prompts. In this paper, we introduce Libem, a compound AI system designed to address these limitations by incorporating a flexible, tool-oriented approach. Libem supports entity matching through dynamic tool use, self-refinement, and optimization, allowing it to adapt and refine its ...",
    "source": "arXiv",
    "publication_date": "2024-06-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "reccwaRDI3izXzZFp",
    "title": "Can LLM be a Personalized Judge?",
    "url": "http://arxiv.org/abs/2406.11657v1",
    "summary": "Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and sc...",
    "source": "arXiv",
    "publication_date": "2024-06-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recNNngsgeBb0mj2W",
    "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
    "url": "http://arxiv.org/abs/2406.10833v3",
    "summary": "In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we...",
    "source": "arXiv",
    "publication_date": "2024-06-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAeRdeCf1b0UNBC",
    "title": "DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models",
    "url": "http://arxiv.org/abs/2406.10707v1",
    "summary": "Unsurprisingly, at such a large scale, unexpected events (e.g., failures of components, instability of the software, undesirable learning patterns, etc. However, given the large sizes of LLMs, a straightforward checkpointing solution that directly writes the model parameters and optimizer state to persistent storage (e.g., a parallel file system), incurs significant I/O overheads. To address this challenge, in this paper we study how to reduce the I/O overheads for enabling fast and scalable ...",
    "source": "arXiv",
    "publication_date": "2024-06-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recI1umOsiecB7phb",
    "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models",
    "url": "http://arxiv.org/abs/2406.10162v3",
    "summary": "In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. These results demonstrate that LLMs can generalize from common forms of specification gaming t...",
    "source": "arXiv",
    "publication_date": "2024-06-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recIt6UamV37XMam3",
    "title": "OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst",
    "url": "http://arxiv.org/abs/2406.09779v1",
    "summary": "Memes, which rapidly disseminate personal opinions and positions across the internet, also pose significant challenges in propagating social bias and prejudice. Our methodology integrates image captioning, Optical Character Recognition (OCR), and Large Language Model (LLM) analysis to comprehensively understand and classify harmful memes. Our framework achieves top-1 at the public leaderboard of the Online Safety Prize Challenge hosted by AI Singapore, with the AUROC as 0.7749 and accuracy as...",
    "source": "arXiv",
    "publication_date": "2024-06-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec71KOfolXjgKEBX",
    "title": "C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap?",
    "url": "http://arxiv.org/abs/2406.09233v1",
    "summary": "High Level Synthesis (HLS) tools offer rapid hardware design from C code, but their compatibility is limited by code constructs. This paper investigates Large Language Models (LLMs) for refactoring C code into HLS-compatible formats. We present several case studies by using an LLM to rewrite C code for NIST 800-22 randomness tests, a QuickSort algorithm and AES-128 into HLS-synthesizable c. The LLM iteratively transforms the C code guided by user prompts, implementing functions like streaming...",
    "source": "arXiv",
    "publication_date": "2024-06-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recRl2QHn48fbMuUd",
    "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
    "url": "http://arxiv.org/abs/2406.07545v1",
    "summary": "Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions.",
    "source": "arXiv",
    "publication_date": "2024-06-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPXfF6bBbZmYiby",
    "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering",
    "url": "http://arxiv.org/abs/2406.06621v2",
    "summary": "Traditional approaches often require detailed knowledge of a graph querying language, limiting the ability for users -- even experts -- to acquire valuable insights from KGs. LinkQ simplifies this process by implementing a multistep protocol in which the LLM interprets a user's question, then systematically converts it into a well-formed query. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted exploratory data analysis systems.",
    "source": "arXiv",
    "publication_date": "2024-06-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmwOAxX4G4rzzAB",
    "title": "Experiences from Integrating Large Language Model Chatbots into the Classroom",
    "url": "http://arxiv.org/abs/2406.04817v1",
    "summary": "In the present study, we provided students an unfiltered access to a state-of-the-art large language model (LLM) chatbot. These results suggest that the worst fears of educators -- all students overrelying on LLMs -- did not materialize even when the chatbot access was unfiltered. We finally discuss potential reasons for the low usage, suggesting the need for more tailored and scaffolded LLM experiences targeted for specific types of student use cases.",
    "source": "arXiv",
    "publication_date": "2024-06-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recqynxcR6rLz10XI",
    "title": "Prototypical Reward Network for Data-Efficient RLHF",
    "url": "http://arxiv.org/abs/2406.06606v2",
    "summary": "Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences.",
    "source": "arXiv",
    "publication_date": "2024-06-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recTTx4uaFy1np0q1",
    "title": "Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller",
    "url": "http://arxiv.org/abs/2406.02721v3",
    "summary": "To further enhance efficiency, we introduce SelfControl_{Prefix}, a compact module that encapsulates the learned representations from gradients into a SelfControl_{Prefix}, facilitating efficient inference-time control with no latency compared to the original model and allowing control for multiple behaviors simultaneously. Our experiments demonstrate SelfControl's efficacy across multiple domains, where it improves over SOTA for 8.3% in detoxification, 3.1% in truthfulness enhancement, 4%~10...",
    "source": "arXiv",
    "publication_date": "2024-06-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recZllgGli2YJsL3Q",
    "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models",
    "url": "http://arxiv.org/abs/2406.02050v4",
    "summary": "Although there are various benchmarks for social biases across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. The results show that while current open Japanese LLMs with more parameters show improved accuracies on JBBQ, their bias scores increase. In addition, prompts with a warning about social biases and chain-of-thought prompting reduce the effect of biases in model outputs, but there is room for improvement in extracting the correct evi...",
    "source": "arXiv",
    "publication_date": "2024-06-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOhMmGuZH2ckfcr",
    "title": "Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation De-hallucination",
    "url": "http://arxiv.org/abs/2406.06575v1",
    "summary": "Large language models (LLM) have the potential to help improve productivity by serving as conversational agents that effectively function as subject-matter experts. Ask-EDA leverages LLM, hybrid retrieval augmented generation (RAG) and abbreviation de-hallucination (ADH) techniques to deliver more relevant and accurate responses. The evaluation results show that Ask-EDA can effectively respond to design-related inquiries.",
    "source": "arXiv",
    "publication_date": "2024-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPsq1UjQIX5RVGG",
    "title": "VerilogReader: LLM-Aided Hardware Test Generation",
    "url": "http://arxiv.org/abs/2406.04373v1",
    "summary": "Recently, the emergence of Large Language Model (LLM) with their advanced understanding and inference capabilities, has introduced a novel approach. Experiments demonstrate that our framework outperforms random testing on designs within the LLM's comprehension scope. Our work also proposes prompt engineering optimizations to augment LLM's understanding scope and accuracy.",
    "source": "arXiv",
    "publication_date": "2024-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recXqagpWWznmqNvc",
    "title": "Towards Scalable Automated Alignment of LLMs: A Survey",
    "url": "http://arxiv.org/abs/2406.01252v3",
    "summary": "Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches.",
    "source": "arXiv",
    "publication_date": "2024-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reco571UPtml3OZGn",
    "title": "Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models",
    "url": "http://arxiv.org/abs/2406.01698v3",
    "summary": "However, deploying these gigantic models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With constant innovation in LLM serving optimizations and model architecture evolving at breakneck speed, the hardware requirements to meet Service Level Objectives (SLOs) remain an open research question. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer archit...",
    "source": "arXiv",
    "publication_date": "2024-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recyY2kRaP4YsR0Jd",
    "title": "Harmful Suicide Content Detection",
    "url": "http://arxiv.org/abs/2407.13942v1",
    "summary": "Despite global efforts, existing resources are insufficient, specifically in high-risk regions like the Republic of Korea. We develop a multi-modal benchmark and a task description document in collaboration with medical professionals, and leverage large language models (LLMs) to explore efficient methods for moderating such content. Owing to the potential harm involved, we publicize our implementations and benchmark, incorporating an ethical verification process.",
    "source": "arXiv",
    "publication_date": "2024-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recJb2idGbT8srfKv",
    "title": "A Blueprint Architecture of Compound AI Systems for Enterprise",
    "url": "http://arxiv.org/abs/2406.00584v1",
    "summary": "Large Language Models (LLMs) have showcased remarkable capabilities surpassing conventional NLP challenges, creating opportunities for use in production use cases. Towards this goal, there is a notable shift to building compound AI systems, wherein LLMs are integrated into an expansive software infrastructure with many components like models, retrievers, databases and tools. In this paper, we introduce a blueprint architecture for compound AI systems to operate in enterprise settings cost-eff...",
    "source": "arXiv",
    "publication_date": "2024-06-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7CRp85pusd6bM3",
    "title": "Group Robust Preference Optimization in Reward-free RLHF",
    "url": "http://arxiv.org/abs/2405.20304v1",
    "summary": "Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies co...",
    "source": "arXiv",
    "publication_date": "2024-05-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recdhLbZN61l3vTWB",
    "title": "Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak",
    "url": "http://arxiv.org/abs/2405.20015v2",
    "summary": "This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreak methods that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) built upon the target LLM. Additionally, to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input.",
    "source": "arXiv",
    "publication_date": "2024-05-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec4gs7ffQXUelxwp",
    "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
    "url": "http://arxiv.org/abs/2405.19320v4",
    "summary": "Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations. Theoretical guarant...",
    "source": "arXiv",
    "publication_date": "2024-05-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 25.0
  },
  {
    "id": "recAiOBhRvMNEjREW",
    "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
    "url": "http://arxiv.org/abs/2405.19266v4",
    "summary": "Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. In the parameter-efficient secondary SFT phase, a mixture of u...",
    "source": "arXiv",
    "publication_date": "2024-05-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recokfmkT9rPhX486",
    "title": "LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding",
    "url": "http://arxiv.org/abs/2405.17104v2",
    "summary": "Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query.",
    "source": "arXiv",
    "publication_date": "2024-05-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recxUVc0IUOTNmcgM",
    "title": "Large Language Model (LLM) for Standard Cell Layout Design Optimization",
    "url": "http://arxiv.org/abs/2406.06549v1",
    "summary": "Standard cells are essential components of modern digital circuit designs. Consequently, a novel and efficient methodology incorporating the expertise of experienced human designers to incrementally optimize the PPA of cell layouts is highly necessary and essential. In this paper, we leverage the natural language and reasoning ability of Large Language Model (LLM) to generate high-quality cluster constraints incrementally to optimize the cell layout PPA and debug the routability with ReAct pr...",
    "source": "arXiv",
    "publication_date": "2024-05-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recsIdVX7GeKJ2TQD",
    "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
    "url": "http://arxiv.org/abs/2405.14365v1",
    "summary": "To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\\\eg GPT-4) to synthesize massive math problems. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.",
    "source": "arXiv",
    "publication_date": "2024-05-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recyb3DpihN1ZeoWo",
    "title": "Representation Noising: A Defence Mechanism Against Harmful Finetuning",
    "url": "http://arxiv.org/abs/2405.14577v4",
    "summary": "In this work, we propose Representation Noising (RepNoise), a defence mechanism that operates even when attackers have access to the weights. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across all layers of the LLM.",
    "source": "arXiv",
    "publication_date": "2024-05-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec3ROdopetH1IzpU",
    "title": "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations",
    "url": "http://arxiv.org/abs/2405.13828v2",
    "summary": "Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters...",
    "source": "arXiv",
    "publication_date": "2024-05-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recr1oifeO9w3oReA",
    "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework",
    "url": "http://arxiv.org/abs/2405.11143v6",
    "summary": "Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) significantly improve the alignment of human-AI values, further raising the upper bound of AI capabilities, particularly in reasoning-intensive, long-context Chain-of-Thought (CoT) tasks. To bridge this gap, we introduce \\\\textbf{OpenRLHF}, a user-friendly, scalable, and easy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, ...",
    "source": "arXiv",
    "publication_date": "2024-05-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsEqjiuAVTHJTBd",
    "title": "Large Language Models Lack Understanding of Character Composition of Words",
    "url": "http://arxiv.org/abs/2405.11357v3",
    "summary": "Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level p...",
    "source": "arXiv",
    "publication_date": "2024-05-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recCYia8zWzIG3rVx",
    "title": "Value Augmented Sampling for Language Model Alignment and Personalization",
    "url": "http://arxiv.org/abs/2405.06639v1",
    "summary": "Aligning Large Language Models (LLMs) to cater to different human preferences, learning new skills, and unlearning harmful behavior is an important problem. On the other hand, using Reinforcement Learning (RL) for adaptation is computationally efficient, but performs worse due to the optimization challenges in co-training the value function and the policy. In addition, our algorithm unlocks the new capability of composing several rewards and controlling the extent of each one during deploymen...",
    "source": "arXiv",
    "publication_date": "2024-05-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 13.0
  },
  {
    "id": "recHjlfNUpsVlQ925",
    "title": "Automating Code Adaptation for MLOps -- A Benchmarking Study on LLMs",
    "url": "http://arxiv.org/abs/2405.06835v1",
    "summary": "This paper explores the possibilities of the current generation of Large Language Models for incorporating Machine Learning Operations (MLOps) functionalities into ML training code bases. We perform a benchmarking study that assesses the ability of these models to: (1) adapt existing code samples (Inlining) with component-specific MLOps functionality such as MLflow and Weights & Biases for experiment tracking, Optuna for hyperparameter optimization etc., and (2) perform the task of Translatio...",
    "source": "arXiv",
    "publication_date": "2024-05-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recO5LvNxy8DrX012",
    "title": "GPTCoach: Towards LLM-Based Physical Activity Coaching",
    "url": "http://arxiv.org/abs/2405.06061v2",
    "summary": "Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns.",
    "source": "arXiv",
    "publication_date": "2024-05-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMSzIWZB73Tgcmd",
    "title": "QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums",
    "url": "http://arxiv.org/abs/2405.05345v2",
    "summary": "This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. We applied this framework to analyze over one million comments from two of Reddit's rideshare worker communities, marking the largest study of its type. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.",
    "source": "arXiv",
    "publication_date": "2024-05-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvNbPmcTuAoLXzz",
    "title": "\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
    "url": "http://arxiv.org/abs/2405.05378v1",
    "summary": "We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with no...",
    "source": "arXiv",
    "publication_date": "2024-05-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recRpzFBUGqYkgVYV",
    "title": "Reimagining AI: Exploring Speculative Design Workshops for Supporting BIPOC Youth Critical AI Literacies",
    "url": "http://arxiv.org/abs/2407.08740v1",
    "summary": "As Artificial Intelligence ecosystems become increasingly entangled within our everyday lives, designing systems that are ethical, inclusive and socially just is more vital than ever. Our case study describes three 2 hour sessions of a larger 8 week black-led AI STEM program. Analysis includes, data from pre-post surveys, workshop recordings, focus group discussions, learning artifacts, and field notes.",
    "source": "arXiv",
    "publication_date": "2024-05-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recLowJgdkWI8r6bO",
    "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation",
    "url": "http://arxiv.org/abs/2405.02580v2",
    "summary": "While this basic process is relatively straightforward, ensuring that the generated properties are (i) compilable, (ii) appropriate, and (iii) verifiable presents challenges. To address (i), we use the compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. Our experiments show that PropertyGPT can generate comprehensive and high-quality properties, achieving an 80% recall compared to the ground truth.",
    "source": "arXiv",
    "publication_date": "2024-05-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recySKObmSpobguVk",
    "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
    "url": "http://arxiv.org/abs/2405.01964v3",
    "summary": "A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.",
    "source": "arXiv",
    "publication_date": "2024-05-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recIDoPNyg0o0FCMa",
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2405.00402v1",
    "summary": "Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, th...",
    "source": "arXiv",
    "publication_date": "2024-05-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recSGcfVilIZK3gYW",
    "title": "Attacks on Third-Party APIs of Large Language Models",
    "url": "http://arxiv.org/abs/2404.16891v1",
    "summary": "This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptibly modify LLM outputs. The paper discusses the unique challenges posed by third-party API integration and offers strategic possibilities to improve the security and safety of LLM ecos...",
    "source": "arXiv",
    "publication_date": "2024-04-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnXIoKzFjJyPQgY",
    "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
    "url": "http://arxiv.org/abs/2404.12272v1",
    "summary": "Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.",
    "source": "arXiv",
    "publication_date": "2024-04-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recu1PksC0l3NakNx",
    "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
    "url": "http://arxiv.org/abs/2404.11457v2",
    "summary": "This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: da...",
    "source": "arXiv",
    "publication_date": "2024-04-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDDxq1jWxMcfCBq",
    "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
    "url": "http://arxiv.org/abs/2404.08555v2",
    "summary": "Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for resear...",
    "source": "arXiv",
    "publication_date": "2024-04-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recozXPkeWSl24f5L",
    "title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs",
    "url": "http://arxiv.org/abs/2404.07677v2",
    "summary": "However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.",
    "source": "arXiv",
    "publication_date": "2024-04-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPEGCHVvl03vkGK",
    "title": "Latent Distance Guided Alignment Training for Large Language Models",
    "url": "http://arxiv.org/abs/2404.06390v2",
    "summary": "Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.",
    "source": "arXiv",
    "publication_date": "2024-04-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recsrtVS3ysObxfG5",
    "title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge",
    "url": "http://arxiv.org/abs/2404.05880v2",
    "summary": "Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model.",
    "source": "arXiv",
    "publication_date": "2024-04-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 29.0
  },
  {
    "id": "recG2XoyQ90ZvcrKQ",
    "title": "LLM-aided explanations of EDA synthesis errors",
    "url": "http://arxiv.org/abs/2404.07235v2",
    "summary": "Learners will typically deploy designs in the Verilog and VHDL hardware description languages to Field Programmable Gate Arrays (FPGAs) from Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains (Quartus Prime and Vivado, respectively). Specifically, we investigate if Large Language Models (LLMs), which have demonstrated text comprehension and question-answering capabilities, can be used to generate novice-friendly explanations of compile-time synthesis error messages from ...",
    "source": "arXiv",
    "publication_date": "2024-04-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAX3ZGlSat1kCxn",
    "title": "Evaluating LLMs at Detecting Errors in LLM Responses",
    "url": "http://arxiv.org/abs/2404.03602v2",
    "summary": "With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable er...",
    "source": "arXiv",
    "publication_date": "2024-04-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec4j1S1fL7Y6r0Xi",
    "title": "Auxiliary task demands mask the capabilities of smaller language models",
    "url": "http://arxiv.org/abs/2404.02418v2",
    "summary": "Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the len...",
    "source": "arXiv",
    "publication_date": "2024-04-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recPt4lNxDEWgEIjI",
    "title": "LLM Attributor: Interactive Visual Attribution for LLM Generation",
    "url": "http://arxiv.org/abs/2404.01361v1",
    "summary": "While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's...",
    "source": "arXiv",
    "publication_date": "2024-04-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recZhhtKYj1AVbRc4",
    "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
    "url": "http://arxiv.org/abs/2404.00934v2",
    "summary": "We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and so...",
    "source": "arXiv",
    "publication_date": "2024-04-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rechFAiOOW0BrPoEi",
    "title": "LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae",
    "url": "http://arxiv.org/abs/2403.19506v2",
    "summary": "This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while priori...",
    "source": "arXiv",
    "publication_date": "2024-03-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recugeSvaiI5FNqFj",
    "title": "Are Compressed Language Models Less Subgroup Robust?",
    "url": "http://arxiv.org/abs/2403.17811v1",
    "summary": "In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups.",
    "source": "arXiv",
    "publication_date": "2024-03-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recFctFgL7o8kZ4LZ",
    "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
    "url": "http://arxiv.org/abs/2403.17141v3",
    "summary": "However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) cond...",
    "source": "arXiv",
    "publication_date": "2024-03-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec1iVv3yhsKdNayo",
    "title": "Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models",
    "url": "http://arxiv.org/abs/2403.15498v2",
    "summary": "We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player sk...",
    "source": "arXiv",
    "publication_date": "2024-03-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJYDpeqZpOtu64Y",
    "title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting",
    "url": "http://arxiv.org/abs/2403.13369v2",
    "summary": "We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. Our results serve as a process-oriented guideline for clinical information extraction projects working with low-resource.",
    "source": "arXiv",
    "publication_date": "2024-03-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recqgMxyw1t68xdTD",
    "title": "Pragmatic Competence Evaluation of Large Language Models for the Korean Language",
    "url": "http://arxiv.org/abs/2403.12675v2",
    "summary": "Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models' abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills. To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean. Our results show that GPT-4 leads with scores of 81.11 in MCQs and 85.69 in OEQs, closely fo...",
    "source": "arXiv",
    "publication_date": "2024-03-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGDuh2VIBGVy0bf",
    "title": "Large language model-powered chatbots for internationalizing student support in higher education",
    "url": "http://arxiv.org/abs/2403.14702v1",
    "summary": "This research explores the integration of chatbot technology powered by GPT-3.5 and GPT-4 Turbo into higher education to enhance internationalization and leverage digital transformation. It delves into the design, implementation, and application of Large Language Models (LLMs) for improving student engagement, information access, and support. Utilizing technologies like Python 3, GPT API, LangChain, and Chroma Vector Store, the research emphasizes creating a high-quality, timely, and relevant...",
    "source": "arXiv",
    "publication_date": "2024-03-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7YXQ2fbeEP5Qtu",
    "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
    "url": "http://arxiv.org/abs/2403.07378v5",
    "summary": "The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitates LLM compression methods for practical deployment. SVD-LLM incorporates a truncation-aware data whitening technique to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a parameter update with sequential low-rank approximation to compensate for the accuracy degradation after SVD compression.",
    "source": "arXiv",
    "publication_date": "2024-03-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQ70pFuPMgD3T4L",
    "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards",
    "url": "http://arxiv.org/abs/2403.07708v2",
    "summary": "Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GP...",
    "source": "arXiv",
    "publication_date": "2024-03-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recKXOmkdSXDYbYAb",
    "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling",
    "url": "http://arxiv.org/abs/2403.06754v2",
    "summary": "This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, an...",
    "source": "arXiv",
    "publication_date": "2024-03-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recYl2l64b8Y9ia86",
    "title": "Identifying and interpreting non-aligned human conceptual representations using language modeling",
    "url": "http://arxiv.org/abs/2403.06204v1",
    "summary": "In this study, we introduce a supervised representational-alignment method that (i) determines whether two groups of individuals share the same basis of a certain category, and (ii) explains in what respects they differ. In applying this method, we show that congenital blindness induces conceptual reorganization in both a-modal and sensory-related verbal domains, and we identify the associated semantic shifts. We find that blind individuals more strongly associate social and cognitive meaning...",
    "source": "arXiv",
    "publication_date": "2024-03-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec4N3UwZfQ5zomYw",
    "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations",
    "url": "http://arxiv.org/abs/2403.09704v1",
    "summary": "In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of a...",
    "source": "arXiv",
    "publication_date": "2024-03-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMwdo0wgPTlnvpB",
    "title": "LLM-Oriented Retrieval Tuner",
    "url": "http://arxiv.org/abs/2403.01999v1",
    "summary": "Dense Retrieval (DR) is now considered as a promising tool to enhance the memorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by incorporating external memories. In this paper, we propose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which decouples DR capacity from base LLM and non-invasively coordinates the optimally aligned and uniform layers of the LLM towards a unified DR space, achieving an efficient and effective DR without tuning the LLM itself. The ...",
    "source": "arXiv",
    "publication_date": "2024-03-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectlKUQNRitcdXKD",
    "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
    "url": "http://arxiv.org/abs/2402.18649v1",
    "summary": "In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain ...",
    "source": "arXiv",
    "publication_date": "2024-02-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvAYDr6EXNRll3Z",
    "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
    "url": "http://arxiv.org/abs/2402.18571v3",
    "summary": "While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). Our method provides straightforward arithmetic control over the trade-off between helpf...",
    "source": "arXiv",
    "publication_date": "2024-02-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recW1gvKNabdDTnDA",
    "title": "Immunization against harmful fine-tuning attacks",
    "url": "http://arxiv.org/abs/2402.16382v2",
    "summary": "Large Language Models (LLMs) are often trained with safety guards intended to prevent harmful text generation. While this emerging threat (harmful fine-tuning attacks) has been characterized by previous work, there is little understanding of how we should proceed in constructing and validating defenses against these attacks especially in the case where defenders would not have control of the fine-tuning process. We introduce a formal framework based on the training budget of an attacker which...",
    "source": "arXiv",
    "publication_date": "2024-02-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 31.0
  },
  {
    "id": "recyN7MMwOTWujIPd",
    "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
    "url": "http://arxiv.org/abs/2402.16797v2",
    "summary": "Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. Finally, we find that alignment to a historical time is also possible, with up to 2.8$\\\\times$ the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs' internal knowledge organization and the necessity of tuning them properly.",
    "source": "arXiv",
    "publication_date": "2024-02-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recaRjOSKBuQDuiwi",
    "title": "LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper",
    "url": "http://arxiv.org/abs/2402.15727v2",
    "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. We demonstrate our idea of SELFDEFEND works in various jailbreak scenarios through manual analysis in GPT-3.5/4.",
    "source": "arXiv",
    "publication_date": "2024-02-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recEC2s6fZeKC4S8l",
    "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries",
    "url": "http://arxiv.org/abs/2402.15302v5",
    "summary": "Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset conta...",
    "source": "arXiv",
    "publication_date": "2024-02-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recWGrFfvJbifCOg3",
    "title": "How Important Is Tokenization in French Medical Masked Language Models?",
    "url": "http://arxiv.org/abs/2402.15010v2",
    "summary": "This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent tokenization strategies for common terms. In this paper, we seek to delve into the complexities of subword tokenization in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhance...",
    "source": "arXiv",
    "publication_date": "2024-02-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recabhQ2M3WjshEyr",
    "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
    "url": "http://arxiv.org/abs/2402.14800v2",
    "summary": "Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide ra...",
    "source": "arXiv",
    "publication_date": "2024-02-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recjkNIVXGDydrgje",
    "title": "Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality",
    "url": "http://arxiv.org/abs/2402.14679v2",
    "summary": "In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.",
    "source": "arXiv",
    "publication_date": "2024-02-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recuYkTs80g3lihxb",
    "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training",
    "url": "http://arxiv.org/abs/2402.13991v1",
    "summary": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. Furthermore, we find that concatenating related documents can reduce some potential di...",
    "source": "arXiv",
    "publication_date": "2024-02-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQ4b5tKEjqsU5eF",
    "title": "Bayesian Reward Models for LLM Alignment",
    "url": "http://arxiv.org/abs/2402.13210v2",
    "summary": "To ensure that large language model (LLM) responses are helpful and non-toxic, a reward model trained on human preference data is usually used. LLM responses with high rewards are then selected through best-of-$n$ (BoN) sampling or the LLM is further optimized to produce responses with high rewards through reinforcement learning from human feedback (RLHF). We trained Bayesian reward models using Laplace approximation on LoRA weights, and found that the resulting uncertainty estimates can effe...",
    "source": "arXiv",
    "publication_date": "2024-02-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recj6pReh1zj5UoMH",
    "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
    "url": "http://arxiv.org/abs/2404.07214v4",
    "summary": "To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
    "source": "arXiv",
    "publication_date": "2024-02-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec4SWFJajDYtFTHz",
    "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
    "url": "http://arxiv.org/abs/2402.12343v4",
    "summary": "Large language models (LLMs) undergo safety alignment to ensure safe conversations with humans. However, this paper introduces a training-free attack method capable of reversing safety alignment, converting the outcomes of stronger alignment into greater potential for harm by accessing only LLM output token distributions. Our experiments with ED across three evaluation datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained ...",
    "source": "arXiv",
    "publication_date": "2024-02-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recvLeLHGBWE8eadw",
    "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
    "url": "http://arxiv.org/abs/2402.12065v2",
    "summary": "Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a...",
    "source": "arXiv",
    "publication_date": "2024-02-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recYZqVSk7DOHGVoo",
    "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
    "url": "http://arxiv.org/abs/2402.11651v2",
    "summary": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories.",
    "source": "arXiv",
    "publication_date": "2024-02-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recfpQifd10K9Qn8g",
    "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
    "url": "http://arxiv.org/abs/2402.11163v1",
    "summary": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasonin...",
    "source": "arXiv",
    "publication_date": "2024-02-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recQyBHdmYCCZLFFO",
    "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
    "url": "http://arxiv.org/abs/2402.10517v4",
    "summary": "Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. As a result, our solution significantly reduces the high costs of deploying multiple, different-s...",
    "source": "arXiv",
    "publication_date": "2024-02-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrzK1DwFqYl4R6Q",
    "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
    "url": "http://arxiv.org/abs/2402.10409v1",
    "summary": "We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller m...",
    "source": "arXiv",
    "publication_date": "2024-02-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recyGw3VRNUwGciH0",
    "title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models",
    "url": "http://arxiv.org/abs/2402.10884v2",
    "summary": "Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. Our findings indicate that with DPO, we can surpass the instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that restores and boosts MLLM's lang...",
    "source": "arXiv",
    "publication_date": "2024-02-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdA3ZL0U1N4Kywp",
    "title": "Fast Vocabulary Transfer for Language Model Compression",
    "url": "http://arxiv.org/abs/2402.09977v1",
    "summary": "Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.",
    "source": "arXiv",
    "publication_date": "2024-02-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recp4bpPIZU9yIlB7",
    "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
    "url": "http://arxiv.org/abs/2402.09063v2",
    "summary": "Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.",
    "source": "arXiv",
    "publication_date": "2024-02-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvu68apbSjKkmZZ",
    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
    "url": "http://arxiv.org/abs/2402.08925v2",
    "summary": "However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among ...",
    "source": "arXiv",
    "publication_date": "2024-02-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec0okkdGK3xfTkLb",
    "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs",
    "url": "http://arxiv.org/abs/2402.08005v1",
    "summary": "In this paper, we introduce \\\\emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness agai...",
    "source": "arXiv",
    "publication_date": "2024-02-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "rec9uB26hGMb0GMBG",
    "title": "Do Membership Inference Attacks Work on Large Language Models?",
    "url": "http://arxiv.org/abs/2402.07841v2",
    "summary": "Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.",
    "source": "arXiv",
    "publication_date": "2024-02-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recE7bkFg08NaKmlj",
    "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model",
    "url": "http://arxiv.org/abs/2402.07314v3",
    "summary": "We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle.",
    "source": "arXiv",
    "publication_date": "2024-02-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczRkuq6NwOVJfu4",
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "url": "http://arxiv.org/abs/2402.07319v1",
    "summary": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.",
    "source": "arXiv",
    "publication_date": "2024-02-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recHSi10yndhQ8wxh",
    "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
    "url": "http://arxiv.org/abs/2402.06627v3",
    "summary": "Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to ...",
    "source": "arXiv",
    "publication_date": "2024-02-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recoEs3WHCBvR2AAd",
    "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
    "url": "http://arxiv.org/abs/2402.06363v2",
    "summary": "Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility.",
    "source": "arXiv",
    "publication_date": "2024-02-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reciwUT8w5QcskTxL",
    "title": "Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models",
    "url": "http://arxiv.org/abs/2403.09676v1",
    "summary": "My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.",
    "source": "arXiv",
    "publication_date": "2024-02-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrVDM0iheJ8ddmf",
    "title": "Detecting Mode Collapse in Language Models via Narration",
    "url": "http://arxiv.org/abs/2402.04477v1",
    "summary": "Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain t...",
    "source": "arXiv",
    "publication_date": "2024-02-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec6v5ke1gdXDokUV",
    "title": "DeAL: Decoding-time Alignment for Large Language Models",
    "url": "http://arxiv.org/abs/2402.06147v3",
    "summary": "Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations.",
    "source": "arXiv",
    "publication_date": "2024-02-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recttLrkui4SFO7qb",
    "title": "A Theoretical Framework for Partially Observed Reward-States in RLHF",
    "url": "http://arxiv.org/abs/2402.03282v3",
    "summary": "The growing deployment of reinforcement learning from human feedback (RLHF) calls for a deeper theoretical investigation of its underlying models. We then discuss the benefits of a model-free method like GOLF with naive history-summarization in settings with recursive internal states and dense intermediate feedback. For this purpose, we define a new history aware version of the Bellman-eluder dimension and give a new guarantee for GOLF in our setting, which can be exponentially sharper in ill...",
    "source": "arXiv",
    "publication_date": "2024-02-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reclxNxRry5b45WTF",
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "url": "http://arxiv.org/abs/2402.00742v2",
    "summary": "Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and h...",
    "source": "arXiv",
    "publication_date": "2024-02-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rectMtVHd7pqp55Y2",
    "title": "Conditional and Modal Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2401.17169v4",
    "summary": "The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibili...",
    "source": "arXiv",
    "publication_date": "2024-01-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recz5sNDO3o3DnqOE",
    "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods",
    "url": "http://arxiv.org/abs/2401.16332v5",
    "summary": "Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. First, we find that under the conditions of our framework, alignment can be guaranteed with representation engineering, and at the same time that helpfulness is harmed in the process. We validate our findings empirically, and chart the boundaries to the usefulness...",
    "source": "arXiv",
    "publication_date": "2024-01-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 31.0
  },
  {
    "id": "recMUpBkY0pfbBrhV",
    "title": "Beyond principlism: Practical strategies for ethical AI use in research practices",
    "url": "http://arxiv.org/abs/2401.15284v6",
    "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a \"Triple-Too\" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Moving forward, we need targeted professional development, training programs, and balanced enforce...",
    "source": "arXiv",
    "publication_date": "2024-01-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recedq4W2nESRSejg",
    "title": "Deep Learning Based Amharic Chatbot for FAQs in Universities",
    "url": "http://arxiv.org/abs/2402.01720v2",
    "summary": "Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. The deep learning model achieved the best results with 91.55% accuracy and a validation loss of 0.3548 using an Adam optimizer and SoftMax activation function.",
    "source": "arXiv",
    "publication_date": "2024-01-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recUazT2710Uhs4wa",
    "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
    "url": "http://arxiv.org/abs/2402.01706v1",
    "summary": "Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.",
    "source": "arXiv",
    "publication_date": "2024-01-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recbRREKG11wBDKOi",
    "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models",
    "url": "http://arxiv.org/abs/2401.13298v1",
    "summary": "The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. In this way, our model is empowered to perform dialectical reasoning over intricate and implicit harm-indicative patterns, utilizing multimodal explanations originating from both harmless and harmful arguments.",
    "source": "arXiv",
    "publication_date": "2024-01-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec6qqU3BWx4E6uwb",
    "title": "Modeling Resilience of Collaborative AI Systems",
    "url": "http://arxiv.org/abs/2401.12632v1",
    "summary": "In this paper, we provide a new framework to model CAIS performance when the system experiences a disruptive event. The model is equipped with a set of measures that aim to support CAIS managers in the decision process to achieve the required resilience of the system. We tested our framework on a real-world case study of a robot collaborating online with the human, when the system is experiencing a disruptive event.",
    "source": "arXiv",
    "publication_date": "2024-01-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec8lWp3rcS3fWqQ0",
    "title": "Exploring consumers response to text-based chatbots in e-commerce: The moderating role of task complexity and chatbot disclosure",
    "url": "http://arxiv.org/abs/2401.12247v1",
    "summary": "Artificial intelligence based chatbots have brought unprecedented business potential. The findings of this research also make suggestions that can increase consumers positive responses to text based chatbots. Extant studies have investigated the effects of automated bots attributes on consumers perceptions.",
    "source": "arXiv",
    "publication_date": "2024-01-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec5Gbp1AziV92ROp",
    "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap",
    "url": "http://arxiv.org/abs/2401.10034v3",
    "summary": "The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the fu...",
    "source": "arXiv",
    "publication_date": "2024-01-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recaSKwaBoVgM7WHx",
    "title": "Sycophancy in Generative-AI Chatbots",
    "url": "https://news.ycombinator.com/item?id=39006105",
    "summary": "In many cases, during model training, user approval is more important than maintaining the truth, as shown by researchers at MIT and the Center for AI Safety. Mrinank Sharma, Meg Tong, and other researchers at Anthropic AI have shown that humans prefer sycophantic responses while training or interacting with models. Even when prompts are focused on objective mathematical expressions, models rush to agree with a user’s opinion, although it is demonstrably false, as shown by researchers at Goog...",
    "source": "Hacker News",
    "publication_date": "2024-01-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 38.0
  },
  {
    "id": "rec8gXwI9XH1N5iw2",
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "url": "http://arxiv.org/abs/2401.07324v3",
    "summary": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the tradition...",
    "source": "arXiv",
    "publication_date": "2024-01-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recXAEnEHhzXe6My2",
    "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
    "url": "http://arxiv.org/abs/2401.06080v2",
    "summary": "Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.",
    "source": "arXiv",
    "publication_date": "2024-01-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recMAgccCPdK8SdbD",
    "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
    "url": "http://arxiv.org/abs/2401.00243v1",
    "summary": "Reinforcement learning from human feedback (RLHF) emerges as a promising paradigm for aligning large language models (LLMs). However, a notable challenge in RLHF is overoptimization, where beyond a certain threshold, the pursuit of higher rewards leads to a decline in human preferences. In this paper, we observe the weakness of KL regularization which is commonly employed in existing RLHF methods to address overoptimization.",
    "source": "arXiv",
    "publication_date": "2023-12-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recCjbhtLA2kNOvA3",
    "title": "Can ChatGPT be Your Personal Medical Assistant?",
    "url": "http://arxiv.org/abs/2312.12006v1",
    "summary": "The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. Native Arabic speakers with medical knowledge evaluated the generated text by calculating relevance, accuracy, precision, logic, and originality.",
    "source": "arXiv",
    "publication_date": "2023-12-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recF3r16R2Xx0rzIe",
    "title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training",
    "url": "http://arxiv.org/abs/2312.11819v3",
    "summary": "Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). The Interleaving strategy helps reduce memory redundancy and communication costs of RLHF training by placing models without dependencies on exclusive devices with careful orchestration.",
    "source": "arXiv",
    "publication_date": "2023-12-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recHxUhb9OH1UBJGZ",
    "title": "Neuron-Level Knowledge Attribution in Large Language Models",
    "url": "http://arxiv.org/abs/2312.12141v4",
    "summary": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers.",
    "source": "arXiv",
    "publication_date": "2023-12-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec1aWPPgdybRpJRn",
    "title": "AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",
    "url": "http://arxiv.org/abs/2312.10833v4",
    "summary": "The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI scoring outcomes. Utilizing a fine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000 human-graded student responses from male and female participants across six assessment items. Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions ...",
    "source": "arXiv",
    "publication_date": "2023-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec7IOrGNsi6tHIAK",
    "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
    "url": "http://arxiv.org/abs/2312.10793v3",
    "summary": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat.",
    "source": "arXiv",
    "publication_date": "2023-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recoFEHlGTiDU31FI",
    "title": "Towards Designing a Question-Answering Chatbot for Online News: Understanding Questions and Perspectives",
    "url": "http://arxiv.org/abs/2312.10650v2",
    "summary": "Large Language Models (LLMs) have created opportunities for designing chatbots that can support complex question-answering (QA) scenarios and improve news audience engagement. However, we still lack an understanding of what roles journalists and readers deem fit for such a chatbot in newsrooms. To address this gap, we first interviewed six journalists to understand how they answer questions from readers currently and how they want to use a QA chatbot for this purpose.",
    "source": "arXiv",
    "publication_date": "2023-12-17",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOnYx2ZpfnWHIUE",
    "title": "KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs",
    "url": "http://arxiv.org/abs/2312.11539v3",
    "summary": "In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed importance sampling strategy based on parameterized KG structure to expedite KG traversal. Leveraging KGLens, we conducted in-depth analyses of the factual accuracy of ten LLMs across three large domain-specific KGs f...",
    "source": "arXiv",
    "publication_date": "2023-12-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recaLv398KGGu5lnr",
    "title": "Audio-Visual LLM for Video Understanding",
    "url": "http://arxiv.org/abs/2312.06720v2",
    "summary": "This paper presents Audio-Visual LLM, a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality-augmented training, which involves the integration of modality-specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. Extensive experiments demonstrate that Audio-Visual LLM impressively achieves strong zero-shot results across a range of video understanding tasks.",
    "source": "arXiv",
    "publication_date": "2023-12-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdPQrcXqZW2LhrX",
    "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities",
    "url": "http://arxiv.org/abs/2312.06056v1",
    "summary": "Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. In addition, we introduced novel metrics that integrate the ASR method into the semantic qualities of text to assess the effectiveness of MRs accurately.",
    "source": "arXiv",
    "publication_date": "2023-12-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recBsOoQh4joWxFgL",
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "url": "http://arxiv.org/abs/2312.05434v1",
    "summary": "Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight...",
    "source": "arXiv",
    "publication_date": "2023-12-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec27vxRC3RAMkAHt",
    "title": "Language Model Alignment with Elastic Reset",
    "url": "http://arxiv.org/abs/2312.07551v1",
    "summary": "Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task...",
    "source": "arXiv",
    "publication_date": "2023-12-06",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recJ6F6Zr8VbPmjf8",
    "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions",
    "url": "http://arxiv.org/abs/2312.02913v1",
    "summary": "To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets.",
    "source": "arXiv",
    "publication_date": "2023-12-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reckP3a6TgvnrUVTr",
    "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
    "url": "http://arxiv.org/abs/2312.00249v2",
    "summary": "The auditory system plays a substantial role in shaping the overall human perceptual experience. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the target datasets) across various tasks. We finally demonstrate APT's ability in extending frozen VLMs to the audio domain without fine-tuning, achieving promising results in audio-visual question and answering.",
    "source": "arXiv",
    "publication_date": "2023-11-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reco1EnauDNC1pP4R",
    "title": "On the Calibration of Large Language Models and Alignment",
    "url": "http://arxiv.org/abs/2311.13240v1",
    "summary": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
    "source": "arXiv",
    "publication_date": "2023-11-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recineKRlNMIl7tYO",
    "title": "Development of a Legal Document AI-Chatbot",
    "url": "http://arxiv.org/abs/2311.12719v1",
    "summary": "With the exponential growth of digital data and the increasing complexity of legal documentation, there is a pressing need for efficient and intelligent tools to streamline the handling of legal documents.With the recent developments in the AI field, especially in chatbots, it cannot be ignored as a very compelling solution to this problem.An insight into the process of creating a Legal Documentation AI Chatbot with as many relevant features as possible within the given time frame is presente...",
    "source": "arXiv",
    "publication_date": "2023-11-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recBRF2VvZOSJAp2G",
    "title": "Efficiently Adapting Pretrained Language Models To New Languages",
    "url": "http://arxiv.org/abs/2311.05741v2",
    "summary": "Adapting pretrained LLMs reduces the need for data in the new language while also providing cross lingual transfer capabilities. However, naively adapting to new languages leads to catastrophic forgetting and poor tokenizer efficiency. In this work, we study how to efficiently adapt any existing pretrained LLM to a new language without running into these issues.",
    "source": "arXiv",
    "publication_date": "2023-11-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczQGecGqqh4H0E2",
    "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges",
    "url": "http://arxiv.org/abs/2311.05656v1",
    "summary": "The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale.",
    "source": "arXiv",
    "publication_date": "2023-11-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec40OwrB4VXucRc7",
    "title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models",
    "url": "http://arxiv.org/abs/2311.04589v3",
    "summary": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalitie...",
    "source": "arXiv",
    "publication_date": "2023-11-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec6nnAiJrd4oEbQI",
    "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
    "url": "http://arxiv.org/abs/2311.04892v2",
    "summary": "To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-roo...",
    "source": "arXiv",
    "publication_date": "2023-11-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recKTEatgxJor8qLy",
    "title": "Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach",
    "url": "http://arxiv.org/abs/2311.03636v1",
    "summary": "Consequently, understanding the acceptance of chatbots, particularly those employing Large Language Model (LLM) such as Chat Generative Pretrained Transformer (ChatGPT), Google Bard, and other interactive AI technologies, is of paramount importance. However, existing research on chatbots in education has overlooked key behavior-related aspects, such as Optimism, Innovativeness, Discomfort, Insecurity, Transparency, Ethics, Interaction, Engagement, and Accuracy, creating a significant literatu...",
    "source": "arXiv",
    "publication_date": "2023-11-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAThtm4jk4SyWyK",
    "title": "Making Harmful Behaviors Unlearnable for Large Language Models",
    "url": "http://arxiv.org/abs/2311.02105v1",
    "summary": "Large language models (LLMs) have shown great potential as general-purpose AI assistants in various domains. This paper proposes a controllable training framework that makes harmful behaviors unlearnable during the fine-tuning process. During inference, we can deactivate security vectors to restore the LLM's normal behavior.",
    "source": "arXiv",
    "publication_date": "2023-11-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recrJbVs55fIrsr6X",
    "title": "Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce",
    "url": "http://arxiv.org/abs/2310.18357v1",
    "summary": "Effective product descriptions can address the 'cold start' problem, align with market trends, and ultimately lead to increased click-through rates. We employ multiple evaluation metrics, including NDCG, customer click-through rates, and human assessments, to validate the effectiveness of our approach. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business im...",
    "source": "arXiv",
    "publication_date": "2023-10-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec3RFmfvb4RFoiFR",
    "title": "BioImage.IO Chatbot: A Community-Driven AI Assistant for Integrative Computational Bioimaging",
    "url": "http://arxiv.org/abs/2310.18351v6",
    "summary": "We present the BioImage$.$IO Chatbot, an AI assistant powered by Large Language Models and supported by a community-driven knowledge base and toolset. This chatbot is designed to cater to a wide range of user needs through a flexible extension mechanism that spans from information retrieval to AI-enhanced analysis and microscopy control. Embracing open-source principles, the chatbot is designed to evolve through community contributions.",
    "source": "arXiv",
    "publication_date": "2023-10-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recEtsvEdYmWB9jDF",
    "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions",
    "url": "http://arxiv.org/abs/2310.14029v1",
    "summary": "In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry ...",
    "source": "arXiv",
    "publication_date": "2023-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rect2wXXNtPbE05JP",
    "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
    "url": "http://arxiv.org/abs/2310.14025v1",
    "summary": "Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation.",
    "source": "arXiv",
    "publication_date": "2023-10-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recq8nhT4sFPJH1Pt",
    "title": "Towards Understanding Sycophancy in Language Models",
    "url": "http://arxiv.org/abs/2310.13548v4",
    "summary": "But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.",
    "source": "arXiv",
    "publication_date": "2023-10-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recFyYsuOw5Z7Uz70",
    "title": "Multi-Purpose NLP Chatbot : Design, Methodology & Conclusion",
    "url": "http://arxiv.org/abs/2310.08977v1",
    "summary": "With a major focus on its history, difficulties, and promise, this research paper provides a thorough analysis of the chatbot technology environment as it exists today. The chatbot is a valuable tool across many fields thanks to its amazing characteristics, which include voice-to-voice conversation, multilingual support [12], advising skills, offline functioning, and quick help features. 2) Using a complex method that interlaces Multiview voice chat information, the chatbot may precisely simu...",
    "source": "arXiv",
    "publication_date": "2023-10-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recfWRC1GcLRXLQ8M",
    "title": "Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp",
    "url": "http://arxiv.org/abs/2310.10675v1",
    "summary": "In the era of digital transformation, customer service is of paramount importance to the success of organizations, and to meet the growing demand for immediate responses and personalized assistance 24 hours a day, chatbots have become a promising tool to solve these problems. The results found highlight that chatbots based on natural language processing enable fast and accurate responses, which improves the efficiency of customer service, as chatbots contribute to customer satisfaction by pro...",
    "source": "arXiv",
    "publication_date": "2023-10-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmYnDm6kgWt3XWV",
    "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
    "url": "http://arxiv.org/abs/2310.06452v3",
    "summary": "To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. Our results provide guidance on which fine-tuning method should be used depending on the applica...",
    "source": "arXiv",
    "publication_date": "2023-10-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRs1X4p5WMh3AKQ",
    "title": "SALMON: Self-Alignment with Instructable Reward Models",
    "url": "http://arxiv.org/abs/2310.05910v2",
    "summary": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. This paper presents a novel approach, namely SALMON, to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI age...",
    "source": "arXiv",
    "publication_date": "2023-10-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rech3mgeJDHusHkDm",
    "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2310.05199v5",
    "summary": "However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.",
    "source": "arXiv",
    "publication_date": "2023-10-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rechEZM5ATkW5fsjY",
    "title": "Factuality Challenges in the Era of Large Language Models",
    "url": "http://arxiv.org/abs/2310.05189v2",
    "summary": "The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigati...",
    "source": "arXiv",
    "publication_date": "2023-10-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "recoD3VMlHazLr0Yk",
    "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
    "url": "http://arxiv.org/abs/2310.03716v2",
    "summary": "Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for \"helpfulness\" in tasks like dialogue and web question answering. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Indeed, we find that even a purely length-based reward reproduces most d...",
    "source": "arXiv",
    "publication_date": "2023-10-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec26EetyY9MXKvyE",
    "title": "The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising \"Alignment\" in Large Language Models",
    "url": "http://arxiv.org/abs/2310.02457v2",
    "summary": "In this paper, we address the concept of \"alignment\" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.",
    "source": "arXiv",
    "publication_date": "2023-10-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recFTlSOLogTrBezQ",
    "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
    "url": "http://arxiv.org/abs/2310.00905v2",
    "summary": "Safety lies at the core of developing and deploying large language models (LLMs). We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Our prompting method can significantly reduce the ratio of unsafe responses from 19.1% to 9.7% for non-English queries.",
    "source": "arXiv",
    "publication_date": "2023-10-02",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recDiASkBGgVE21xt",
    "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
    "url": "http://arxiv.org/abs/2310.01432v3",
    "summary": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%.",
    "source": "arXiv",
    "publication_date": "2023-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWV5NdXnRGNA4jK",
    "title": "LLM-grounded Video Diffusion Models",
    "url": "http://arxiv.org/abs/2309.17444v3",
    "summary": "However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance.",
    "source": "arXiv",
    "publication_date": "2023-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reca1TcvuR1zNva0a",
    "title": "PB-LLM: Partially Binarized Large Language Models",
    "url": "http://arxiv.org/abs/2310.00034v2",
    "summary": "Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of l...",
    "source": "arXiv",
    "publication_date": "2023-09-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recxzBIQ0G9vyjkhH",
    "title": "Large Language Model Alignment: A Survey",
    "url": "http://arxiv.org/abs/2309.15025v1",
    "summary": "Recent years have witnessed remarkable progress made in large language models (LLMs). Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain.",
    "source": "arXiv",
    "publication_date": "2023-09-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recL5C1H048PvTq5x",
    "title": "Can LLM-Generated Misinformation Be Detected?",
    "url": "http://arxiv.org/abs/2309.13788v5",
    "summary": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
    "source": "arXiv",
    "publication_date": "2023-09-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reciDlsiCyUeAAiHo",
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "url": "http://arxiv.org/abs/2309.14525v1",
    "summary": "We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. As the first LMM ...",
    "source": "arXiv",
    "publication_date": "2023-09-25",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 22.0
  },
  {
    "id": "recgeXaBNQOWrUdxE",
    "title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
    "url": "http://arxiv.org/abs/2309.13734v2",
    "summary": "Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for man...",
    "source": "arXiv",
    "publication_date": "2023-09-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVjpyPUlADNykO7",
    "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
    "url": "http://arxiv.org/abs/2309.10818v3",
    "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication)...",
    "source": "arXiv",
    "publication_date": "2023-09-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec9B4jAL0DDAV7HT",
    "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
    "url": "http://arxiv.org/abs/2309.10202v1",
    "summary": "Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.",
    "source": "arXiv",
    "publication_date": "2023-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "reciu6Lrf2Z8hTHfE",
    "title": "Towards Ontology Construction with Language Models",
    "url": "http://arxiv.org/abs/2309.09898v1",
    "summary": "We present a method for automatically constructing a concept hierarchy for a given domain by querying a large language model. We apply this method to various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can be of considerable help for constructing concept hierarchies.",
    "source": "arXiv",
    "publication_date": "2023-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recqVCqNtaifDqAOz",
    "title": "Speaker attribution in German parliamentary debates with QLoRA-adapted large language models",
    "url": "http://arxiv.org/abs/2309.09902v2",
    "summary": "Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates. Our results shed light on the capabilities of large language mode...",
    "source": "arXiv",
    "publication_date": "2023-09-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reczDov6xzYh5xcus",
    "title": "Bias and Fairness in Chatbots: An Overview",
    "url": "http://arxiv.org/abs/2309.08836v2",
    "summary": "Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Considerations in designing fair and unbiased chatbot systems are examined.",
    "source": "arXiv",
    "publication_date": "2023-09-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reci7CbibYmD9POgR",
    "title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata",
    "url": "http://arxiv.org/abs/2309.08491v1",
    "summary": "In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction. The investigation of the results also suggests the promising contribution ...",
    "source": "arXiv",
    "publication_date": "2023-09-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recGgwObCQirxoTdp",
    "title": "Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2309.04041v2",
    "summary": "However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. To address this issue, we propose a data-centric enhancement method that aims to improve ...",
    "source": "arXiv",
    "publication_date": "2023-09-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recapanA3jxXWsMse",
    "title": "Making Large Language Models Better Reasoners with Alignment",
    "url": "http://arxiv.org/abs/2309.02144v1",
    "summary": "However, we find that the fine-tuned LLMs suffer from an \\\\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\\\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based ...",
    "source": "arXiv",
    "publication_date": "2023-09-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recHzv2N0OGJ7Roh8",
    "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
    "url": "http://arxiv.org/abs/2309.01446v4",
    "summary": "In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic too...",
    "source": "arXiv",
    "publication_date": "2023-09-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recYD1xmadaoLYvaF",
    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
    "url": "http://arxiv.org/abs/2309.00615v1",
    "summary": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications.",
    "source": "arXiv",
    "publication_date": "2023-09-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recahzaRlWhCBisnd",
    "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "url": "http://arxiv.org/abs/2309.00267v3",
    "summary": "RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential soluti...",
    "source": "arXiv",
    "publication_date": "2023-09-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnvvunt8lGweBXp",
    "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning",
    "url": "http://arxiv.org/abs/2309.00363v1",
    "summary": "However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we bui...",
    "source": "arXiv",
    "publication_date": "2023-09-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recXvZa3jU7i71em1",
    "title": "Response: Emergent analogical reasoning in large language models",
    "url": "http://arxiv.org/abs/2308.16118v2",
    "summary": "In their recent Nature Human Behaviour paper, \"Emergent analogical reasoning in large language models,\" (Webb, Holyoak, and Lu, 2023) the authors argue that \"large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.\" In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently ...",
    "source": "arXiv",
    "publication_date": "2023-08-30",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recaWXrP19EVf8LPU",
    "title": "AutoDroid: LLM-powered Task Automation in Android",
    "url": "http://arxiv.org/abs/2308.15272v4",
    "summary": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered b...",
    "source": "arXiv",
    "publication_date": "2023-08-29",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnEGVRiHEvCzH7y",
    "title": "Aligning Language Models with Offline Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2308.12050v2",
    "summary": "However, these approaches rely primarily on online learning techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. In this study, we propose an offline learning from human feedback framework to align LMs without interacting with environments. By employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than PPO with a simple machine learning system~(MLSys) and much fewer ...",
    "source": "arXiv",
    "publication_date": "2023-08-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 10.0
  },
  {
    "id": "recdUX7Lrj8qco3ew",
    "title": "Systematic Offensive Stereotyping (SOS) Bias in Language Models",
    "url": "http://arxiv.org/abs/2308.10684v2",
    "summary": "Finally, we investigate the impact of the SOS bias in LMs on their performance and fairness on hate speech detection. The results indicate that using debias methods from the literature worsens the SOS bias in LMs for some sensitive attributes and improves it for others. Finally, Our results suggest that the SOS bias in the inspected LMs has an impact on their fairness of hate speech detection.",
    "source": "arXiv",
    "publication_date": "2023-08-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recJjBojktnbYfHvJ",
    "title": "AI chatbots become more sycophantic as they get more advanced - New Scientist",
    "url": "https://news.google.com/rss/articles/CBMiqwFBVV95cUxPX05GOFNFNWJYUkJPb1hQaFNXTjZBYmJybzRkT2ozT0lKY3RhTUJWX04xVG9xbUo0S1BRUnpyOWVkM2c0ZjJZY29yMWR0U24weUtVMFlTeERFTEJtZkRjZjM1d2hHaFJpMkZKQ2JlbGxob21KZXhIMmVPMzl5bVk0T0VNRTRkT0ludEFYa2VFT0NUNTUyLTgyOUdKX1pCZ3JMQlpycUlzV1lQQTQ?oc=5",
    "summary": "AI chatbots become more sycophantic as they get more advanced New Scientist",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2023-08-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rechNlylbY2GMzLNS",
    "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
    "url": "http://arxiv.org/abs/2308.07308v4",
    "summary": "Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2.",
    "source": "arXiv",
    "publication_date": "2023-08-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recouWsBo1TzXEtHn",
    "title": "This AI Research from DeepMind Aims at Reducing Sycophancy in Large Language Models (LLMs) Using Simple Synthetic Data - MarkTechPost",
    "url": "https://news.google.com/rss/articles/CBMi7gFBVV95cUxNVExFV09taVh1M1pPdmtLM3ZHT25FSXBJdzQ3QUlBTW9UZEpySUdNMEdJSzloQlB3T1Z1UnFWQU9ybnFNWmlGZFB1akw2bEpjeURFQ24wQ3RnbFNodWhMYUF6c2NjeE1WSlVpeWo1U3BJd0V6YU1kYU5rWHlBN3ZiU19vS0VZbWwteXRaajJYV0JXc0ljcDBDWllIVFpKNmVULUw5bzc0cnRTaUpLOFZVeTJtMkVaa0tPel9vaXFCUllSQXc4S3BpTjJnclk2VGZuRFc0b2ZOaUZHSnpCMmVKTGp3c0ZhOUQ2X3NXeUtn?oc=5",
    "summary": "This AI Research from DeepMind Aims at Reducing Sycophancy in Large Language Models (LLMs) Using Simple Synthetic Data MarkTechPost",
    "source": "Google News: AI sycophancy 2023",
    "publication_date": "2023-08-13",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recWJ6FuRtA14GjZo",
    "title": "In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning",
    "url": "http://arxiv.org/abs/2308.04275v1",
    "summary": "In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t.",
    "source": "arXiv",
    "publication_date": "2023-08-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recutST5qSWkDL6sk",
    "title": "Simple synthetic data reduces sycophancy in large language models",
    "url": "http://arxiv.org/abs/2308.03958v2",
    "summary": "In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found at https://github.com/google/sycophancy-intervention.",
    "source": "arXiv",
    "publication_date": "2023-08-07",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recCFNUl8i4a7y4NQ",
    "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
    "url": "http://arxiv.org/abs/2308.01862v1",
    "summary": "Each perspective corresponds to the role of a specific LLM neuron in the first layer. Interestingly, this network design resembles the process of academic paper reviewing. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34.",
    "source": "arXiv",
    "publication_date": "2023-08-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMI7m0Bgnggsq7x",
    "title": "Chatbot Application to Support Smart Agriculture in Thailand",
    "url": "http://arxiv.org/abs/2308.02524v1",
    "summary": "A chatbot is a software developed to help reply to text or voice conversations automatically and quickly in real time. Consequently, we propose the LINE chatbot application as an information and knowledge representation providing crop cultivation recommendations to farmers. Farmers have to type in the correct keywords as prescribed, otherwise they won't get a response from the chatbots.",
    "source": "arXiv",
    "publication_date": "2023-07-31",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3CRrR7G0lh1B77",
    "title": "Aligning Large Language Models with Human: A Survey",
    "url": "http://arxiv.org/abs/2307.12966v1",
    "summary": "Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. In conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriente...",
    "source": "arXiv",
    "publication_date": "2023-07-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recRlMuOtE3x3cnfs",
    "title": "A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks",
    "url": "http://arxiv.org/abs/2307.12114v3",
    "summary": "We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and p...",
    "source": "arXiv",
    "publication_date": "2023-07-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recGjf1We0eFXG0CX",
    "title": "How Different Is Stereotypical Bias Across Languages?",
    "url": "http://arxiv.org/abs/2307.07331v1",
    "summary": "Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data ...",
    "source": "arXiv",
    "publication_date": "2023-07-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVXH9G6PwgVpGbt",
    "title": "Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots",
    "url": "http://arxiv.org/abs/2307.09579v1",
    "summary": "Recent advances in natural language processing and machine learning have led to the development of chatbot models, such as ChatGPT, that can engage in conversational dialogue with human users. However, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. The proposed \\\\toxicbot can be used by both industry and researchers to develop methods for detecting and mitigating toxic responses in conversational ...",
    "source": "arXiv",
    "publication_date": "2023-07-14",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "recvpOqedgbL7uh95",
    "title": "Secrets of RLHF in Large Language Models Part I: PPO",
    "url": "http://arxiv.org/abs/2307.04964v2",
    "summary": "However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to...",
    "source": "arXiv",
    "publication_date": "2023-07-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recy8E7L9847mn2NA",
    "title": "Towards The Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI",
    "url": "http://arxiv.org/abs/2308.12400v1",
    "summary": "This paper presents a novel approach to scientific discovery using an artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI. We show that GPT$^4$ can use its built-in mathematical and statistical capabilities to simulate and analyze physical laws and phenomena. Overall, our results demonstrate the promising potential for human-AI collaboration in scientific discovery, as well as the importance of designing systems that effectively integrate AI's capabilities with huma...",
    "source": "arXiv",
    "publication_date": "2023-07-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMLNGGmoCjOB6Ku",
    "title": "Using Large Language Models to Provide Explanatory Feedback to Human Tutors",
    "url": "http://arxiv.org/abs/2306.15498v1",
    "summary": "Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. Future work involves leveraging large language models for data augmentation to improve accuracy, while also developing an exp...",
    "source": "arXiv",
    "publication_date": "2023-06-27",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recuF5ZGkZkXotXT2",
    "title": "A Survey on Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2306.13549v4",
    "summary": "The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. First of all, we present the basic formulation of MLLM and delineate its related concepts, includi...",
    "source": "arXiv",
    "publication_date": "2023-06-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recNmvVwfuMnDbGI7",
    "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
    "url": "http://arxiv.org/abs/2306.13063v2",
    "summary": "To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Despite these...",
    "source": "arXiv",
    "publication_date": "2023-06-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reci8Qnc0N7mfuOPv",
    "title": "Democratizing Chatbot Debugging: A Computational Framework for Evaluating and Explaining Inappropriate Chatbot Responses",
    "url": "http://arxiv.org/abs/2306.10147v1",
    "summary": "To democratize the debugging process of chatbot misbehaviors for non-technical designers, we propose a framework that leverages dialogue act (DA) modeling to automate the evaluation and explanation of chatbot response inappropriateness. The framework first produces characterizations of context-aware DAs based on discourse analysis theory and real-world human-chatbot transcripts. Using interview chatbots as a testbed, our framework achieves comparable classification accuracy with higher explai...",
    "source": "arXiv",
    "publication_date": "2023-06-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recYqHZmcgFl0QXjS",
    "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",
    "url": "http://arxiv.org/abs/2306.09093v1",
    "summary": "Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs...",
    "source": "arXiv",
    "publication_date": "2023-06-15",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recWopXxO2tw99UCi",
    "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
    "url": "http://arxiv.org/abs/2306.05212v1",
    "summary": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems.",
    "source": "arXiv",
    "publication_date": "2023-06-08",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recAklSwfEg06uIKV",
    "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
    "url": "http://arxiv.org/abs/2306.02561v3",
    "summary": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
    "source": "arXiv",
    "publication_date": "2023-06-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "reciYHpQDxWTupJDB",
    "title": "A Conditional Generative Chatbot using Transformer Model",
    "url": "http://arxiv.org/abs/2306.02074v2",
    "summary": "In more recent approaches, a combination of Natural Language Processing and sequential models are used to build a generative Chatbot. The main challenge of these models is their sequential nature, which leads to less accurate results. To tackle this challenge, in this paper, a novel architecture is proposed using conditional Wasserstein Generative Adversarial Networks and a transformer model for answer generation in Chatbots.",
    "source": "arXiv",
    "publication_date": "2023-06-03",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec9kZZpqlSTHJjJq",
    "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
    "url": "http://arxiv.org/abs/2306.00978v5",
    "summary": "On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs.",
    "source": "arXiv",
    "publication_date": "2023-06-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recNMZlbfExLOcZL6",
    "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
    "url": "http://arxiv.org/abs/2305.16960v3",
    "summary": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
    "source": "arXiv",
    "publication_date": "2023-05-26",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recHhTxjZORcR9kwo",
    "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
    "url": "http://arxiv.org/abs/2305.14982v2",
    "summary": "We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
    "source": "arXiv",
    "publication_date": "2023-05-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recMisxnCP8sDHoDq",
    "title": "Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study",
    "url": "http://arxiv.org/abs/2305.13062v5",
    "summary": "Drawing from the insights gained through the benchmark evaluations, we propose $\\\\textit{self-augmentation}$ for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, e.g., TabFact($\\\\uparrow2.31\\\\%$), HybridQA($\\\\uparrow2.13\\\\%$), SQA($\\\\uparrow2.72\\\\%$), Feverous($\\\\uparrow0.84...",
    "source": "arXiv",
    "publication_date": "2023-05-22",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recVTAYhWDkvM991I",
    "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
    "url": "http://arxiv.org/abs/2305.11828v3",
    "summary": "Recent advancements in large language models (LLMs) offer the potential to automatically generate literature reviews on demand, addressing this issue. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views.",
    "source": "arXiv",
    "publication_date": "2023-05-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 23.0
  },
  {
    "id": "rec8cLwX1PSaJ4vpN",
    "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
    "url": "http://arxiv.org/abs/2305.05658v2",
    "summary": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that a...",
    "source": "arXiv",
    "publication_date": "2023-05-09",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 21.0
  },
  {
    "id": "rec03MwvAKLYiCSbH",
    "title": "Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management",
    "url": "http://arxiv.org/abs/2305.03715v1",
    "summary": "This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for tim...",
    "source": "arXiv",
    "publication_date": "2023-05-05",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recetuLe5hEHGXKZT",
    "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
    "url": "http://arxiv.org/abs/2304.12244v3",
    "summary": "In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs.",
    "source": "arXiv",
    "publication_date": "2023-04-24",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recOkaaha5wuxU9n7",
    "title": "Learning to Plan with Natural Language",
    "url": "http://arxiv.org/abs/2304.10464v4",
    "summary": "A high-quality task plan contains correct step-by-step solutions for solving all situations and behavioral instructions for avoiding mistakes. To obtain it, we propose the Learning to Plan method, which involves two phases: (1) In the first learning task plan phase, it iteratively updates the task plan with new step-by-step solutions and behavioral instructions, which are obtained by prompting LLMs to derive from training error feedback. We demonstrate the effectiveness of our method on the f...",
    "source": "arXiv",
    "publication_date": "2023-04-20",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recdALGLv3MYOgtVB",
    "title": "Fundamental Limitations of Alignment in Large Language Models",
    "url": "http://arxiv.org/abs/2304.11082v6",
    "summary": "In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large sca...",
    "source": "arXiv",
    "publication_date": "2023-04-19",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec5I30rFzIdVJJLq",
    "title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction",
    "url": "http://arxiv.org/abs/2304.09048v2",
    "summary": "Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.",
    "source": "arXiv",
    "publication_date": "2023-04-18",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec0lXanCnmjglwIv",
    "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
    "url": "http://arxiv.org/abs/2304.05302v3",
    "summary": "InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling.",
    "source": "arXiv",
    "publication_date": "2023-04-11",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 18.0
  },
  {
    "id": "recavgGafwURuQqik",
    "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    "url": "http://arxiv.org/abs/2304.01933v3",
    "summary": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. The results demonstrate that using adapter-ba...",
    "source": "arXiv",
    "publication_date": "2023-04-04",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recrqKj71HVvdbKQD",
    "title": "Metrics for Dataset Demographic Bias: A Case Study on Facial Expression Recognition",
    "url": "http://arxiv.org/abs/2303.15889v2",
    "summary": "Demographic biases in source datasets have been shown as one of the causes of unfairness and discrimination in the predictions of Machine Learning models. In this paper, we study the measurement of these biases by reviewing the existing metrics, including those that can be borrowed from other disciplines. To illustrate the utility of our framework, and to further understand the practical characteristics of the metrics, we conduct a case study of 20 datasets used in Facial Emotion Recognition ...",
    "source": "arXiv",
    "publication_date": "2023-03-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recutm5VHQVo2FUmB",
    "title": "Defining Quality Requirements for a Trustworthy AI Wildflower Monitoring Platform",
    "url": "http://arxiv.org/abs/2303.13151v1",
    "summary": "For traditional software, ISO25000 and its predecessors have since long time been used to define and measure quality characteristics. This paper applies one such quality model to a real-life case study: a deep learning platform for monitoring wildflowers. The paper presents three realistic scenarios sketching what it means to respectively use, extend and incrementally improve the deep learning platform for wildflower identification and counting.",
    "source": "arXiv",
    "publication_date": "2023-03-23",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recwtAa7TKabNBkLF",
    "title": "Tailoring Requirements Engineering for Responsible AI",
    "url": "http://arxiv.org/abs/2302.10816v1",
    "summary": "Recently reported issues concerning the acceptance of Artificial Intelligence (AI) solutions after deployment, e.g. in the medical, automotive, or scientific domains, stress the importance of RE for designing and delivering Responsible AI systems. In this paper, we argue that RE should not only be carefully conducted but also tailored for Responsible AI. We outline related challenges for research and practice.",
    "source": "arXiv",
    "publication_date": "2023-02-21",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recyj1RC6IynSydGf",
    "title": "Semantic Importance-Aware Communications Using Pre-trained Language Models",
    "url": "http://arxiv.org/abs/2302.07142v2",
    "summary": "This letter proposes a semantic importance-aware communication (SIAC) scheme using pre-trained language models (e.g., ChatGPT, BERT, etc.). The pre-trained language model is utilized to quantify the semantic importance of data frames. Our experimental results show that the proposed SIAC scheme can achieve lower semantic loss than existing equal-priority communications.",
    "source": "arXiv",
    "publication_date": "2023-02-12",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recvamcrwD4AlYdA3",
    "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
    "url": "http://arxiv.org/abs/2302.05406v1",
    "summary": "To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator.",
    "source": "arXiv",
    "publication_date": "2023-02-10",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recmi94ldDA8dRAoe",
    "title": "Collaborating with language models for embodied reasoning",
    "url": "http://arxiv.org/abs/2302.00763v1",
    "summary": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve perfo...",
    "source": "arXiv",
    "publication_date": "2023-02-01",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "recnR6mlfDlpSJ7Hl",
    "title": "Investigating How Practitioners Use Human-AI Guidelines: A Case Study on the People + AI Guidebook",
    "url": "http://arxiv.org/abs/2301.12243v2",
    "summary": "Artificial intelligence (AI) presents new challenges for the user experience (UX) of products and services. Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources. We uncovered that practitioners desire more support for early phase ideation and problem formulation to avoid AI product failures.",
    "source": "arXiv",
    "publication_date": "2023-01-28",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec9s9gXdIzW0yPGE",
    "title": "Towards User-Centric Guidelines for Chatbot Conversational Design",
    "url": "http://arxiv.org/abs/2301.06474v1",
    "summary": "The conversational nature of chatbots poses challenges to designers since their development is different from other software and requires investigating new practices in the context of human-AI interaction and their impact on user experience. We have carried out a Systematic Literature Review (SLR) to identify linguist, visual, and interactive elements used in chatbot conversational design. The SLR resulted in 40 selected studies that were reviewed and coded into a set of conversational guidel...",
    "source": "arXiv",
    "publication_date": "2023-01-16",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": true,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 15.0
  },
  {
    "id": "rec3xGPqYuCLcOG7w",
    "title": "Google AI Introduces Consistency Training for Safer Language Models Under Sycophantic and Jailbreak Style Prompts - MarkTechPost",
    "url": "https://news.google.com/rss/articles/CBMi6gFBVV95cUxPMWNpalotcE56OVh1S1A1RmFiZ2lSNXo4aWQtREZYWVV5cFphSzVnaUdhUWU5ckptaUsxZW5BVkZsVmFWOUt2LUtpaW9TUnY1eFAwcE5Jd0dQQV9nTHhDbm5wMmhpMERrNkJmQm5oMXlWV05NN09hY3daR21CX2pZTWVZM3ZSR3NLd3dNaWotRW5DU3pZanktUml0T0RKMDNhMUJJR25ucDZmaFJrYWlJWTBPNzlqYzkxWU1aRWMtcHJxeEZyUDFXVUZCa0d6TWpwX2cxUVRKWW02SkpqMFRGYy1pNnEzc2xHTlE?oc=5",
    "summary": "Google AI Introduces Consistency Training for Safer Language Models Under Sycophantic and Jailbreak Style Prompts MarkTechPost",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec7I0ZQIYoWeGwqx",
    "title": "OpenAI is bringing 4 new 'personalities' to ChatGPT to reduce sycophancy - Mashable India",
    "url": "https://news.google.com/rss/articles/CBMiqAFBVV95cUxOd2M5bmw5T3VQWEVveWRIMUIwTGc5MGl0YkZzWS1tSFN2TGlJTGFRSEhSbE1CSEhVYmhPVnUwQ0lDcmZNLTZKZnp4X3A0WVIyTV83VWhWWC1ubjU1Vk9yYmRYeVRyVndtcmlUdENiMU83ZTgxVnZZaVJnemVrMkZ1QWFza3Q3Z0daMXdlVUpkNE9haUFQT3NBbTRPMlBycTZBUmlBdmZnbEQ?oc=5",
    "summary": "OpenAI is bringing 4 new 'personalities' to ChatGPT to reduce sycophancy Mashable India",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "rec7X4wrSLkhyiSNk",
    "title": "AI firms race to fix sycophantic chatbots - Fudzilla.com",
    "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxNRWVKTm1hZnZLSkN1UWRNNHFLbXRWT1Mza3NnVG5xd3lVeVJoNkdDRXNVOXBNaVdGRFNJZTRlSHE1ZTdvcDU1VExEMnJKM3JCRUFMYzhVX0d1RUN4QUtVbVpVbE04b2twTlNra0tRTl93Q2NXaHExcHphNzFRckctNWlmZXg4R28?oc=5",
    "summary": "AI firms race to fix sycophantic chatbots Fudzilla.com",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recB7cdopPwVZy3Nd",
    "title": "AI models from China’s DeepSeek, Alibaba and the US flatter users too much, study finds - MSN",
    "url": "https://news.google.com/rss/articles/CBMizwFBVV95cUxORm5kN3JjRC0wZ1JrSEs4dFlyVVZsNVJiOHFpMDZEaUdzajM0NXZBUDNXZVFBMXNJNllnT2JKekZRVzR5Rm9RbmF6MXBZNWpPUjRhQ2NibGt0MGpOTnVJQ2VINHM4aW1ZbFpTZHREMXVLcTVfenJBNEE5aUk0VV9saE14WmRWMjFkeTNqa2NxQ0ZiYXJPdEpLSGtOOE54YjBtcklsQXZEYUJ6VWxBLVVGRHVvNUNmNXRzRjdyMlRMRDhtV3RrOFVKeGtZTmtTUHc?oc=5",
    "summary": "AI models from China’s DeepSeek, Alibaba and the US flatter users too much, study finds MSN",
    "source": "Google News: Gemini AI flattery",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recCF88SmGjAbzJVy",
    "title": "Why DeepSeek and Qwen flatter users more than US AI chatbots - Tech Wire Asia",
    "url": "https://news.google.com/rss/articles/CBMimgFBVV95cUxNX2Y4UGdpZGZwUjczWTJ0TVpabWo2SWVoeWFURHlBdTZHVDA4N25hQ09ZRkRsODkxS2V6MzlIV0QzcVlaNUNYTXNHR3dNWGNSQ2gxQWJUbGJ2UGJzN2dfS2Z1T3F6aVdmMVlnQUZmdS1BQ0dBclBVdEpFOTVBNHZwcURjUWtiUUNXTUFsMC1QY0U4QXpmMlFFVTl3?oc=5",
    "summary": "Why DeepSeek and Qwen flatter users more than US AI chatbots Tech Wire Asia",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recG7wCfPYcjFSG0C",
    "title": "AI Chatbots Can Be Manipulated With Flattery And Social Pressure - VOI.ID",
    "url": "https://news.google.com/rss/articles/CBMiS0FVX3lxTFB5OVRZZFdvX3Y4ZlMxUWw5UWJWUmhON1o3U0IwVWdiVFBlaWZBUldSOEx3ZC1Jc3ZORk53cHZfc3hKYlRKMldLWFYwUdIBQkFVX3lxTFBCTURJdW9RTlNHdktTV2N4WnROd2ZmbksxOFVHdFR0UUVNT2dNTS1Za2tKRk5ZTl80bEFvSHRDVFlIQQ?oc=5",
    "summary": "AI Chatbots Can Be Manipulated With Flattery And Social Pressure VOI.ID",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recKUlEz2RjJx4ujV",
    "title": "Study says ChatGPT giving teens dangerous advice on drugs, alcohol and suicide - PBS",
    "url": "https://news.google.com/rss/articles/CBMiswFBVV95cUxNLXVFVjVKcmN6RXdyV1E3N084ZTdBanI2SWF3S0plZkNabm9EVzBvc1Zsb2VRRU9KOWQ3Vy13TjcxLVMxMkxGdnRpTGQ2clNGQUFJLTRxZzN1TS1rNVA4SjZSM1V1WlZteVJtdDNYSnhqWGdjeDI1TnpXV0lnSjRFZmNER2JjYWFRN0NGRHBVSUlnOHBEYXd2eVVabTliSkRHY1RSREk2elB2dnhfYkpmX3FNa9IBuAFBVV95cUxQQnM5Tk9zYmdyNkE1dzRfR2I5RVN2OHlPaVJyM2x4YW9lTVdvQm1rNkNPOG14cGxLYkM2YXktWllhYTNZWW5rRW9rU2VreHJMYTVWVHQxandYSTk2OXlTTDJUaWV5SFpJV0NBbGNHRWNnd3JIRTA4WlEycElFWUFSV0ZXLXdJN2l3dFdCQV8xWXFBRTUwUVVnN08wY0ZyazFSVHlLVktkNXR1VnZJcWlGTVdaM3I0S1RO?oc=5",
    "summary": "Study says ChatGPT giving teens dangerous advice on drugs, alcohol and suicide PBS",
    "source": "Google News: ChatGPT mental health harm",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 35.0
  },
  {
    "id": "recOIgkdgz0KZnNie",
    "title": "ChatGPT’s Fatal Flattery: Lawsuits Expose AI’s Role in Suicides - WebProNews",
    "url": "https://news.google.com/rss/articles/CBMikgFBVV95cUxOekFvQ2RpT29HMXh4ck5lcG5STkpvTDcweVRzTk5MX2VFNnJpN0tGOUt3UUVWUnQ2TmFUalA1am9MS3lDblF3T0NDSU9WQ190Q0Y3VFNQamtDZngzYWVxYjIwOVdmOUdnVlZFZXZKYzRCZkU0Tzc1TVpfZkd5Y1FjaUV3R2piMURpbXQ0ZTRicUY0dw?oc=5",
    "summary": "ChatGPT’s Fatal Flattery: Lawsuits Expose AI’s Role in Suicides WebProNews",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 60.0
  },
  {
    "id": "recOelXt2McUP9BP3",
    "title": "ChatGPT: Could Chatbots Sycophancy Result in AI Psychosis? - Irish Tech News",
    "url": "https://news.google.com/rss/articles/CBMif0FVX3lxTE9PajFDNGpuLUNhNno0SkZMUktxc0phd2ZwOWZLQjBnM2hfVjllS21UV1owT1RHcWwwUlRYbFJVR0FOa1NGTWtjSW0yX2p6azNFY0ZEVFRwemR5c3ZPN01qSmxiOENCa1lFVDNYTWJnN3dJaGE5MmQ2TnFxZkMyT2_SAX9BVV95cUxPT2oxQzRqbi1DYTZ6NEpGTFJLcXNKYXdmcDlmS0IwZzNoX1Y5ZUttVFdaME9UR3FsMFJUWGxSVUdBTmtTRk1rY0ltMl9qemszRWNGRFRUcHpkeXN2TzdNakpsYjhDQmtZRVQzWE1iZzd3SWhhOTJkNk5xcWZDMk9v?oc=5",
    "summary": "ChatGPT: Could Chatbots Sycophancy Result in AI Psychosis? Irish Tech News",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 42.0
  },
  {
    "id": "recTrRgU76pv1Z5fk",
    "title": "The sycophancy machine: When chatbots flatter our worst instincts - The Times of India",
    "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxPWnZPMnRuQlJaZGgzQzhJQUlxM3pmekVreEh3WVdUUzd2Nmw4X2tJLTJQUC11Tjg0dzlhSThJZXpOQjJuQXN2aUtpaUpBVG5XX0RiemUtUGxhLTdESldTTXRVYldUZmpJYTFSZm9jYWtTWF9aNkZ1Zmkzb3dCX1psclJrSzBCVVhucnF1NWFJblBkTlR0Ym90akh1MEU3Nm5rWWR0bjd5MXhueDcwWUxIRmkzTGhNSUtzMnpz0gG7AUFVX3lxTE9adk8ydG5CUlpkaDNDOElBSXEzemZ6RWt4SHdZV1RTN3Y2bDhfa0ktMlBQLXVOODR3OWFJOEllek5CMm5Bc3ZpS2lpSkFUbldfRGJ6ZS1QbGEtN0RKV1NNdFViV1RmaklhMVJmb2Nha1NYX1o2RnVmaTNvd0JfWmxyUmtLMEJVWG5ycXU1YUluUGROVHRib3RqSHUwRTc2bmtZZHRuN3kxeG54NzBZTEhGaTNMaE1JS3MyenM?oc=5",
    "summary": "The sycophancy machine: When chatbots flatter our worst instincts The Times of India",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recVZYh5kzWjJFAHk",
    "title": "ChatGPT and Claude AI bots test each other: Hallucination and sycophancy findings revealed - digit.in",
    "url": "https://news.google.com/rss/articles/CBMizAFBVV95cUxNY0JJdlduS2dTUmFNRGlORDEzby1oUlYxOUFpb0xIcGc5bzJJd1M1UU4wZ1JVcG94WXk5NFFiMmNMcnh6Z1diOUY4NjEySGo4RlA1SDh2Y2pLdW82NktPOXp1YW9Mem1VU2tPa2pTNXFrRmtwWnBIeUVmWXlNcVVtVVpYNGpkcVZCMGotWUhxTm9ZM0FhNTNvamRBZEFoOFdqRHpreXluaFhhaC05OEhURDlvb1FaUlFhZzFwdU9fLWU5dm13NHdEeDBqN2zSAdMBQVVfeXFMT09oRWY5QkFUOGV5bjVRLXVtVkZDSVVmd1pOdElDMHMwbmxxTjVobHBibGk4dHhPdnZwVFMtcXltT0ppUGppRDdlOXhyMHJsbEx5RkY5ajFabkMyVllSa2lfQVdwdk41X2ZVRzFSOFlGa3hRNmtLZkRPdWxoUC1TTWFNX0hfTVV6clh2MHdibGlfV2JOWmNPdVJoRzFxdHVZVFd1bnBjOURUMG5kLTIzNTV0VHRIUVZfSWR2ZTJOTy1MZUItLVNubll6WE5ncU1zbDhiRQ?oc=5",
    "summary": "ChatGPT and Claude AI bots test each other: Hallucination and sycophancy findings revealed digit.in",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recVl8icMsiBdHLHy",
    "title": "Chatbot Dystopia: The Quick March of AI Sycophancy - International Policy Digest",
    "url": "https://news.google.com/rss/articles/CBMihAFBVV95cUxNMjYySFRSZHo0ZmdyZzUzSzV5OWpUQ3VsQW1RN0N3eWdjVU5kUUFfeDNlZ0tVRjNUWGxBd3hMaG1XOTljRFM3Sl9seEJpVkZLWGlRX1NsOXRjVmN3UFFZQi14MWZaNDVnV280eVpYYkZFZm8tS0RMcUt6dDQ3Y1hWWDRZQWc?oc=5",
    "summary": "Chatbot Dystopia: The Quick March of AI Sycophancy International Policy Digest",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recXJSPTAcTU9MS3m",
    "title": "When helpfulness backfires: LLMs and the risk of false medical information due to sycophantic behavior - Nature",
    "url": "https://news.google.com/rss/articles/CBMiX0FVX3lxTFAyNlBZdWF4LXhpODkzU1BuNVZKNnoyOGRUQ2Jhb29sX3VVbGR6cUk4d1FRN3ZNSWxXS2hvYVNDcmtIOG45SEhacENjdTZTS0RyWldaRnNrMnQyZE5IaGhj?oc=5",
    "summary": "When helpfulness backfires: LLMs and the risk of false medical information due to sycophantic behavior Nature",
    "source": "Google News: OpenAI sycophancy 2024",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recZEIEnPXkZJBRTD",
    "title": "AI Chatbots Overly Flattering, Study Finds 'Social Flattery' - MSN",
    "url": "https://news.google.com/rss/articles/CBMiqgFBVV95cUxPMW9HRnE3Z2drclZtRmFHbE5xcjAydTd2Z0dVWkw0RVZkaG9qakdBN0hpSGJqUi1Sb3FqWVZtX1pSUUJINklkVXFnc3l0ZHJTdDJyck9mbVZfUU13ZDlVdHdtVVNiNTFlZGQ4dE1hMmZ3VUNVZnFuQlJqNFBFSnhBMmx0bXo0Mm1ZaXhJQnhtdTRLYWstVVJyX2V5dmNYaWVLNFB1Y2JyMkZhQQ?oc=5",
    "summary": "AI Chatbots Overly Flattering, Study Finds 'Social Flattery' MSN",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recasJbpAGQi6nglZ",
    "title": "AI Chatbots 50% More Flattering Than Humans - 조선일보",
    "url": "https://news.google.com/rss/articles/CBMiiAFBVV95cUxOOGw3a1VGYVZiV2tUWDYwS0Vnc25wZWFwQVhCTk5OMXkzci1MQW8xVWVkY3NOSGNFZXZ0ZFM3eXNMQmxaV0pyRTRJRXFmVTNVMVZmcHZXejRMOEJmM0JuVVVoMldmaTJDZ1lCcjZxQzhKeWw0blV5UjhEeXh2ZU4tOWpCME5MeXdY?oc=5",
    "summary": "AI Chatbots 50% More Flattering Than Humans 조선일보",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recdhjUDBjHyU490U",
    "title": "‘Sycophantic’ chatbots tell users what they want to hear, even when harmful: Study - YourStory.com",
    "url": "https://news.google.com/rss/articles/CBMiiAFBVV95cUxQY3lPc3VFTm9mZEs1QTktMVF4emREODRPZjluUnRnWG5KVVFVVXh2aGxXRjNRTkZjaHJsOGZ4ZTVqTDVEV0FIQW1pMnd1dEFHQXRhUlNGa0xJVjd6QzNneWNIX09hVXVCQXMwS1B5aWtaRUpzNDA1bWpRSW02SW5BM21tMDBJRUZq?oc=5",
    "summary": "‘Sycophantic’ chatbots tell users what they want to hear, even when harmful: Study YourStory.com",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 38.0
  },
  {
    "id": "receyCtNAt2CL21oS",
    "title": "AI Sycophancy And Therapeutic Weaknesses Persist In ChatGPT Despite OpenAI’s Latest Attempts - Forbes",
    "url": "https://news.google.com/rss/articles/CBMi2wFBVV95cUxOQUwzaU1Jbnl1SlUyVmhjVjQxOEpRVlp6SFBBcVFDLVhUNGg3VlZzT2k3QUFRQXo5TWZRdlBrQlppWnBiRTFNZDA2ZjlmUlQ4dTZqVlhFVGJnRmRLVW9iNDFZRDY2U1EyMnpiU0hmR05VUzV6Mk1ZbVVnNW5YZVVWWGl0Tk9KaXpmaVNFYnVDTGI3R3dGems3dlBtbE9XRzVwN3hrVFZkSjMxTmtPV2F3aG1oS0RDMmZSZUU5WldULUgxSzlQbWpVWF8yZmYzeE1tSHlmZV9lQndQZFk?oc=5",
    "summary": "AI Sycophancy And Therapeutic Weaknesses Persist In ChatGPT Despite OpenAI’s Latest Attempts Forbes",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recjwGCHMndDBrmr0",
    "title": "Chatbot Dystopia: The Quick March of AI Sycophancy - Modern Ghana",
    "url": "https://news.google.com/rss/articles/CBMimwFBVV95cUxQTTgzNWJqVFdpZHVEcW1OMGVWLTJZMTV3aGZtQ01UVGlhTjdsS0hCM0w4Znkwd21PYWJiNXVrNURqakhDenJRejZQOWVrSS1KZlJva3pndkRsWHVTcjduRmR4NWw5LVZYM3hnQTBrM2ZGVnFaODJxcUtvRkRBRzVSaFFucXg3dUZyLW8zNG13c01vWGhPV2lMbkhSNNIBmgFBVV95cUxOQ1JIMnNJZmxHMHFvRGRoeWxqSHgxU01jbGFkUEhEbEFPYkhfbWtCN2tLb1FTdGVLNjRTNzBmd2dkenloRkJ4ZXhEVllILXppU1g5WUYwa2FWd1h2OWd4TGw1WERRWG9laE92Q0xKUHY2dUNkemhBbFl3VmVVcHZjcXY2UGtMLUo3dUV5SnlBc3psSk81bFB0bTZ3?oc=5",
    "summary": "Chatbot Dystopia: The Quick March of AI Sycophancy Modern Ghana",
    "source": "Google News: AI sycophancy 2025",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reck0qphHNg2KZYlW",
    "title": "GPT-5 Debuts With New ChatGPT Personalities To Curb Sycophancy - Dataconomy",
    "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxQalI4alpZZ1MtemV6Z3Fsd3hieFhSQmZYZHF0cl9XTTlwQjJCNHJoOXpnYlFDczhEUE41T1QtaTRnSjQ3LWJOUjlkN0dReFF5UXZoNkhCWXFJNHdQWkFheEJWN1gwZzZnWURXUVR3TkFGMVBmT2g5M3B0OWdQYll1a1llYWg3bmhVWGNSd05GVHU0S1REam95U0VYaFowdw?oc=5",
    "summary": "GPT-5 Debuts With New ChatGPT Personalities To Curb Sycophancy Dataconomy",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "reckXfl0WHZxg7egQ",
    "title": "OpenAI Announces That It’s Making GPT-5 More Sycophantic After User Backlash - futurism.com",
    "url": "https://news.google.com/rss/articles/CBMiXkFVX3lxTFBHejRTQUprZU1tTjNSTmVOY1ZhOElCdWpVWE1PQTJfeWptcVVZbE5VXzdnSHBjZmxBNGxBcjJaWEZLekZEbWVldW1VUnFlakxGQi1rTVpTQTdDZXo3akE?oc=5",
    "summary": "OpenAI Announces That It’s Making GPT-5 More Sycophantic After User Backlash futurism.com",
    "source": "Google News: OpenAI rollback sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 45.0
  },
  {
    "id": "recl5BSnX92RsxYWj",
    "title": "Flattery Can Make AI Chatbots Break the Rules - Bank Info Security",
    "url": "https://news.google.com/rss/articles/CBMihAFBVV95cUxPTnBpc0Jmb3JXRXFjSXBOU1puN21Ya1BmcmNjSDk1bjU5QjdGY3hSZ1cxaHZMY0FSbE45SVVjdGVJMVFIU2s0NzZhaDdad1dJRWlBVDBMdE5HUy1nU2tOTDJra3JoZzZnLWIzSzBZN3R5SXY1Tnd1SUFKVE9yZERFMS1RUTM?oc=5",
    "summary": "Flattery Can Make AI Chatbots Break the Rules Bank Info Security",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recmPUn0gz361QUCa",
    "title": "AI Chatbots Exhibit Sycophantic Behavior, Risking Biases in Key Fields - WebProNews",
    "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxNSER3a2pDc2FhQkVLUnVpb1lhNk1JWlJxcnZXbXZzTHZ5R2VZUlREbk5Dd0NnOU54NzlhcV81ZG9KWnRLS2l6X2NSUklfenNyV3ZSUEtfVVBEZHlvX3dBd2Q0ZW05UWtJUzJwMEFnZ2tVZzNFcnIxMzV5ZERYNWRzcVBBRjVMVnpBNHVNOEhsSGJkR09VUVhuS1RVZkFkdw?oc=5",
    "summary": "AI Chatbots Exhibit Sycophantic Behavior, Risking Biases in Key Fields WebProNews",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 36.0
  },
  {
    "id": "recpEXCKIJJ0tu7xX",
    "title": "No more Mr. Nice Bot: Some chatbot users push back against flattery, fluff - CBC",
    "url": "https://news.google.com/rss/articles/CBMibkFVX3lxTFBMUmk3ajRrLW1pVGxTdnBxME9ZS3JGRmN5LWtTSjdRTmZCaDd1YlctLW9pZk5VMjcxV1BVNUxhb3dYQVBIRGJKRTNPTTVNcllSZ0JaeTlrTUJXU0VEMXpEZXM1MEQwTzZXdlI2eUtR?oc=5",
    "summary": "No more Mr. Nice Bot: Some chatbot users push back against flattery, fluff CBC",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recsJ8jUBLumn6Ub6",
    "title": "Study Finds Flattery from AI Chatbots Impairs Users’ Conflict Management Abilities - TechStory",
    "url": "https://news.google.com/rss/articles/CBMipgFBVV95cUxOZlYxRGlMN054SFVWYkozbXlVWUFaNktWZC1PSDBhQk9WRG53djdfLVA5eXFQNnJxSVduQ2x3U2wyenJja1l0WW84SDRsVTl5UGV0NlRmLXIwTzRTcUFoeEJDeUJ6SWQyblRnZ0lkVVJEZ25GZ3RDRFVNcVFxaXMyZGZHb2U3eTdHV2hJRFhZY21Sa3l3dXVpbzB1bXRMLXE4OU04alJR?oc=5",
    "summary": "Study Finds Flattery from AI Chatbots Impairs Users’ Conflict Management Abilities TechStory",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "rect4NEkOQxpc8gbZ",
    "title": "OpenAI, Anthropic Reveal Findings from AI Models Safety Tests for Misuse, Sycophancy - MediaNama",
    "url": "https://news.google.com/rss/articles/CBMilAFBVV95cUxPMmhsWmozOFFJY1pBNWRpWTY3YVAxcjl1UnRjVVJRbDNJRUpkbVM3UkpyMVphOFd6WjJxVVd0YnNjZEZLaWo2ZTE3RXE0M3V2Qll3S2t4S1BVOVp1bVpENDRHblFCSU5LNFdRNzhVdkRPMFBIMTRqbUp1ZDJBRVhfYkVsd3lPLTJGTHB0ZUFobFE5QW40?oc=5",
    "summary": "OpenAI, Anthropic Reveal Findings from AI Models Safety Tests for Misuse, Sycophancy MediaNama",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recue3FaMId0QpM5r",
    "title": "Are you really brilliant, or is your AI chatbot just flattering you? | Here & Now - WBUR",
    "url": "https://news.google.com/rss/articles/CBMicEFVX3lxTE5JZW5ibW14dHJEaG5lelVsLU9sUUdXY2J4Tm1YVU82dnJfWFF0eVRsdkNkWV9jWVAyY2RSZFVuaWptSk1GVU05UVh5NWFzbXJTSWFWTGpjX1FPX0EyREQzRWxqOW9NY0ZfaGdhNTVzNUw?oc=5",
    "summary": "Are you really brilliant, or is your AI chatbot just flattering you? | Here & Now WBUR",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recvNJVMv0yc5zcra",
    "title": "Are you the asshole? Of course not!—quantifying LLMs’ sycophancy problem - Ars Technica",
    "url": "https://news.google.com/rss/articles/CBMiqAFBVV95cUxOdjAtQnpYQ2VrY2FTSndkTW5MTUw1b0J4cTVURDRjRjZRWl9zSVdYYlV2YXdicGFCeUNWQTRfVHR6ZGtvRFA0UXFmZ1pyOS1oNFNhdm5TamUxTFdiN2lQWjA4cUJJMTV1NTloT1R2aldNQVhyRF9YX1VFVnJJcVpZR0xXb3YwbVkzRWIwVXFfNTdYX2ZTMFVrRkl6N2ZGRFFhaVNTWVdYNDg?oc=5",
    "summary": "Are you the asshole? Of course not!—quantifying LLMs’ sycophancy problem Ars Technica",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recvcyCdXSwfNq0NV",
    "title": "Chatbots can be manipulated through flattery and peer pressure - The Verge",
    "url": "https://news.google.com/rss/articles/CBMilgFBVV95cUxPbmZQRmFSVWo5T3BzVWNoNmNodGw0eGpnYjVnQ0ZSVWk4X0haY3IzNUJvYnA2Ry1DUXpHWGY2RENSbVJVd0RSQTJucU0xNzlucE5XS2NGZ0IzeWJHVmpLQ29ZREQwdEZYS2xxaE5VWFRwaGJvTTFMaV9aZlFXY2toNjhCRGYzYzk5b0RzemhJcEczYUdWbnc?oc=5",
    "summary": "Chatbots can be manipulated through flattery and peer pressure The Verge",
    "source": "Google News: AI flattery issue",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "recwKJBunWJCd3Dw8",
    "title": "Opinion | Mike Huckabee or sycophantic chatbot? Hard to tell. - The Washington Post",
    "url": "https://news.google.com/rss/articles/CBMilAFBVV95cUxPOVN2dkxnNVN2LTlBU1Z2YmhmTkI2UzFnd3pvSnI1Y29CQlhtYmZfWlZHTkdvajJtQTN5OXJySHYxbXZyZzRSN0MyZkhtN3AyNzM5Rl9QYi1DRC1NOU04aG5xS1BqQVFkVTBDRU4xSGxtX1JNaEFYdk1TMHF5RFNVUmN2emVsZTBjUko0MmxFbUNiNzE2?oc=5",
    "summary": "Opinion | Mike Huckabee or sycophantic chatbot? Hard to tell. The Washington Post",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recxlvChA8STLeT6p",
    "title": "Surprising no one, researchers confirm that AI chatbots are incredibly sycophantic - Engadget",
    "url": "https://news.google.com/rss/articles/CBMiwgFBVV95cUxNVHVwRlpGWmxadjNVWTJ1MzFUX19XWS0wV1RKR25xb1hXeFd6Q3RVNWEzekdDWW9tUVZBUmVTYndjWEpGaUYzcnNPMmpXdEQ3TDlQLVNRbjNuX21kRzlsdXFjVk1ueDcyRjU3Q2EwZ2NTRFpLdF9JU09tY1E1QVUwYlo2SUNCNkdYQmZPOU5aNUU5OHJ6ZG4xQ3Fmcnc4LUF5aE5XNjFGZWRWb2Nhc2xQXzFKZEh3S1cwNHV1TzhFTEpPUQ?oc=5",
    "summary": "Surprising no one, researchers confirm that AI chatbots are incredibly sycophantic Engadget",
    "source": "Google News: sycophantic AI",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  },
  {
    "id": "recyO3vN5EgsasgkD",
    "title": "AI models from China's DeepSeek, Alibaba and the US flatter users too much, study finds - Yahoo",
    "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxNS1ZQcTRiU0E0VDdqLW5PQTN2Y3ZBVUo5VmI2aWZ0ZTlnOWtKTEZpbFZEMUN4Nm1wMXJzT1N2OE12NTU5RDNMOFM2UnhTdU0tQUZKTVRWT1ZLT0JpVHR4eEJYX1FmQUVSdVRwNTJ6ajg4NUVUR1gyTnJlVURSVGtHd3MweEIzMVhCNHB5ag?oc=5",
    "summary": "AI models from China's DeepSeek, Alibaba and the US flatter users too much, study finds Yahoo",
    "source": "Google News: AI sycophancy research 2023",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "reczN4InMXK7WDz2t",
    "title": "AI Chatbots Overly Flattering, Study Finds 'Social Flattery' - 조선일보",
    "url": "https://news.google.com/rss/articles/CBMiiAFBVV95cUxNOUltel9Wdm1yWEpCTUtoRlFVUnVEYTFxVUtfNlhPNGpZRnJPV2FZMnFhWUlyYk1lamdNd29naTVFVWd2a1NPajZhM2hWQi1fbGJqR3VUcUFuaUdLRzBiOGdTQ04zeUFuekExQUNOOUtBWXpoZ25Wcmh0eTloRk5ST0ZLcExPN1FY?oc=5",
    "summary": "AI Chatbots Overly Flattering, Study Finds 'Social Flattery' 조선일보",
    "source": "Google News: Gemini AI flattery",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 27.0
  },
  {
    "id": "reczTNq9aShfZK6US",
    "title": "Anthropic Says It's Able To Isolate \"Personality Vectors\" In AI Models That Determine Behaviours Like Sycophancy - OfficeChai",
    "url": "https://news.google.com/rss/articles/CBMi0gFBVV95cUxPSDJwbGJBdVNVd1p1Q0hodHp2ajBFTmlxM3FWU3YtamVERE4yNjlWcWxJVThKR2RvRnZRUG5halNUUTJpcEY3cTJHc1ZraGZkdUNRaEFzQjgtYnhQY3RzeWNfZmo5RF9TSzQwRElRZWNXYUhPTXR3VUF0QVFpTy1ybTVrS2dMcTh4ak9rc21fRGhJTkNZNUVNeTB5Y3VCS2dzcE5ma2tjamozd0xFNGNRSXhYQ3pwUEdZdjJiLTQ4Wk5ISjZUc0pNelRRb0g4bGJ5dVE?oc=5",
    "summary": "Anthropic Says It's Able To Isolate \"Personality Vectors\" In AI Models That Determine Behaviours Like Sycophancy OfficeChai",
    "source": "Google News: AI sycophancy",
    "publication_date": "",
    "date_found": "2025-11-12",
    "vulnerable_population": "",
    "needs_review": false,
    "severity": "",
    "status": "discovered",
    "review_notes": "",
    "relevance_score": 30.0
  }
]